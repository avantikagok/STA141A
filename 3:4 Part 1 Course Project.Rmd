
---
title: "Part 1 Course Project"
author: ""
date: "14 Feb 2025"
output:
  html_document: default
  pdf_document: default
---

```{r}
library(tidyverse)
library(dplyr)
```

```{r, include=FALSE}
getwd()
setwd("/Users/davisavantika")
session=list()
for(i in 1:18){
  session[[i]]=readRDS(paste("~/Code/STA141AProject/Data/session",i,'.rds',sep=''))
  print(session[[i]]$mouse_name)
  print(session[[i]]$date_exp)
  print(length(session[[i]]$contrast_left))
  print(length(session[[i]]$contrast_right))
  print(length(session[[i]]$feedback_type))
  print(length(session[[i]]$brain_area))
  print(length(session[[i]]$spks))
  print(length(session[[i]]$time))
}
```



## Overview

This document contains instructions on the **course project** for STA 141A Winter 2025. This document is made with `R markdown`. The `rmd` file to generate this document is available on the course website. 

# Background


In this project, we analyze a subset of data collected by Steinmetz et al. (2019). While this document provides the basic understanding of the experiments, it is highly recommended that one consults the original publication for a more comprehensive understanding in order to improve the quality of the analysis report.


In the study conducted by Steinmetz et al. (2019), experiments were performed on a total of 10 mice over 39 sessions. Each session comprised several hundred trials, during which visual stimuli were randomly presented to the mouse on two screens positioned on both sides of it. The stimuli varied in terms of contrast levels, which took values in {0, 0.25, 0.5, 1}, with 0 indicating the absence of a stimulus. The mice were required to make decisions based on the visual stimuli, using a wheel controlled by their forepaws. A reward or penalty (i.e., feedback) was subsequently administered based on the outcome of their decisions. In particular, 

- When left contrast > right contrast, success (1) if turning the wheel to the right and failure (-1) otherwise.  
- When right contrast > left contrast, success (1) if turning the wheel to the left and failure (-1) otherwise.  
- When both left and right contrasts are zero, success (1) if holding the wheel still and failure (-1) otherwise. 
- When left and right contrasts are equal but non-zero, left or right will be randomly chosen (50%) as the correct choice. 


The activity of the neurons in the mice's visual cortex was recorded during the trials and made available in the form of spike trains, which are collections of timestamps corresponding to neuron firing. In this project, we focus specifically on the spike trains of neurons from the onset of the stimuli to 0.4 seconds post-onset. In addition, we only use 18 sessions (Sessions 1 to 18) from four mice: Cori, Frossman, Hence, and Lederberg.


# Data structure 

---

A total of 18 RDS files are provided that contain the records from 18 sessions. In each RDS file, you can find the name of mouse from `mouse_name` and date of the experiment from `date_exp`. 

Five variables are available for each trial, namely 

- `feedback_type`: type of the feedback, 1 for success and -1 for failure
- `contrast_left`: contrast of the left stimulus
- `contrast_right`: contrast of the right stimulus
- `time`: centers of the time bins for `spks`  
- `spks`: numbers of spikes of neurons in the visual cortex in time bins defined in `time`
- `brain_area`: area of the brain where each neuron lives

Take the 11th trial in Session 5 for example, we can see that the left contrast for this trial is `r 
session[[5]]$contrast_left[11]`  the right contrast is `r 
session[[5]]$contrast_right[11]`, and the feedback (i.e., outcome) of the trial is `r session[[5]]$feedback_type[11]`. There are a total of `r length(session[[5]]$brain_area)` neurons in this trial from `r length(unique(session[[5]]$brain_area))` areas of the brain. The spike trains of these neurons are stored in `session[[5]]$spks[[11]]` which is a `r dim(session[[5]]$spks[[11]])[1]` by `r dim(session[[5]]$spks[[11]])[2]` matrix with each entry being the number of spikes of one neuron (i.e., row) in each time bin (i.e., column).

```{r}
#dim(session[[5]]$spks[[11]])[1]

#dim(session[[5]]$spks[[11]])[2]
```



# Question of interest


The primary objective of this project is to build a predictive model to predict the outcome (i.e., feedback type) of each trial using the neural activity data (i.e., spike trains in `spks`), along with the stimuli (the left and right contrasts). Given the complexity of the data (and that this is a course project), we break the predictive modeling into three parts as follows. 


```{r}
#outcome = feedback type
#input: spks, left and right contrast
```


Part 1. Exploratory data analysis. In this part, we will explore the features of the data sets in order to build our prediction model. In particular, we would like to (i) describe the data structures across sessions (e.g., number of neurons, number of trials, stimuli conditions, feedback types), (ii) explore the neural activities during each trial, (iii) explore the changes across trials, and (iv) explore homogeneity and heterogeneity across sessions and mice. 


```{r, include=FALSE}
print(session[[1]]$spks[114])
print(length(session[[1]]$brain_area))
print(session[[1]]$time)
```
#Understanding the dataset
This dataset contains 18 sessions. Each session contains within it the data of several variables: mouse name, date of experiment, left contrast, right contrast, feedback type, brain area, spikes, and time. 

The left contrast, right contrast, feedback type, time, and spikes variables have the same length. This indicates that for a given contrast combination, the feedback type and the neural spike activity vs time was recorded. 

Looking closer at the spike data, each contrast combination resulted in the spike activity being documented over the number of neurons analyzed and over time. The number of neurons matches the length of the brain area data, as each neuron is classified by its brain area. 

#Number of neurons across sessions
```{r}
summarydata <- data.frame(session=c(1:18), trials=c(1:18), neuronnumber=c(1:18), feedbacknumber=c(1:18))



for(i in 1:18) {
  summarydata$trials[i] <- length(session[[i]]$spks)
  summarydata$feedbacknumber[i] <- length(session[[i]]$feedback_type)
  summarydata$neuronnumber[i] <- length(session[[i]]$brain_area)
}

summarydata
```

#This code summarizes the basic facts about each session's data into a dataframe. This was taken from the class code to initally summarize data to identify what trends and relationships between variables to look into more closely. 



```{r}
n.session=length(session)

# in library tidyverse
meta <- tibble(
  session_num = c(1:18),
  mouse_name = rep('name',n.session),
  date_exp =rep('dt',n.session),
  n_brain_area = rep(0,n.session),
  n_neurons = rep(0,n.session),
  n_trials = rep(0,n.session),
  success_rate = rep(0,n.session)
)


for(i in 1:n.session){
  tmp = session[[i]];
  meta[i,2]=tmp$mouse_name;
  meta[i,3]=tmp$date_exp;
  meta[i,4]=length(unique(tmp$brain_area));
  meta[i,5]=dim(tmp$spks[[1]])[1];
  meta[i,6]=length(tmp$feedback_type);
  meta[i,7]=mean(tmp$feedback_type+1)/2;
  }

meta



ggplot(meta, mapping=aes(x=session_num, y=success_rate, color=mouse_name)) + geom_col()

    #kable(meta, format = "html", table.attr = "class='table table-striped'",digits=2) 
```


```{r}
# Calculate mean and standard deviation for each mouse
mouse_stats <- aggregate(success_rate ~ mouse_name, data = meta, 
                        FUN = function(x) c(mean = mean(x), sd = sd(x)))
mouse_stats <- do.call(data.frame, mouse_stats)

# Create a bar plot with error bars
ggplot(mouse_stats, aes(x = mouse_name, y = success_rate.mean, fill = mouse_name)) +
  geom_bar(stat = "identity") +
  geom_errorbar(aes(ymin = success_rate.mean - success_rate.sd, 
                   ymax = success_rate.mean + success_rate.sd),
                width = 0.2) +
  labs(title = "Average Success Rate by Mouse with Standard Deviation",
       x = "Mouse Name",
       y = "Average Success Rate") +
  theme_minimal() +
  theme(legend.position = "none")
```

We see that the average success rate in this experiment differs by mouse. Lederberg has the highest average success and Cori has the lowest average success. However, it is important to note that the sessions are not equally distributed by mouse. Cori only has 3 sessions worth of data, while Lederberg has 7 sessions worth of data. 



Is there a correlation between brain area types and success rate?
```{r}
ggplot(meta, mapping=aes(x=n_brain_area, y=success_rate, color=mouse_name)) + geom_point() + geom_line() +  geom_smooth(method = "lm", se = FALSE, linetype = "dashed") + ggtitle("Number of brain areas vs Success Rate per Mouse over 18 sessions")
```
There seems to be a complex relationship between the number of brain areas whose neurons fire during this experiment and the success rate. I hypothesized that there might be a trend where an increased of brain areas fired may lead to higher success response rates. However, each mouse seems to demonstrate their own trend for the relationship between the variables of n_brain_area and success_rate. 

Cori seems to have a slight positive linear trend, mice Forssmann and Hench seem to have a negative linear trend, and mouse Lederberg seems to have essentially no trend. 


```{r}
ggplot(meta, mapping=aes(x=n_brain_area, y=n_neurons, color=mouse_name)) + geom_point() + geom_line(linetype="dotted") +  geom_smooth(method = "lm", se = FALSE) + ggtitle("Number of brain areas vs Number of Neurons per Mouse over 18 sessions")
```

This graph demonstrates data towards the hypothesis that an increased number of brain areas involved resulted in a larger number of neurons being fired. However, the data suggests that there isn't a consistent trend between these two variables. In three out of the four mice, there is a general positive trend, where greater number of brain areas yield larger numbers of fired neurons. However in mouse Cori, we see the opposite trend. 

This graph also demonstrates that different sessions of the same mouse have different numbers of brain areas and neurons fired. This is an interesting finding, as one would assume that the same replicated experiment would fire the same number of neurons and the same brain areas would be activated in the same mouse. 


One next question to ask would be to understand how contrast pairs yield to successes or failures. 



```{r}
ggplot(meta, mapping=aes(x=session_num, y=n_brain_area, color=mouse_name)) + geom_col() +ggtitle("Session Number vs Number of Brain Areas by Mouse")
```
This bar graph demonstrates in a different way that each session fired a different number of brain areas. Even sessions within the same mouse were not consistent in the number of brain areas fired. 



```{r}
session[[1]]$spks[1]
session[[1]]$brain_area[1]

average_spike_area <- function(trials, this_session) {
  # Initialize an empty list to store results
  results_list <- list()
  
  # Loop through each trial in the provided trials
  for (i.t in trials) {
    spk.trial <- this_session$spks[[i.t]]
    area <- this_session$brain_area
    spk.count <- apply(spk.trial, 1, sum)
    spk.average <- tapply(spk.count, area, mean)
    
    # Convert to data frame and store trial index
    trial_df <- data.frame(
      trial = i.t,
      brain_area = names(spk.average),
      avg_spike_count = as.numeric(spk.average)
    )
    
    results_list[[i.t]] <- trial_df
  }
  
  # Combine all results into a single data frame
  results_df <- do.call(rbind, results_list)
  
  return(results_df)
}

i = 2
trial_indices <- 1:(length(session[[i]]$feedback_type))  # Example: trials 1 through 10
results_df <- average_spike_area(trial_indices, session[[i]])
print(results_df)

ggplot(results_df, mapping=aes(x=brain_area, y=avg_spike_count, color=trial)) + geom_point() +stat_summary(fun = mean, geom = "point", size = 4, color = "red") 

```
This data frame demonstrates the average spike count in each brain region and in each trial of session 2. The graph shows us that there there is a difference between spike count and brain area. 


```{r}
# Initialize an empty list to store results from all sessions
all_results_list <- list()

# Loop through all sessions (i = 1:18)
for (i in 1:18) {
  trial_indices <- 1:(length(session[[i]]$feedback_type))  # Define trials
  session_results <- average_spike_area(trial_indices, session[[i]])
  session_results$session <- i  # Add session number
  all_results_list[[i]] <- session_results  # Store results
}

# Combine all session data into one data frame
all_results_df <- do.call(rbind, all_results_list)

# Plot data for all sessions, faceting by session
ggplot(all_results_df, aes(x = brain_area, y = avg_spike_count,  color = trial)) + 
  geom_point(size = 1) +
  stat_summary(fun = mean, geom = "point", size = 3, color = "red") +  # Red points for mean
  facet_wrap(~ session, scales = "free_x", ncol = 6) +  # Facet by session, adjust columns
  theme_minimal() +
  labs(title = "Average Spike Count per Brain Area Across Sessions",
       x = "Brain Area",
       y = "Avg Spike Count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for readability

```
This graph demonstrates the average spike count of each brain area per session across all 18 sessions. 


```{r}
average_spike_area <- function(this_session) {
  # Get the number of trials in the session dynamically
  num_trials <- length(this_session$spks)
  
  # Initialize an empty list to store results
  results_list <- list()
  
  # Loop through each trial
  for (i.t in seq_len(num_trials)) {
    spk.trial <- this_session$spks[[i.t]]
    area <- this_session$brain_area
    spk.count <- apply(spk.trial, 1, sum)
    spk.average <- tapply(spk.count, area, mean)
    
    # Convert to data frame and store trial index
    trial_df <- data.frame(
      trial = i.t,
      brain_area = names(spk.average),
      avg_spike_count = as.numeric(spk.average)
    )
    
    results_list[[i.t]] <- trial_df
  }
  
  # Combine all results into a single data frame
  results_df <- do.call(rbind, results_list)
  
  return(results_df)
}

# Function to process all sessions and account for different trial counts
average_spike_area_all_sessions <- function(sessions) {
  # Initialize an empty list for all session results
  all_results_list <- list()
  
  # Loop through each session
  for (session_idx in seq_along(sessions)) {
    
    this_session <- sessions[[session_idx]]  # ✅ Fix: Use [[ ]] instead of [ ]
    
    # Get the trial data for this session
    session_df <- average_spike_area(this_session)
    
    # Add session index
    session_df$session <- session_idx
    
    # Store in list
    all_results_list[[session_idx]] <- session_df
  }
  
  # Combine all session results into one big data frame
  all_results_df <- do.call(rbind, all_results_list)
  
  return(all_results_df)
}

# ✅ Fix: Use `list()`, not `c()`
#sessionlist <- list(session[[1]], session[[2]], session[[3]])

# ✅ Fix: Use `lapply` for all 18 sessions
sessionlist <- lapply(1:18, function(i) session[[i]])

# Example usage
results_df_allsessions <- average_spike_area_all_sessions(sessionlist)

# Print the first few rows
print(head(results_df_allsessions))
dim(results_df_allsessions)
```

```{r}
ggplot(results_df_allsessions, aes(x=brain_area, y=avg_spike_count, color=factor(session))) + 
  geom_jitter(width = 0.2, height = 0, alpha = 0.5) +  # Jitter to reduce overlap
  stat_summary(fun = mean, geom = "point", size = 5, shape = 18, color = "black") +  # Larger black mean points
  scale_color_viridis_d(option = "plasma") +  # Better color scale
  theme_minimal() +  # Cleaner theme
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +  # Rotate x-axis labels
  labs(title = "Average Spike Count by Brain Area and Session",
       x = "Brain Area",
       y = "Avg Spike Count",
       color = "Session")  # Clearer labels
```
This graph collects the results across all sessions to identify the spike rate of different brain areas. The aggregate data also demonstrates certain brain areas with higher spike counts than other areas. 

Using this data, I will do further investigation to understand whether higher average spike rates in these brain areas have any correlation with response success. I hypothesize currently that higher spike rates may lead to increased response success. 


```{r}
results_df_allsessions$mousename <- rep(0, nrow(results_df_allsessions))
results_df_allsessions$feedback <- rep(0, nrow(results_df_allsessions))
results_df_allsessions$rightcontrast <- rep(0, nrow(results_df_allsessions))
results_df_allsessions$leftcontrast <- rep(0, nrow(results_df_allsessions))
results_df_allsessions$contrastdiff <- rep(0, nrow(results_df_allsessions))

for(i in 1:nrow(results_df_allsessions)){
  sessnum <- as.numeric(results_df_allsessions$session[i])
  results_df_allsessions$mousename[i] <- session[[sessnum]]$mouse_name
  trialnum <-  as.numeric(results_df_allsessions$trial[i])
  results_df_allsessions$feedback[i] <- session[[sessnum]]$feedback_type[[trialnum]]
  results_df_allsessions$rightcontrast[i] <- session[[sessnum]]$contrast_right[[trialnum]]
  results_df_allsessions$leftcontrast[i] <- session[[sessnum]]$contrast_left[[trialnum]]
  results_df_allsessions$contrastdiff[i] <- results_df_allsessions$rightcontrast[i] - results_df_allsessions$leftcontrast[i]
}

head(results_df_allsessions)
```
```{r}
# Filter the data for session 2
session2_data <- results_df_allsessions[results_df_allsessions$session == 5, ]

# If you want to see how spike counts vary by brain area



# Assuming your original plot code is similar to this
ggplot(session2_data, aes(x = trial, y = avg_spike_count, color = as.factor(feedback))) +
  geom_line(alpha = 0.5) +
  facet_wrap(~ brain_area) +
  labs(title = "Average Spike Count by Trial for Each Brain Area in Session 2",
       x = "Trial",
       y = "Average Spike Count") +
  
  # Add smoothed trend lines for each feedback value
  geom_smooth(method = "loess", se = FALSE, linewidth = 1.2) +
  
  # Customize colors and labels
  scale_color_manual(values = c("-1" = "red", "1" = "turquoise"), 
                     name = "feedback") +
  
  theme_minimal() +
  theme(legend.position = "bottom")
```
















```{r}
results_df_allsessions[20000, ]
unique(results_df_allsessions$contrastdiff)


contrastdiff_counts <- results_df_allsessions %>%
  ungroup() %>% 
  mutate(contrastdiff = as.numeric(as.character(contrastdiff))) %>%
  count(contrastdiff) %>%
  mutate(proportion = n / sum(n))

contrastdiff_counts

contrastdiff_counts$sign <- ifelse(contrastdiff_counts$contrastdiff < 0, "negative", 
                       ifelse(contrastdiff_counts$contrastdiff > 0, "positive", "zero"))


print(contrastdiff_counts)





```
This table investigates the number and combination of contrast differences within the total data. The table demonstrates that there is not an equal proportion of each contrast combination within the data. 

```{r}
ggplot(contrastdiff_counts, mapping=aes(x = "", y = proportion, fill = factor(contrastdiff))) + geom_bar(stat = "identity", width = 1) +
  coord_polar(theta = "y") + labs(title = "Pie chart demonstrating proportion of trials with contrast differences", subtitle= "(contrast_right - contrast_left)", fill="Contrast Difference") 
```
The pie chart above further illustrates this point. We see from the pie chart as well as from the table that the value of 0 contrast_diff makes up a much larger proportion of the data than the other contrast_diffs. I calculated the value of "contrast_diff" through the method: contrast_right - contrast_left. Therefore, negative values of contrast_diff indicate that there was a higher contrast_left value than contrast_right, while positive values indicate that there was a higher contrast_right value than contrast left. 

A contrast_diff value of 0 can come from two methods. One scenario is when both left and right contrasts are 0. Another scenario is when left and right contrasts are equal, but non-zero. 

The constrast_diff value of 0 is particularly important for me to consider in regards to prediction power in further steps of this assignment. Given that the mouse's actions in the second case do not have a bearing on the success of the trial, it stands out as different compared to other trials. Thus, including this data may be problematic to overall predictions. 

"When left and right contrasts are equal but non-zero, left or right will be randomly chosen (50%) as the correct choice." 

```{r}
ggplot(contrastdiff_counts, mapping = aes(x=contrastdiff, y=proportion, fill = sign)) + geom_col() + facet_wrap(~abs(contrastdiff))
```




```{r}
zero_contrast_diff <- results_df_allsessions %>%
  filter(contrastdiff == 0) %>%
  ungroup %>% 
  mutate(left_contrast_category = ifelse(leftcontrast != 0, "Non-zero", "Zero")) %>%
  count(left_contrast_category) %>%
  mutate(proportion = n / sum(n))

zero_contrast_diff

```

```{r}
ggplot(zero_contrast_diff, mapping=aes(x = "", y = proportion, fill = factor(left_contrast_category))) + geom_bar(stat = "identity", width = 1) +
  coord_polar(theta = "y") + labs(title = "Contrast_diff = 0 trials", subtitle= "Proportion of trials where left/right contrast do not equal 0", fill="Left Contrast Category") 
```


I chose to investigate the trials where contrast_diff = 0 in more detail, in order to understand how many of these trials involve both left contrast and right contrast being 0 versus the left contrast and right contrast having equivalent but non-zero values. In the second case, if right contrast and left contrast are equal but non-zero, their difference (contrast_diff) will equal 0. I chose to look at the left contrast values, comparing the instances where left contrast = 0 versus instances where left contrast did not equal 0. 

As I mentioned previously, this distinction is important because the mouse's behavior in the first scenario truly yields a success/failure based on the mouse's decision, while the second scenario yields a success/failure randomly. Therefore, I wanted to see what proportion of trials where contrast_diff = 0 came from each scenario. 

The analysis demonstrates that around 80% of trials where contrast_diff = 0 come from the first scenario, where the left and right contrasts were both 0, and the mouse either successfully held the wheel still or incorrectly moved the wheel. 

20% of trials were instances of the left and right contrasts being equal but non-zero. This means that 0.32896102	* 0.1877473 = 0.06176154  (6.17%) of all trials had feedbacks that were randomly assigned, rather than being correlated with the mouse's behavior.  This is a relatively small proportion of the total data, but is important to note when I build my prediction model. 





```{r}
ggplot(results_df_allsessions, aes(x=brain_area, y=avg_spike_count, color=brain_area)) + 
  geom_jitter(width = 0.2, height = 0, alpha = 0.5) +  # Jitter to reduce overlap
  stat_summary(fun = mean, geom = "point", size = 5, shape = 18, color = "black") +  # Larger black mean points
  scale_color_viridis_d(option = "plasma") +  # Better color scale
  theme_minimal() +  # Cleaner theme
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +  # Rotate x-axis labels
  labs(title = "Average Spike Count by Brain Area and Mouse",
       x = "Brain Area",
       y = "Avg Spike Count",
       color = "Session") +  # Clearer labels
  facet_wrap(~mousename)
```
The graph above reiterates the differences in brain areas being triggered in different mice. It specifically demonstrates that the brain areas with highest average spike count may not be the same for each moouse. 


```{r}
common_brain_areas <- results_df_allsessions %>%
  group_by(brain_area) %>%
  summarise(session_count = n_distinct(session)) %>%
  filter(session_count == n_distinct(results_df_allsessions$session)) %>%
  pull(brain_area)

# Print common brain areas
print(common_brain_areas)
```
I wanted to check if there was a certain brain area that was common to all sessions that could be an indicator of success rate by firing. However, there are no brain areas that are common to all 18 sessions. 

```{r}
session_counts_per_mouse <- results_df_allsessions %>%
  group_by(mousename) %>%
  summarise(total_sessions = n_distinct(session))

# Find common brain areas per mouse
common_brain_areas_per_mouse <- results_df_allsessions %>%
  group_by(mousename, brain_area) %>%
  summarise(session_count = n_distinct(session), .groups = "drop") %>%
  inner_join(session_counts_per_mouse, by = "mousename") %>%
  filter(session_count == total_sessions) %>%
 dplyr::select(mousename, brain_area)

# Print results
print(common_brain_areas_per_mouse)
```
I then checked if there were brain areas common to the sessions of a given mouse. There is only one brain area common to mouse Cori and mouse Hench, being "root". The other two mice do not have a common brain area across all of their respective sessions. 

```{r}

ggplot(results_df_allsessions, aes(x = factor(feedback), y = avg_spike_count, fill = factor(feedback))) + 
  geom_point() +  # Boxplot to show distribution
  stat_summary(fun = mean, geom = "point", size = 3, color = "red", shape = 18) +  # Mean points
  facet_wrap(~ brain_area, scales = "free") +  # Separate plots for each brain area
  scale_fill_viridis_d(option = "plasma") +  # Color scale
  theme_minimal() +  # Cleaner theme
  labs(title = "Average Spike Count vs Feedback Type for Each Brain Area",
       x = "Feedback Type",
       y = "Average Spike Count",
       fill = "Feedback") +  
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for readability

```

```{r}

# Compute mean feedback per brain area
feedback_summary <- results_df_allsessions %>%
  group_by(brain_area, feedback) %>%
  summarize(mean_spike = mean(avg_spike_count, na.rm = TRUE), .groups = "drop") %>%
  spread(feedback, mean_spike)  # Convert feedback categories into separate columns

# Compute difference between max and min feedback means
feedback_summary <- feedback_summary %>%
  mutate(feedback_diff = apply(dplyr::select(., -brain_area), 1, function(x) max(x, na.rm = TRUE) - min(x, na.rm = TRUE))) %>% arrange(desc(by = feedback_diff))

# Find the brain area with the largest difference
largest_diff_brain_area <- feedback_summary %>%
  arrange(desc(feedback_diff)) %>%
  head(1)

feedback_summary
print(largest_diff_brain_area)

```
The dataframe above was designed to understand whether there is a difference in feedback type based on brain area. However, there seems to be a weak-to-no trend between the average spike count and feedback type. There isn't a distinctly greater spike count for positive feedback results versus negative feedback results. This dataframe identifies the top brain areas with the greatest differences in spike count. 



```{r}
# Assuming your data is in a dataframe called 'df'
# with columns 'brain_area', '-1', and '1'

# Perform paired t-test
result <- t.test(feedback_summary$`-1`, feedback_summary$`1`, paired = TRUE)

# Print the result
print(result)

# If you want to see the p-value specifically
print(paste("p-value:", result$p.value))
```
I conducted a paired t-test to see if there was a statistically significant difference in average spike count between successful and unsuccessful trials across all brain areas recorded in this dataset. The paired t-test yielded a p-value of 9.564e-09. If we set a threshold of 0.05, we see that the p-value is significant. This indicates that there is a statistically significant difference in average spike count between successful and unsuccessful trials. This tells us that average spike count is an indicator of success/failure, and should be included in our prediction model. 


```{r}
t_test_result <- t.test(avg_spike_count ~ feedback, data = results_df_allsessions, paired = FALSE)
print(t_test_result)
```
I also conducted a t-test on the entire dataset. Here, we also see a statistically difference in the means between the average spike counts of successful versus unsuccessful trials. This corroberates the previous t-test. 







```{r}
feedbacksummary_long <- feedback_summary %>%
  pivot_longer(cols = c(`-1`, `1`), names_to = "feedback", values_to = "value") 


print(feedbacksummary_long)
```








```{r}

ggplot(results_df_allsessions, aes(x = factor(feedback), y = avg_spike_count, fill = factor(feedback))) + 
  geom_point() +  # Boxplot to show distribution
  stat_summary(fun = mean, geom = "point", size = 3, color = "red", shape = 18) +  # Mean points
  facet_wrap(~ mousename, scales = "free") +  # Separate plots for each brain area
  scale_fill_viridis_d(option = "plasma") +  # Color scale
  theme_minimal() +  # Cleaner theme
  labs(title = "Average Spike Count vs Feedback Type for Each Mouse",
       x = "Feedback Type",
       y = "Average Spike Count",
       fill = "Feedback") +  
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for readability

```


```{r}
ggplot(results_df_allsessions, aes(x=session, y=avg_spike_count)) + 
  geom_jitter(width = 0.2, height = 0, alpha = 0.5) +  # Jitter to reduce overlap
  stat_summary(fun = mean, geom = "point", size = 5, shape = 18, color = "red") +  # Larger black mean points
  scale_color_viridis_d(option = "plasma") +  # Better color scale
  theme_minimal() +  # Cleaner theme
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +  # Rotate x-axis labels
  labs(title = "Average Spike Count by Session",
       x = "Session",
       y = "Avg Spike Count")  # Clearer labels
```
Some sessions have a higher average spike count than others. 

```{r}

results_df_allsessions <- results_df_allsessions %>%
  group_by(session) %>%
  mutate(avg_spike_count_session_mean = mean(avg_spike_count, na.rm = TRUE))

results_df_allsessions <- results_df_allsessions %>%
  group_by(session, trial) %>%
  mutate(avg_spike_count_trial_mean = mean(avg_spike_count, na.rm = TRUE))

results_df_allsessions


```


```{r}
results_df_allsessions$feedback <- as.factor(results_df_allsessions$feedback)
results_df_allsessions$contrastdiff <- as.factor(results_df_allsessions$contrastdiff)

# Compute proportions
results_allsessions_summary <- results_df_allsessions %>%
  group_by(contrastdiff, feedback) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(contrastdiff) %>%
  mutate(prop = count / sum(count))  # Normalize by contrastdiff

# Plot using proportions
ggplot(results_allsessions_summary, aes(x = contrastdiff, y = prop, fill = feedback)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "Contrast Difference", y = "Proportion", fill = "Feedback") 

results_brainarea_summary <- results_df_allsessions %>%
  group_by(contrastdiff, feedback, brain_area) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(contrastdiff, brain_area) %>%
  mutate(prop = count / sum(count))  # Normalize by contrastdiff

results_allsessions_summary
results_brainarea_summary 

ggplot(results_brainarea_summary, aes(x = contrastdiff, y = prop, fill = feedback)) +
  geom_bar(stat = "identity", position = "dodge") + facet_wrap(~brain_area) +
  labs(x = "Contrast Difference", y = "Proportion", fill = "Feedback") 

results_mouse_summary <- results_df_allsessions %>%
  group_by(contrastdiff, feedback, mousename) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(contrastdiff) %>%
  mutate(prop = count / sum(count))  # Normalize by contrastdiff

ggplot(results_mouse_summary, aes(x = contrastdiff, y = prop, fill = feedback)) +
  geom_bar(stat = "identity", position = "dodge") + facet_wrap(~mousename) +
  labs(x = "Contrast Difference", y = "Proportion", fill = "Feedback") 



df_diff <- results_allsessions_summary %>%
  group_by(contrastdiff) %>%
  summarise(prop_diff = diff(prop))  # Calculates difference between the two proportions

# Print the new dataframe
print(df_diff)
```
This graph demonstrates that for large contrast trials, such as -1 or 1, there seems to be a larger proportion of successes (1) versus failures compared to low contrast trials (-1). This shows that contrast difference may be a good indicator of success. I will include this variable in my prediction model. 




```{r}
ggplot(df_diff, mapping=aes(x=contrastdiff, y=prop_diff)) + geom_col()
```
This graph demonstrates there is a larger difference between the proportion of successes (1) versus proportion of failures (-1) in groups whith higher absolute contrastdiff values (such as -1 or 1) than ones with low absolute contrastdiff values (0). This shows us that contrast values within a trial seem to be correlated with mouse success. I will plan on including contrast values within my prediction model. 





```{r}
contingency_table <- xtabs(count ~ contrastdiff + feedback, data = results_allsessions_summary)
print(contingency_table)

# Chi-square test of independence
chi_square_result <- chisq.test(contingency_table)
print(chi_square_result)

# If you want to use Fisher's exact test (especially for small counts)
# Note: This might be computationally intensive for large tables
fisher_result <- fisher.test(contingency_table, simulate.p.value = TRUE, B = 10000)
print(fisher_result)

# Visualize the relationship
library(ggplot2)

# Convert to long format for proportion plotting
results_allsessions_summary$prop <- results_allsessions_summary$count / sum(results_allsessions_summary$count)

# Plot the proportions
ggplot(results_allsessions_summary, aes(x = contrastdiff, y = prop, fill = feedback)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Proportion by Contrast Difference and Feedback",
       x = "Contrast Difference",
       y = "Proportion") +
  theme_minimal()
```
I wanted to conduct a statistical test to see whether there was a statistically significant correlation between contrast difference and feedback type. I conducted a Chi Square Test of Independence and a Fisher Test of Indepenence. Both tests yielded p-values under 0.05, indicating that tere is a relationship between contrast difference and feedback type. Thus, this gives me evidence to include contrast difference, or a similar interaction variable between left and right contrast, into my prediction model. 


```{r}
ggplot(results_allsessions_summary, aes(x = contrastdiff, y = prop, fill = feedback)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "Contrast Difference", y = "Proportion", fill = "Feedback")  + facet_wrap(~ feedback)

ggplot(results_df_allsessions, mapping=aes(x=contrastdiff, y=avg_spike_count)) + geom_jitter() + stat_summary(fun = mean, geom = "point", size = 1, shape = 18, color = "red") + facet_wrap(~ brain_area)


avg_spike_count_per_contrastdiff <- results_df_allsessions %>%
  group_by(contrastdiff, feedback) %>%
  summarise(mean_spike_count = mean(avg_spike_count, na.rm = TRUE)) 

# Print the new dataframe
print(avg_spike_count_per_contrastdiff)

```
```{r}
ggplot(avg_spike_count_per_contrastdiff, aes(x = contrastdiff, y = mean_spike_count, group = feedback, color = feedback)) +
  geom_line() +
  geom_point() +
  facet_wrap(~ feedback) +  # Optional: facet by feedback for separate panels
  labs(title = "Mean Spike Count vs. Contrast Difference",
       x = "Contrast Difference",
       y = "Mean Spike Count")
```

```{r}
ggplot(results_df_allsessions, mapping=aes(x=trial, y=avg_spike_count_trial_mean)) + geom_line() + facet_wrap(~session)
```



```{r}
avg_spike_count_diff <- avg_spike_count_per_contrastdiff %>%
  group_by(contrastdiff) %>%
  summarise(mean_spike_diff = diff(mean_spike_count)) 

avg_spike_count_diff

ggplot(avg_spike_count_diff, mapping=aes(x=contrastdiff, y=mean_spike_diff)) + geom_col()
```


I now want to conduct PCA on the average spike data for each session. I want to do this in order to reduce the number of dimensions needed to account for brain area spike count data.

```{r}
# Load required libraries
library(dplyr)
library(tidyr)
library(ggplot2)
library(stats)

results_df_allsessions <- results_df_allsessions %>% ungroup()

# Create empty lists to store results
pca_results <- list()
summary_results <- list()
optimal_pcs <- list()

# Run PCA for each session separately
session_ids <- unique(results_df_allsessions$session)

# Create a dataframe to store optimal PC counts
optimal_pc_df <- data.frame(session = integer(), optimal_pcs = integer())

for (session_id in session_ids) {
  # Filter data for this session
  session_data <- results_df_allsessions %>% 
    dplyr::filter(session == session_id)
  
  # Reshape for PCA - create wide format with brain areas as columns
  session_wide <- session_data %>%
    dplyr::select(trial, mousename, brain_area, avg_spike_count) %>%
    tidyr::pivot_wider(
      id_cols = c(trial, mousename),
      names_from = brain_area,
      values_from = avg_spike_count
    )
  
  # Extract brain area data for PCA
  brain_areas <- session_wide %>% dplyr::select(-trial, -mousename)
  
  # Handle NAs if any
  brain_areas <- brain_areas %>% 
    mutate(across(everything(), ~ifelse(is.na(.), mean(., na.rm = TRUE), .)))
  
  # Skip if insufficient data
  if (nrow(brain_areas) < 2 || ncol(brain_areas) < 2) {
    cat("Skipping session", session_id, "due to insufficient data\n")
    next
  }
  
  # Run PCA
  pca_result <- prcomp(brain_areas, scale = TRUE)
  
  # Store PCA result
  pca_results[[as.character(session_id)]] <- pca_result
  
  # Calculate variance explained
  var_explained <- pca_result$sdev^2 / sum(pca_result$sdev^2)
  cumulative_var <- cumsum(var_explained)
  
  # Store summary
  summary_results[[as.character(session_id)]] <- data.frame(
    PC = 1:length(var_explained),
    Variance = var_explained,
    Cumulative = cumulative_var
  )
  
  # Determine optimal number of PCs based on 80% variance explained
  optimal_pc_count <- min(which(cumulative_var >= 0.8))
  optimal_pcs[[as.character(session_id)]] <- optimal_pc_count
  
  # Add to optimal PC dataframe
  optimal_pc_df <- rbind(optimal_pc_df, 
                         data.frame(session = session_id, 
                                   optimal_pcs = optimal_pc_count))
  
  # Print results for this session
  cat("Session", session_id, ":\n")
  cat("- Number of PCs for 80% variance:", optimal_pc_count, "\n")
  cat("- Variance explained by first 3 PCs:", round(cumulative_var[3] * 100, 2), "%\n\n")
  
  # Create scree plot for this session
  scree_plot <- ggplot(summary_results[[as.character(session_id)]], 
                      aes(x = PC, y = Variance * 100)) +
    geom_col(fill = "steelblue") +
    geom_line() +
    geom_point() +
    labs(title = paste("Scree Plot - Session", session_id),
         x = "Principal Component",
         y = "Variance Explained (%)") +
    theme_minimal()
  
  # Optionally print the plot (comment out if you don't want it printed)
  print(scree_plot)
}

# Now create the final dataset with PCA components
# Start with the original data
result_df <- df

# For each session, get the PC scores and join them
for (session_id in names(pca_results)) {
  # Get the session data that was used for PCA
  session_data <- results_df_allsessions %>% 
    dplyr::filter(session == as.numeric(session_id))
  
  # Reshape for PCA again to get the same row order
  session_wide <- session_data %>%
    dplyr::select(trial, mousename, brain_area, avg_spike_count) %>%
    tidyr::pivot_wider(
      id_cols = c(trial, mousename),
      names_from = brain_area,
      values_from = avg_spike_count
    )
  
  # Get optimal PC count for this session
  n_pcs <- optimal_pcs[[session_id]]
  
  # Get PC scores
  pca_scores <- as.data.frame(pca_results[[session_id]]$x[, 1:n_pcs, drop = FALSE])
  
  # Add identifying information that matches the wide format
  pca_scores$trial <- session_wide$trial
  pca_scores$mousename <- session_wide$mousename
  pca_scores$session <- as.numeric(session_id)
  
  # Create unique identifiers to join on
  pca_scores$join_id <- paste(pca_scores$session, pca_scores$trial, pca_scores$mousename, sep = "_")
  
  # Reshape to long format for easier joining
  pca_scores_long <- pca_scores %>%
    tidyr::pivot_longer(
      cols = starts_with("PC"),
      names_to = "pc_name",
      values_to = "pc_value"
    )
  
  # Create a new dataframe with trial, mousename, brain_area combinations
  pc_data <- session_data %>%
    dplyr::select(trial, session, mousename, brain_area) %>%
    dplyr::mutate(join_id = paste(session, trial, mousename, sep = "_"))
  
  # Join with PC scores
  pc_data <- pc_data %>%
    dplyr::left_join(pca_scores_long, by = "join_id")
  
  # Create column names that combine PC and session
  pc_data <- pc_data %>%
    dplyr::mutate(pc_session_col = paste0(pc_name, "_session", session))
  
  # Select only needed columns and spread the PC values
  pc_data_wide <- pc_data %>%
    dplyr::select(trial, session, mousename, brain_area, pc_session_col, pc_value) %>%
    tidyr::pivot_wider(
      id_cols = c(trial, session, mousename, brain_area),
      names_from = pc_session_col,
      values_from = pc_value
    )
  
  # Join with the result dataframe
  result_df <- result_df %>%
    dplyr::left_join(pc_data_wide, by = c("trial", "session", "mousename", "brain_area"))
}

# View the first few rows of the result
#head(result_df)

# Create a summary of the optimal PCs for each session
#print(optimal_pc_df)
```



```{r}
# Try PCA on just one session
session_id <- unique(results_df_allsessions$session)[1]  # Use the first session

# Filter data for this session
session_data <- results_df_allsessions[results_df_allsessions$session == session_id, ]
print(paste("Analyzing session", session_id, "with", nrow(session_data), "rows"))

# Show what data we're working with
print(head(session_data))

# Create unique trial/mousename combinations
trial_mouse_combinations <- unique(paste(session_data$trial, session_data$mousename, sep="_"))
print(paste("Found", length(trial_mouse_combinations), "unique trial/mouse combinations"))

# Create unique brain areas
brain_areas <- unique(session_data$brain_area)
print(paste("Found", length(brain_areas), "unique brain areas"))

# Create table showing counts for verification
counts <- table(session_data$brain_area)
print(counts)

# Try a simpler approach with reshape
library(reshape2)
wide_data <- dcast(session_data, trial + mousename ~ brain_area, 
                   value.var = "avg_spike_count", fun.aggregate = mean)
print(head(wide_data))

# Extract just brain area columns
brain_cols <- setdiff(colnames(wide_data), c("trial", "mousename"))
pca_data <- wide_data[, brain_cols]

# Handle NAs if any
na_counts <- colSums(is.na(pca_data))
if(any(na_counts > 0)) {
  print("Found NAs, replacing with column means")
  for(col in colnames(pca_data)) {
    if(any(is.na(pca_data[[col]]))) {
      pca_data[[col]][is.na(pca_data[[col]])] <- mean(pca_data[[col]], na.rm=TRUE)
    }
  }
}

# Run PCA
pca_result <- prcomp(pca_data, scale = TRUE)

# Calculate variance explained
var_explained <- pca_result$sdev^2 / sum(pca_result$sdev^2)
cumulative_var <- cumsum(var_explained)

# Find number of PCs for 80% and 90% variance
pcs_80 <- min(which(cumulative_var >= 0.8))
if (is.infinite(pcs_80)) pcs_80 <- length(var_explained)

# Add the code to find PCs for 90% variance
pcs_90 <- min(which(cumulative_var >= 0.9))
if (is.infinite(pcs_90)) pcs_90 <- length(var_explained)

# Print summary
print(summary(pca_result))
print(paste("Variance explained by first 3 PCs:", 
            round(cumulative_var[min(3, length(cumulative_var))] * 100, 2), "%"))
print(paste("Number of PCs needed for 80% variance:", pcs_80))
print(paste("Number of PCs needed for 90% variance:", pcs_90))

# Create a data frame showing variance explained by each PC
variance_df <- data.frame(
  PC = 1:length(var_explained),
  Variance = var_explained * 100,  # Convert to percentage
  CumulativeVariance = cumulative_var * 100  # Convert to percentage
)

# Print the variance table
print(variance_df)
```






```{r}
# Load required libraries
library(reshape2)

# Function to analyze a single session
analyze_session <- function(data_df, session_id) {
  # Filter data for this session
  session_data <- data_df[data_df$session == session_id, ]
  
  # Skip if no data
  if (nrow(session_data) == 0) {
    return(list(success = FALSE, message = "No data for this session"))
  }
  
  # Reshape to wide format
  tryCatch({
    wide_data <- dcast(session_data, trial + mousename ~ brain_area, 
                     value.var = "avg_spike_count", fun.aggregate = mean)
    
    # Extract brain area columns
    brain_cols <- setdiff(colnames(wide_data), c("trial", "mousename"))
    
    # Skip if not enough brain areas
    if (length(brain_cols) < 2) {
      return(list(success = FALSE, 
                  message = paste("Only", length(brain_cols), "brain areas")))
    }
    
    pca_data <- wide_data[, brain_cols]
    
    # Handle NAs
    for(col in brain_cols) {
      if(any(is.na(pca_data[[col]]))) {
        pca_data[[col]][is.na(pca_data[[col]])] <- mean(pca_data[[col]], na.rm=TRUE)
      }
    }
    
    # Run PCA
    pca_result <- prcomp(pca_data, scale = TRUE)
    
    # Calculate variance explained
    var_explained <- pca_result$sdev^2 / sum(pca_result$sdev^2)
    cumulative_var <- cumsum(var_explained)
    
    # Find PCs for different variance levels
    pcs_80 <- min(which(cumulative_var >= 0.8))
    if (is.infinite(pcs_80)) pcs_80 <- length(var_explained)
    
    pcs_90 <- min(which(cumulative_var >= 0.9))
    if (is.infinite(pcs_90)) pcs_90 <- length(var_explained)
    
    pcs_99 <- min(which(cumulative_var >= 1.0))
    if (is.infinite(pcs_99)) pcs_99 <- length(var_explained)
    
    return(list(
      success = TRUE,
      session_id = session_id,
      n_trials = nrow(pca_data),
      n_brain_areas = length(brain_cols),
      pcs_80 = pcs_80,
      pcs_90 = pcs_90,
      pcs_99 = pcs_99,
      var_3pcs = cumulative_var[min(3, length(cumulative_var))] * 100,
      total_pcs = length(var_explained)
    ))
  }, error = function(e) {
    return(list(success = FALSE, message = paste("Error:", e$message)))
  })
}

# Get all sessions
all_sessions <- sort(unique(results_df_allsessions$session))

# Analyze all sessions
results <- list()
for (session_id in all_sessions) {
  cat("Analyzing session", session_id, "\n")
  result <- analyze_session(results_df_allsessions, session_id)
  results[[as.character(session_id)]] <- result
  
  if (result$success) {
    cat("  Success! Session", session_id, "needs", result$pcs_99, "PCs for 99% variance\n")
    cat("  (", result$n_trials, "trials,", result$n_brain_areas, "brain areas)\n\n")
  } else {
    cat("  Failed:", result$message, "\n\n")
  }
}

# Create summary table
summary_table <- data.frame(
  Session = integer(),
  NumTrials = integer(),
  NumBrainAreas = integer(), 
  PCs_80_percent = integer(),
  PCs_90_percent = integer(),
  PCs_99_percent = integer(),
  Variance_3PCs = numeric(),
  TotalPCs = integer()
)

for (session_id in names(results)) {
  result <- results[[session_id]]
  if (result$success) {
    summary_table <- rbind(summary_table, data.frame(
      Session = as.integer(session_id),
      NumTrials = result$n_trials,
      NumBrainAreas = result$n_brain_areas,
      PCs_80_percent = result$pcs_80,
      PCs_90_percent = result$pcs_90,
      PCs_99_percent = result$pcs_99,
      Variance_3PCs = round(result$var_3pcs, 2),
      TotalPCs = result$total_pcs
    ))
  }
}

# Sort by session and print
summary_table <- summary_table[order(summary_table$Session), ]
print(summary_table)

# Calculate some summary statistics
if (nrow(summary_table) > 0) {
  cat("\nSummary Statistics:\n")
  cat("Average PCs needed for 80% variance:", mean(summary_table$PCs_80_percent), "\n")
  cat("Average PCs needed for 90% variance:", mean(summary_table$PCs_90_percent), "\n")
  cat("Average PCs needed for 99% variance:", mean(summary_table$PCs_99_percent), "\n")
  cat("Average percentage of variance explained by top 3 PCs:", mean(summary_table$Variance_3PCs), "%\n")
}
```






```{r}
# First check if spks has elements
if (length(session[[1]]$spks) == 0) {
  cat("The spks list is empty\n")
} else {
  # Check if any elements have length > 0
  element_lengths <- sapply(session[[1]]$spks, length)
  
  if (all(element_lengths == 0)) {
    cat("All elements in spks are empty\n")
  } else {
    cat("Found non-empty elements in spks. Continuing with matrix creation...\n")
    
    # Proceed with the original code
    n_trials <- length(session[[1]]$spks)
    max_length <- max(element_lengths)
    
    cat("Number of trials:", n_trials, "\n")
    cat("Max length of spike vectors:", max_length, "\n")
    
    spike_matrix <- matrix(0, nrow = n_trials, ncol = max_length)
    
    # Fill the matrix with spike data - more robustly
    for (i in 1:n_trials) {
      if (length(session[[1]]$spks[[i]]) > 0) {
        spk_data <- session[[1]]$spks[[i]]
        spike_matrix[i, 1:length(spk_data)] <- spk_data
      }
    }
    
    # Check if the matrix has any non-zero values
    if (all(spike_matrix == 0)) {
      cat("Warning: Spike matrix contains only zeros\n")
    } else {
      # Check column variances
      col_vars <- apply(spike_matrix, 2, var)
      non_zero_vars <- sum(col_vars > 0)
      
      cat("Columns with non-zero variance:", non_zero_vars, "out of", ncol(spike_matrix), "\n")
      
      if (non_zero_vars >= 2) {
        # Keep only columns with variance
        spike_matrix_filtered <- spike_matrix[, col_vars > 0, drop = FALSE]
        cat("Running PCA on filtered matrix with dimensions:", dim(spike_matrix_filtered), "\n")
        
        # Run PCA
        pca_result <- prcomp(spike_matrix_filtered, scale = TRUE)
        print(summary(pca_result))
      } else {
        cat("Not enough columns with variance for PCA\n")
      }
    }
  }
}
```



```{r}
# Create a function to analyze a single session
analyze_session_pca <- function(session_data, session_id) {
  # First check if spks has elements
  if (length(session_data$spks) == 0) {
    cat("Session", session_id, ": The spks list is empty\n")
    return(list(success = FALSE, message = "The spks list is empty"))
  } 
  
  # Check if any elements have length > 0
  element_lengths <- sapply(session_data$spks, length)
  
  if (all(element_lengths == 0)) {
    cat("Session", session_id, ": All elements in spks are empty\n")
    return(list(success = FALSE, message = "All elements in spks are empty"))
  } 
  
  cat("Session", session_id, ": Found non-empty elements in spks. Continuing with matrix creation...\n")
  
  # Proceed with the original code
  n_trials <- length(session_data$spks)
  max_length <- max(element_lengths)
  
  cat("  Number of trials:", n_trials, "\n")
  cat("  Max length of spike vectors:", max_length, "\n")
  
  spike_matrix <- matrix(0, nrow = n_trials, ncol = max_length)
  
  # Fill the matrix with spike data - more robustly
  for (i in 1:n_trials) {
    if (length(session_data$spks[[i]]) > 0) {
      spk_data <- session_data$spks[[i]]
      spike_matrix[i, 1:length(spk_data)] <- spk_data
    }
  }
  
  # Check if the matrix has any non-zero values
  if (all(spike_matrix == 0)) {
    cat("  Warning: Spike matrix contains only zeros\n")
    return(list(success = FALSE, message = "Spike matrix contains only zeros"))
  }
  
  # Check column variances
  col_vars <- apply(spike_matrix, 2, var)
  non_zero_vars <- sum(col_vars > 0)
  
  cat("  Columns with non-zero variance:", non_zero_vars, "out of", ncol(spike_matrix), "\n")
  
  if (non_zero_vars >= 2) {
    # Keep only columns with variance
    spike_matrix_filtered <- spike_matrix[, col_vars > 0, drop = FALSE]
    cat("  Running PCA on filtered matrix with dimensions:", dim(spike_matrix_filtered), "\n")
    
    # Run PCA
    tryCatch({
      pca_result <- prcomp(spike_matrix_filtered, scale = TRUE)
      
      # Calculate variance explained
      var_explained <- pca_result$sdev^2 / sum(pca_result$sdev^2)
      cumulative_var <- cumsum(var_explained)
      
      # Number of PCs for different thresholds
      pcs_90 <- min(which(cumulative_var >= 0.9))
      if (is.infinite(pcs_90)) pcs_90 <- length(var_explained)
      
      pcs_95 <- min(which(cumulative_var >= 0.95))
      if (is.infinite(pcs_95)) pcs_95 <- length(var_explained)
      
      pcs_99 <- min(which(cumulative_var >= 0.99))
      if (is.infinite(pcs_99)) pcs_99 <- length(var_explained)
      
      pcs_100 <- length(var_explained)
      
      cat("  PCs for 90% variance:", pcs_90, "\n")
      cat("  PCs for 95% variance:", pcs_95, "\n")
      cat("  PCs for 99% variance:", pcs_99, "\n")
      cat("  PCs for 100% variance:", pcs_100, "\n")
      
      return(list(
        success = TRUE,
        session_id = session_id,
        n_trials = n_trials,
        n_features = non_zero_vars,
        pcs_90 = pcs_90,
        pcs_95 = pcs_95,
        pcs_99 = pcs_99,
        pcs_100 = pcs_100,
        var_explained = var_explained
      ))
    }, error = function(e) {
      cat("  Error in PCA:", e$message, "\n")
      return(list(success = FALSE, message = paste("Error in PCA:", e$message)))
    })
  } else {
    cat("  Not enough columns with variance for PCA\n")
    return(list(success = FALSE, message = "Not enough columns with variance for PCA"))
  }
}

# Process all 18 sessions
results <- list()
for (i in 1:18) {
  cat("\nAnalyzing session", i, "\n")
  
  # Check if session exists
  if (exists(paste0("session", i)) || exists("session") && length(session) >= i) {
    # Get the correct session data
    if (exists(paste0("session", i))) {
      session_data <- get(paste0("session", i))
    } else {
      session_data <- session[[i]]
    }
    
    # Analyze the session
    result <- analyze_session_pca(session_data, i)
    results[[as.character(i)]] <- result
  } else {
    cat("Session", i, "data not found\n")
    results[[as.character(i)]] <- list(success = FALSE, message = "Session data not found")
  }
}

# Create summary table
summary_table <- data.frame(
  Session = integer(),
  NumTrials = integer(),
  NumFeatures = integer(),
  PCs_90_percent = integer(),
  PCs_95_percent = integer(),
  PCs_99_percent = integer(),
  PCs_100_percent = integer()
)

for (session_id in names(results)) {
  result <- results[[session_id]]
  if (result$success) {
    summary_table <- rbind(summary_table, data.frame(
      Session = as.integer(session_id),
      NumTrials = result$n_trials,
      NumFeatures = result$n_features,
      PCs_90_percent = result$pcs_90,
      PCs_95_percent = result$pcs_95,
      PCs_99_percent = result$pcs_99,
      PCs_100_percent = result$pcs_100
    ))
  } else {
    # Add a row with NA values for failed sessions
    summary_table <- rbind(summary_table, data.frame(
      Session = as.integer(session_id),
      NumTrials = NA,
      NumFeatures = NA,
      PCs_90_percent = NA,
      PCs_95_percent = NA,
      PCs_99_percent = NA,
      PCs_100_percent = NA
    ))
  }
}

# Sort by session and print
summary_table <- summary_table[order(summary_table$Session), ]
print(summary_table)

# Calculate percentage of features needed
summary_table$Percent_PCs_90 <- (summary_table$PCs_90_percent / summary_table$NumFeatures) * 100
summary_table$Percent_PCs_95 <- (summary_table$PCs_95_percent / summary_table$NumFeatures) * 100
summary_table$Percent_PCs_99 <- (summary_table$PCs_99_percent / summary_table$NumFeatures) * 100
summary_table$Percent_PCs_100 <- (summary_table$PCs_100_percent / summary_table$NumFeatures) * 100

# Print average percentages (excluding NAs)
cat("\nAverage percentage of features needed:\n")
cat("For 90% variance:", mean(summary_table$Percent_PCs_90, na.rm = TRUE), "%\n")
cat("For 95% variance:", mean(summary_table$Percent_PCs_95, na.rm = TRUE), "%\n")
cat("For 99% variance:", mean(summary_table$Percent_PCs_99, na.rm = TRUE), "%\n")
cat("For 100% variance:", mean(summary_table$Percent_PCs_100, na.rm = TRUE), "%\n")
```

```{r}
summary_table
```



```{r}
# Load required libraries
library(boot)

# Function to calculate PCs needed for 90% variance in bootstrap samples
bootstrap_pca <- function(session_data, session_id, n_bootstrap = 1000) {
  # First check if spks has elements
  if (length(session_data$spks) == 0 || all(sapply(session_data$spks, length) == 0)) {
    return(list(success = FALSE, message = "No valid spike data"))
  }
  
  # Create spike matrix as before
  element_lengths <- sapply(session_data$spks, length)
  n_trials <- length(session_data$spks)
  max_length <- max(element_lengths)
  
  spike_matrix <- matrix(0, nrow = n_trials, ncol = max_length)
  
  for (i in 1:n_trials) {
    if (length(session_data$spks[[i]]) > 0) {
      spk_data <- session_data$spks[[i]]
      spike_matrix[i, 1:length(spk_data)] <- spk_data
    }
  }
  
  # Check for constant columns
  col_vars <- apply(spike_matrix, 2, var)
  non_zero_vars <- col_vars > 0
  
  if (sum(non_zero_vars) < 2) {
    return(list(success = FALSE, message = "Not enough variable columns for PCA"))
  }
  
  # Keep only columns with variance
  spike_matrix_filtered <- spike_matrix[, non_zero_vars, drop = FALSE]
  
  # Define bootstrap function to calculate PCs for 90% variance
  calc_pcs_90 <- function(data, indices) {
    # Resample data
    resampled_data <- data[indices, ]
    
    # Run PCA
    pca_result <- tryCatch({
      prcomp(resampled_data, scale = TRUE)
    }, error = function(e) {
      return(NULL)
    })
    
    if (is.null(pca_result)) {
      return(NA)
    }
    
    # Calculate variance explained
    var_explained <- pca_result$sdev^2 / sum(pca_result$sdev^2)
    cumulative_var <- cumsum(var_explained)
    
    # PCs for 90% variance
    pcs_90 <- min(which(cumulative_var >= 0.9))
    if (is.infinite(pcs_90)) pcs_90 <- length(var_explained)
    
    return(pcs_90)
  }
  
  # Perform bootstrap
  boot_results <- boot(data = spike_matrix_filtered, 
                       statistic = calc_pcs_90, 
                       R = n_bootstrap)
  
  # Calculate statistics
  mean_pcs <- mean(boot_results$t, na.rm = TRUE)
  median_pcs <- median(boot_results$t, na.rm = TRUE)
  ci_pcs <- boot.ci(boot_results, type = "perc", conf = 0.95)
  
  # Return results
  return(list(
    success = TRUE,
    session_id = session_id,
    n_trials = n_trials,
    n_features = sum(non_zero_vars),
    mean_pcs_90 = mean_pcs,
    median_pcs_90 = median_pcs,
    ci_low = if (!is.null(ci_pcs)) ci_pcs$percent[4] else NA,
    ci_high = if (!is.null(ci_pcs)) ci_pcs$percent[5] else NA,
    boot_samples = boot_results$t
  ))
}

# Process all sessions
bootstrap_results <- list()
for (i in 1:18) {
  cat("\nBootstrapping session", i, "\n")
  
  # Check if session exists
  if (exists(paste0("session", i)) || exists("session") && length(session) >= i) {
    # Get the correct session data
    if (exists(paste0("session", i))) {
      session_data <- get(paste0("session", i))
    } else {
      session_data <- session[[i]]
    }
    
    # Bootstrap the session
    result <- bootstrap_pca(session_data, i)
    bootstrap_results[[as.character(i)]] <- result
    
    if (result$success) {
      cat("  Success! For session", i, ":\n")
      cat("  Mean PCs for 90% variance:", result$mean_pcs_90, "\n")
      cat("  Median PCs for 90% variance:", result$median_pcs_90, "\n")
      cat("  95% CI:", result$ci_low, "-", result$ci_high, "\n")
    } else {
      cat("  Failed:", result$message, "\n")
    }
  } else {
    cat("Session", i, "data not found\n")
    bootstrap_results[[as.character(i)]] <- list(success = FALSE, message = "Session data not found")
  }
}

# Create summary table
bootstrap_summary <- data.frame(
  Session = integer(),
  NumTrials = integer(),
  NumFeatures = integer(),
  Mean_PCs_90 = numeric(),
  Median_PCs_90 = numeric(),
  CI_Low = numeric(),
  CI_High = numeric()
)

for (session_id in names(bootstrap_results)) {
  result <- bootstrap_results[[session_id]]
  if (result$success) {
    bootstrap_summary <- rbind(bootstrap_summary, data.frame(
      Session = as.integer(session_id),
      NumTrials = result$n_trials,
      NumFeatures = result$n_features,
      Mean_PCs_90 = result$mean_pcs_90,
      Median_PCs_90 = result$median_pcs_90,
      CI_Low = result$ci_low,
      CI_High = result$ci_high
    ))
  } else {
    # Add a row with NA values for failed sessions
    bootstrap_summary <- rbind(bootstrap_summary, data.frame(
      Session = as.integer(session_id),
      NumTrials = NA,
      NumFeatures = NA,
      Mean_PCs_90 = NA,
      Median_PCs_90 = NA,
      CI_Low = NA,
      CI_High = NA
    ))
  }
}

# Sort by session and print
bootstrap_summary <- bootstrap_summary[order(bootstrap_summary$Session), ]
print(bootstrap_summary)

# Calculate an optimal PC count across all sessions
successful_sessions <- bootstrap_summary[!is.na(bootstrap_summary$Mean_PCs_90), ]

if (nrow(successful_sessions) > 0) {
  # Find the maximum of the upper CI bounds
  max_ci_high <- max(successful_sessions$CI_High, na.rm = TRUE)
  
  # Find the median of the median PC counts
  median_of_medians <- median(successful_sessions$Median_PCs_90, na.rm = TRUE)
  
  # Calculate a robust number that works for all sessions
  robust_pc_count <- ceiling(max_ci_high)
  
  cat("\nRecommended consistent PC count for all sessions:", robust_pc_count, "\n")
  cat("This covers the 95% confidence interval upper bound for all sessions\n")
  cat("Median of median PC counts across sessions:", median_of_medians, "\n")
  
  # Calculate what percentage of features this represents for each session
  bootstrap_summary$Percent_Features <- (robust_pc_count / bootstrap_summary$NumFeatures) * 100
  
  cat("\nVariance explained with", robust_pc_count, "PCs for each session:\n")
  
  # Calculate variance explained by the robust PC count for each session
  for (i in 1:nrow(bootstrap_summary)) {
    session_id <- bootstrap_summary$Session[i]
    if (exists(paste0("session", session_id)) || exists("session") && length(session) >= session_id) {
      # Get correct session data
      if (exists(paste0("session", session_id))) {
        session_data <- get(paste0("session", session_id))
      } else {
        session_data <- session[[session_id]]
      }
      
      # Skip if no valid data
      if (length(session_data$spks) == 0 || all(sapply(session_data$spks, length) == 0)) {
        cat("  Session", session_id, ": No valid data\n")
        next
      }
      
      # Create spike matrix
      element_lengths <- sapply(session_data$spks, length)
      spike_matrix <- matrix(0, nrow = length(session_data$spks), ncol = max(element_lengths))
      
      for (j in 1:length(session_data$spks)) {
        if (length(session_data$spks[[j]]) > 0) {
          spk_data <- session_data$spks[[j]]
          spike_matrix[j, 1:length(spk_data)] <- spk_data
        }
      }
      
      # Filter columns
      col_vars <- apply(spike_matrix, 2, var)
      non_zero_vars <- col_vars > 0
      
      if (sum(non_zero_vars) < 2) {
        cat("  Session", session_id, ": Not enough variable columns\n")
        next
      }
      
      spike_matrix_filtered <- spike_matrix[, non_zero_vars, drop = FALSE]
      
      # Run PCA
      tryCatch({
        pca_result <- prcomp(spike_matrix_filtered, scale = TRUE)
        var_explained <- pca_result$sdev^2 / sum(pca_result$sdev^2)
        cumulative_var <- cumsum(var_explained)
        
        # Get variance explained by robust PC count
        pc_count <- min(robust_pc_count, length(cumulative_var))
        var_explained_robust <- cumulative_var[pc_count] * 100
        
        cat("  Session", session_id, ": ", round(var_explained_robust, 2), "% variance explained with", pc_count, "PCs\n")
        
        # Update the variance explained in the summary
        bootstrap_summary$Variance_Explained[i] <- var_explained_robust
      }, error = function(e) {
        cat("  Session", session_id, ": Error in PCA:", e$message, "\n")
      })
    }
  }
}
```





******************************************



I will continue to explore the relationship between feedback type and the brain areas of the neurons that spike in each trial. 

I will also see if the same neurons that spike during a successful feedback hold true with different mice. 

I will see if there is a difference in neuron type or feedback success in right versus left contrast. 


is there a time point where there are more spikes?
average_spk_count = average across all 40 time points i think 

Part 2. Data integration. Using the findings in Part 1, we will propose an approach to combine data across trials by (i) extracting the shared patters across sessions and/or (ii) addressing the differences between sessions. The goal of this part is to enable the borrowing of information across sessions to enhance the prediction performance in Part 3. 




```{r, eval=FALSE}
results_df_allsessions
```



```{r, eval=FALSE}


#need to make a dataset without session because it is specifically grouping by session 
set.seed(123)
results_df_allsessions$feedback <- as.numeric(results_df_allsessions$feedback)
results_df_allsessions$contrastdiff <- as.numeric(results_df_allsessions$contrastdiff)


results_df_allsessions_pca <- results_df_allsessions %>% ungroup() %>%dplyr::select(avg_spike_count, contrastdiff, leftcontrast, rightcontrast) %>% prcomp(center = TRUE, scale = TRUE)



ggplot(results_df_allsessions, aes(x=results_df_allsessions_pca$x[,1], y=results_df_allsessions_pca$x[,2], color=as.factor(
  feedback))) + geom_jitter(alpha=0.5)



plot(results_df_allsessions_pca , type='l')

```







One problem I noticed with this PCA analysis is that I cannot include the categorical variable of "mousename". Based on my exploratory data analysis, I concluded that the success rate differs between mice, and therefore the variable of "mousename" contributes to the variance of the data and should be included in a prediction model. As I conduct further analysis, I will note that "mousename" data is not included in the resulting PCA data.

```{r, eval=FALSE}
# First, extract the PCA results into a tidy format
pca_data <- as.data.frame(results_df_allsessions_pca$x[,1:2])
colnames(pca_data) <- c("PC1", "PC2")

# Add the original feedback column for coloring
pca_data$feedback <- results_df_allsessions$feedback

# Create a more informative PCA plot
ggplot(pca_data, aes(x = PC1, y = PC2, color = as.factor(feedback))) +
  geom_point(alpha = 0.5) +
  stat_ellipse(aes(group = as.factor(feedback)), type = "norm") +
  labs(
    title = "PCA of Neural Data",
    subtitle = paste0("PC1 explains ", round(100*results_df_allsessions_pca$sdev[1]^2/sum(results_df_allsessions_pca$sdev^2), 1), "% variance"),
    x = paste0("PC1 (", round(100*results_df_allsessions_pca$sdev[1]^2/sum(results_df_allsessions_pca$sdev^2), 1), "%)"),
    y = paste0("PC2 (", round(100*results_df_allsessions_pca$sdev[2]^2/sum(results_df_allsessions_pca$sdev^2), 1), "%)"),
    color = "Feedback"
  ) +
  theme_minimal() +
  scale_color_brewer(palette = "Set1")

# To visualize the variable loadings
library(factoextra)
fviz_pca_biplot(results_df_allsessions_pca,
                geom.ind = "point",
                col.ind = as.factor(results_df_allsessions$feedback),
                addEllipses = TRUE,
                label = "var",
                col.var = "black",
                repel = TRUE)
```

```{r}
# Extract the variance information
std_devs <- results_df_allsessions_pca$sdev
variance <- std_devs^2
prop_variance <- variance / sum(variance)
cum_variance <- cumsum(prop_variance)

# Create a data frame for the table
pca_variance_table <- data.frame(
  PC = 1:length(std_devs),
  StandardDeviation = std_devs,
  Variance = variance,
  ProportionOfVariance = prop_variance,
  CumulativeVariance = cum_variance
)

# Format the table with percentages for easier reading
pca_variance_table$ProportionOfVariance <- sprintf("%.2f%%", pca_variance_table$ProportionOfVariance * 100)
pca_variance_table$CumulativeVariance <- sprintf("%.2f%%", pca_variance_table$CumulativeVariance * 100)

# Print the table
print(pca_variance_table)
```

Now, I will try K means clustering: 


```{r, eval=FALSE}
library(tidyverse)

# 1. Prepare the data for clustering -dplyr::select just a few important variables
# Based on your EDA, these were the most informative features
kmeans_data <- results_df_allsessions %>% ungroup() %>%
 dplyr::select(avg_spike_count, leftcontrast, rightcontrast) 

# 2. Take a sample to avoid memory issues (adjust sample size as needed)
set.seed(123)  # For reproducibility
sample_size <- 5000
sampled_data <- kmeans_data %>% sample_n(min(nrow(kmeans_data), sample_size))

# 3. Scale the data (important for k-means)
scaled_data <- scale(sampled_data)

# 4. Run k-means with k=4 (you can change this value)
set.seed(123)
km <- kmeans(scaled_data, centers = 4, nstart = 25)

# 5. Examine the cluster sizes
table(km$cluster)

# 6. Look at cluster centers (on the scaled data)
km$centers

# 7. Examine cluster centers (transformed back to original scale)
centers_orig <- t(t(km$centers) * attr(scaled_data, "scaled:scale") + 
                   attr(scaled_data, "scaled:center"))
centers_orig <- as.data.frame(centers_orig)
colnames(centers_orig) <- colnames(sampled_data)
centers_orig$cluster <- 1:nrow(centers_orig)
print(centers_orig)

# 8. Add cluster assignments to the sampled data
sampled_results <- bind_cols(
  sampled_data, 
  cluster = factor(km$cluster)
)

# 9. Create a simple plot of clusters
ggplot(sampled_results, aes(x = avg_spike_count, y = leftcontrast, color = cluster)) +
  geom_point(alpha = 0.7) +
  labs(title = "K-means Clustering Results",
       x = "Average Spike Count",
       y = "Left Contrast") +
  theme_minimal()

# 10. Create a different view of the clusters
ggplot(sampled_results, aes(x = leftcontrast, y = rightcontrast, color = cluster)) +
  geom_point(alpha = 0.7) +
  labs(title = "K-means Clustering by Contrast Values",
       x = "Left Contrast",
       y = "Right Contrast") +
  theme_minimal()

# 11. If you want to analyze cluster relationship with feedback,
# you'll need to include that information first
# Get the feedback data for the samples
if(exists("results_df_allsessions")) {
  # Use the same sampled rows for feedback data
  set.seed(123)
  sampled_indices <- sample(1:nrow(kmeans_data), min(nrow(kmeans_data), sample_size))
  
  # Add feedback data to the sampled results
  sampled_results$feedback <- results_df_allsessions$feedback[sampled_indices]
  
  # Now plot the relationship between clusters and feedback
  ggplot(sampled_results, aes(x = cluster, fill = factor(feedback))) +
    geom_bar(position = "fill") +
    labs(title = "Feedback Distribution by Cluster",
         x = "Cluster",
         y = "Proportion",
         fill = "Feedback") +
    theme_minimal()
  
  # Calculate success rate by cluster
  sampled_results %>%
    group_by(cluster) %>%
    summarize(
      count = n(),
      success_rate = mean(feedback == 1)
    )
}

```


```{r, eval=FALSE}
results_df_allsessions
```

```{r, eval=FALSE}
colnames(results_df_allsessions)
```



```{r, eval=FALSE}
library(tidyverse)

# 1. Prepare the data for clustering -dplyr::select just a few important variables
# Based on your EDA, these were the most informative features
kmeans_data <- results_df_allsessions %>% ungroup() %>%
   dplyr::select(avg_spike_count, contrastdiff) 

# 2. Take a sample to avoid memory issues (adjust sample size as needed)
set.seed(123)  # For reproducibility
sample_size <- 5000
sampled_data <- kmeans_data %>% sample_n(min(nrow(kmeans_data), sample_size))

# 3. Scale the data (important for k-means)
scaled_data <- scale(sampled_data)

# 4. Run k-means with k=4 (you can change this value)
set.seed(123)
km <- kmeans(scaled_data, centers = 4, nstart = 25)

# 5. Examine the cluster sizes
table(km$cluster)

# 6. Look at cluster centers (on the scaled data)
km$centers

# 7. Examine cluster centers (transformed back to original scale)
centers_orig <- t(t(km$centers) * attr(scaled_data, "scaled:scale") + 
                   attr(scaled_data, "scaled:center"))
centers_orig <- as.data.frame(centers_orig)
colnames(centers_orig) <- colnames(sampled_data)
centers_orig$cluster <- 1:nrow(centers_orig)
print(centers_orig)

# 8. Add cluster assignments to the sampled data
sampled_results <- bind_cols(
  sampled_data, 
  cluster = factor(km$cluster)
)

# 9. Create a simple plot of clusters
ggplot(sampled_results, aes(x = avg_spike_count, y = contrastdiff, color = cluster)) +
  geom_point(alpha = 0.7) +
  labs(title = "K-means Clustering Results",
       x = "Average Spike Count",
       y = "Left Contrast") +
  theme_minimal()

# 10. Create a different view of the clusters
ggplot(sampled_results, aes(x = leftcontrast, y = rightcontrast, color = cluster)) +
  geom_point(alpha = 0.7) +
  labs(title = "K-means Clustering by Contrast Values",
       x = "Left Contrast",
       y = "Right Contrast") +
  theme_minimal()

# 11. If you want to analyze cluster relationship with feedback,
# you'll need to include that information first
# Get the feedback data for the samples
if(exists("results_df_allsessions")) {
  # Use the same sampled rows for feedback data
  set.seed(123)
  sampled_indices <- sample(1:nrow(kmeans_data), min(nrow(kmeans_data), sample_size))
  
  # Add feedback data to the sampled results
  sampled_results$feedback <- results_df_allsessions$feedback[sampled_indices]
  
  # Now plot the relationship between clusters and feedback
  ggplot(sampled_results, aes(x = cluster, fill = factor(feedback))) +
    geom_bar(position = "fill") +
    labs(title = "Feedback Distribution by Cluster",
         x = "Cluster",
         y = "Proportion",
         fill = "Feedback") +
    theme_minimal()
  
  # Calculate success rate by cluster
  sampled_results %>%
    group_by(cluster) %>%
    summarize(
      count = n(),
      success_rate = mean(feedback == 1)
    )
}

```































```{r, eval=FALSE}
library(tidyverse)

# 1. Prepare the data for clustering
kmeans_data <- results_df_allsessions %>% ungroup() %>%
  dplyr::select(avg_spike_count, leftcontrast, rightcontrast) 

# 2. Take a sample to avoid memory issues
set.seed(123)  # For reproducibility
sample_size <- 5000
sampled_data <- kmeans_data %>% sample_n(min(nrow(kmeans_data), sample_size))

# 3. Scale the data
scaled_data <- scale(sampled_data)

# 4. Elbow Method - calculate total within-cluster sum of squares for different k values
wss <- numeric(10)  # We'll try k from 1 to 10
for (i in 1:10) {
  # Print progress
  cat("Running k-means with k =", i, "\n")
  
  # Run k-means
  km <- kmeans(scaled_data, centers = i, nstart = 25)
  
  # Store the within-cluster sum of squares
  wss[i] <- km$tot.withinss
}

# 5. Create a data frame for plotting
elbow_df <- data.frame(k = 1:10, wss = wss)

# 6. Print the WSS values for each k
print(elbow_df)

# 7. Plot the elbow curve
ggplot(elbow_df, aes(x = k, y = wss)) +
  geom_line() +
  geom_point(size = 3) +
  labs(title = "Elbow Method for Optimal Number of Clusters",
       x = "Number of Clusters (k)",
       y = "Total Within-Cluster Sum of Squares") +
  theme_minimal()

# 8. How to interpret:
# Look for the "elbow" point in the curve - where adding more clusters
# doesn't significantly reduce the within-cluster sum of squares.
# This point represents a good balance between cluster fit and model complexity.
```

This graph was created to help me understand what the optimal number of clusters would be given all of the data for the variables I considered to be indicators of success or failure (average spike count, left contrast, and right contrast). I chose to do K means clustering as another form of dimensional reduction to compare to the earlier PCA approach. 

We see an elbow in this graph at k = 4. Up until k = 4, each addition of a cluster reduced the within-cluster sum of squares. However, at k = 4, the rate of reduction became less steep, and each addition of a cluster did not affect the within-cluster sum of squares to a great degree. Based on this, I will continue my analysis with k = 4 clusters. 


Part 3. Model training and prediction. Finally, we will build a prediction model to predict the outcome (i.e., feedback types). The performance will be evaluated on two test sets of 100 trials randomlydplyr::selected from Session 1 and Session 18, respectively. The test sets will be released on the day of submission when you need to evaluate the performance of your model. 


To continue with the prediction modeling, I will take a sample of the data initially to use to create prediction models and compare them with one another to find the most appropriate one for this dataset. I chose to use 10,000 trials of the total data to do this. I will randomlydplyr::select 10,000 trials from the total data to ensure that there is no bias based on confounding factors such as "session", as just taking the first 10,000 trials of the total dataset may not capture the full variability of the dataset. 

Here, I also wanted to balance the number of succcess trials and failure trials, so that the training datasets would not be biased towards successes or failures. 


```{r, eval=FALSE}
library(tidyverse)
library(caret)  # For cross-validation functions

# 1. Set a seed for reproducibility
set.seed(123)

# 2. First, check the original class distribution
cat("Original class distribution in results_df_allsessions:\n")
table(results_df_allsessions$feedback)

# 3. Create a balanced sample with equal numbers of each class
# Find the minimum count to determine how many we can sample from each class
feedback_counts <- table(results_df_allsessions$feedback)
cat("Class counts in original data:", feedback_counts, "\n")

# Make sure we can get 5000 of each class (for 10,000 total)
samples_per_class <- 5000
if (any(feedback_counts < samples_per_class)) {
  # If we can't get 5000 of each, use the maximum possible
  samples_per_class <- min(feedback_counts)
  cat("Warning: Reduced sample size to", samples_per_class * 2, 
      "total because minimum class count is", samples_per_class, "\n")
}

# Sample equally from each class
balanced_data <- results_df_allsessions %>%
  ungroup() %>%
  # Group by feedback to sample separately from each class
  group_by(feedback) %>%
  # Sample equal numbers from each class
  sample_n(size = samples_per_class, replace = FALSE) %>%
  ungroup() %>%
  #dplyr::select only the columns you need for modeling
 dplyr::select(feedback, avg_spike_count, leftcontrast, rightcontrast) %>%
  # Make sure feedback is a factor for classification
  mutate(feedback = factor(feedback)) %>%
  # Remove any rows with NA values
  na.omit()

# Verify the balanced sample
cat("Balanced sample class distribution:\n")
table(balanced_data$feedback)

# 4. Create stratified folds for cross-validation
# createFolds ensures proportional class representation in each fold
set.seed(456)  # Different seed for fold creation
k_folds <- 4
fold_indices <- createFolds(balanced_data$feedback, k = k_folds, list = TRUE, returnTrain = FALSE)

# 5. Create individual dataframes for each train and test set
# Test set 1
test_set_1 <- balanced_data[fold_indices[[1]], ]
# Train set 1 (all rows except those in test_set_1)
train_set_1 <- balanced_data[-fold_indices[[1]], ]

# Test set 2
test_set_2 <- balanced_data[fold_indices[[2]], ]
# Train set 2
train_set_2 <- balanced_data[-fold_indices[[2]], ]

# Test set 3
test_set_3 <- balanced_data[fold_indices[[3]], ]
# Train set 3
train_set_3 <- balanced_data[-fold_indices[[3]], ]

# Test set 4
test_set_4 <- balanced_data[fold_indices[[4]], ]
# Train set 4
train_set_4 <- balanced_data[-fold_indices[[4]], ]

# 6. Print information about each fold to verify balance
cat("Fold 1:\n")
cat("Training set:", nrow(train_set_1), "rows\n")
cat("Test set:", nrow(test_set_1), "rows\n")
cat("Training class balance:", table(train_set_1$feedback), "\n")
cat("Test class balance:", table(test_set_1$feedback), "\n\n")

cat("Fold 2:\n")
cat("Training set:", nrow(train_set_2), "rows\n")
cat("Test set:", nrow(test_set_2), "rows\n")
cat("Training class balance:", table(train_set_2$feedback), "\n")
cat("Test class balance:", table(test_set_2$feedback), "\n\n")

cat("Fold 3:\n")
cat("Training set:", nrow(train_set_3), "rows\n")
cat("Test set:", nrow(test_set_3), "rows\n")
cat("Training class balance:", table(train_set_3$feedback), "\n")
cat("Test class balance:", table(test_set_3$feedback), "\n\n")

cat("Fold 4:\n")
cat("Training set:", nrow(train_set_4), "rows\n")
cat("Test set:", nrow(test_set_4), "rows\n")
cat("Training class balance:", table(train_set_4$feedback), "\n")
cat("Test class balance:", table(test_set_4$feedback), "\n\n")

# 7. Calculate class proportions to verify near-perfect balance
proportion_check <- function(data_set, name) {
  counts <- table(data_set$feedback)
  proportions <- prop.table(counts)
  cat(name, "class proportions:", round(proportions, 3), "\n")
}

cat("Class proportion verification:\n")
proportion_check(balanced_data, "Overall sample")
proportion_check(train_set_1, "Train set 1")
proportion_check(test_set_1, "Test set 1")
```






```{r, eval=FALSE}
library(tidyverse)
library(caret)

# Define the basic train_and_evaluate function
train_and_evaluate <- function(train_data, test_data) {
  # Train the model
  model <- glm(feedback ~ ., data = train_data, family = "binomial")
  
  # Make predictions
  predictions_prob <- predict(model, newdata = test_data, type = "response")
  predictions <- ifelse(predictions_prob > 0.5, "1", "-1")
  predictions <- factor(predictions, levels = levels(test_data$feedback))
  
  # Calculate accuracy
  accuracy <- mean(predictions == test_data$feedback)
  
  # Create confusion matrix
  cm <- confusionMatrix(predictions, test_data$feedback)
  
  return(list(
    model = model,
    accuracy = accuracy,
    confusion_matrix = cm
  ))
}

# 1. Assuming your existing dataframes are named:
# train_set_1, test_set_1, train_set_2, test_set_2, etc.

# 2. Combine all training data for clustering
# This gives us a larger dataset to identify more stable clusters
all_training_data <- rbind(train_set_1, train_set_2, train_set_3, train_set_4)

# 3. Perform k-means clustering on the combined training data
set.seed(123)  # For reproducibility
features_for_clustering <- all_training_data %>%
 dplyr::select(avg_spike_count, leftcontrast, rightcontrast)

# Scale the data for better clustering
scaled_features <- scale(features_for_clustering)


# Perform k-means clustering with optimal k (4 in this example)
k <- 4  # From your elbow method analysis
km_result <- kmeans(scaled_features, centers = k, nstart = 25)

# 4. Function to assign clusters to new data
assign_clusters <- function(new_data, km_model, original_center, original_scale) {
  #dplyr::select the same features used for clustering
  features <- new_data %>%
   dplyr::select(avg_spike_count, leftcontrast, rightcontrast)
  
  # Scale using the same parameters as the original data
  scaled_data <- scale(features, 
                      center = original_center,
                      scale = original_scale)
  
  # Calculate distance to each cluster center
  n_obs <- nrow(scaled_data)
  n_centers <- nrow(km_model$centers)
  distances <- matrix(0, nrow = n_obs, ncol = n_centers)
  
  for (i in 1:n_obs) {
    for (j in 1:n_centers) {
      distances[i, j] <- sqrt(sum((scaled_data[i,] - km_model$centers[j,])^2))
    }
  }
  
  # Assign each observation to the nearest cluster
  clusters <- apply(distances, 1, which.min)
  
  return(list(
    cluster = clusters,
    distances = distances
  ))
}

# 5. Add cluster assignments to all datasets
# For training sets
train_assignments_1 <- assign_clusters(
  train_set_1, 
  km_result, 
  attr(scaled_features, "scaled:center"),
  attr(scaled_features, "scaled:scale")
)
train_set_1_enhanced <- train_set_1 %>%
  mutate(cluster = factor(train_assignments_1$cluster))

train_assignments_2 <- assign_clusters(
  train_set_2, 
  km_result, 
  attr(scaled_features, "scaled:center"),
  attr(scaled_features, "scaled:scale")
)
train_set_2_enhanced <- train_set_2 %>%
  mutate(cluster = factor(train_assignments_2$cluster))

train_assignments_3 <- assign_clusters(
  train_set_3, 
  km_result, 
  attr(scaled_features, "scaled:center"),
  attr(scaled_features, "scaled:scale")
)
train_set_3_enhanced <- train_set_3 %>%
  mutate(cluster = factor(train_assignments_3$cluster))

train_assignments_4 <- assign_clusters(
  train_set_4, 
  km_result, 
  attr(scaled_features, "scaled:center"),
  attr(scaled_features, "scaled:scale")
)
train_set_4_enhanced <- train_set_4 %>%
  mutate(cluster = factor(train_assignments_4$cluster))

# For test sets
test_assignments_1 <- assign_clusters(
  test_set_1, 
  km_result, 
  attr(scaled_features, "scaled:center"),
  attr(scaled_features, "scaled:scale")
)
test_set_1_enhanced <- test_set_1 %>%
  mutate(cluster = factor(test_assignments_1$cluster))

test_assignments_2 <- assign_clusters(
  test_set_2, 
  km_result, 
  attr(scaled_features, "scaled:center"),
  attr(scaled_features, "scaled:scale")
)
test_set_2_enhanced <- test_set_2 %>%
  mutate(cluster = factor(test_assignments_2$cluster))

test_assignments_3 <- assign_clusters(
  test_set_3, 
  km_result, 
  attr(scaled_features, "scaled:center"),
  attr(scaled_features, "scaled:scale")
)
test_set_3_enhanced <- test_set_3 %>%
  mutate(cluster = factor(test_assignments_3$cluster))

test_assignments_4 <- assign_clusters(
  test_set_4, 
  km_result, 
  attr(scaled_features, "scaled:center"),
  attr(scaled_features, "scaled:scale")
)
test_set_4_enhanced <- test_set_4 %>%
  mutate(cluster = factor(test_assignments_4$cluster))

# 6. Compare models with and without clustering information (using fold 1)
# Model without cluster information
cat("Evaluating model WITHOUT clustering information:\n")
result_without_cluster <- train_and_evaluate(train_set_1, test_set_1)
cat("Accuracy without cluster:", round(result_without_cluster$accuracy * 100, 2), "%\n\n")

# Model with cluster information
cat("Evaluating model WITH clustering information:\n")
result_with_cluster <- train_and_evaluate(train_set_1_enhanced, test_set_1_enhanced)
cat("Accuracy with cluster:", round(result_with_cluster$accuracy * 100, 2), "%\n\n")

# 7. Create distance features
# Function to convert distances to features
distances_to_features <- function(distances) {
  distance_df <- as.data.frame(distances)
  colnames(distance_df) <- paste0("dist_to_cluster_", 1:ncol(distances))
  return(distance_df)
}

# Add distance features to train and test sets
train_distances_1 <- distances_to_features(train_assignments_1$distances)
train_set_1_with_distances <- cbind(train_set_1, train_distances_1)

test_distances_1 <- distances_to_features(test_assignments_1$distances)
test_set_1_with_distances <- cbind(test_set_1, test_distances_1)

# Train and evaluate model with distance features
cat("Evaluating model with DISTANCE features:\n")
result_with_distances <- train_and_evaluate(train_set_1_with_distances, test_set_1_with_distances)
cat("Accuracy with distance features:", round(result_with_distances$accuracy * 100, 2), "%\n\n")

# 8. Evaluate all 4 folds with the enhanced approach
evaluate_all_folds <- function() {
  results <- list()
  
  # Fold 1
  results[[1]] <- train_and_evaluate(train_set_1_enhanced, test_set_1_enhanced)
  
  # Fold 2
  results[[2]] <- train_and_evaluate(train_set_2_enhanced, test_set_2_enhanced)
  
  # Fold 3
  results[[3]] <- train_and_evaluate(train_set_3_enhanced, test_set_3_enhanced)
  
  # Fold 4
  results[[4]] <- train_and_evaluate(train_set_4_enhanced, test_set_4_enhanced)
  
  # Calculate overall accuracy
  accuracies <- sapply(results, function(x) x$accuracy)
  mean_accuracy <- mean(accuracies)
  
  # Print results
  cat("Results for all folds with cluster feature:\n")
  for (i in 1:4) {
    cat(sprintf("Fold %d accuracy: %.2f%%\n", i, accuracies[i] * 100))
  }
  cat(sprintf("\nMean accuracy across all folds: %.2f%%\n", mean_accuracy * 100))
  
  return(results)
}

# Run evaluation on all folds
all_fold_results <- evaluate_all_folds()

# 9. Visualize the importance of features (including cluster)
if (requireNamespace("randomForest", quietly = TRUE)) {
  library(randomForest)
  
  # Train a random forest model
  rf_model <- randomForest(feedback ~ ., data = train_set_1_enhanced, importance = TRUE)
  
  # Get variable importance
  importance_df <- as.data.frame(importance(rf_model))
  importance_df$Variable <- rownames(importance_df)
  
  # Plot variable importance
  ggplot(importance_df, aes(x = reorder(Variable, MeanDecreaseGini), y = MeanDecreaseGini)) +
    geom_col() +
    coord_flip() +
    labs(title = "Variable Importance (including cluster)",
         x = "Variable",
         y = "Mean Decrease in Gini") +
    theme_minimal()
}
```
I compared the pediction power of 








```{r, eval=FALSE}
library(randomForest)
library(ggplot2)

# Train a random forest model
rf_model <- randomForest(feedback ~ ., data = train_set_1_enhanced, importance = TRUE)

# Get variable importance
importance_df <- as.data.frame(importance(rf_model))
importance_df$Variable <- rownames(importance_df)

# Plot variable importance
ggplot(importance_df, aes(x = reorder(Variable, MeanDecreaseGini), y = MeanDecreaseGini)) +
  geom_col() +
  coord_flip() +
  labs(title = "Variable Importance (including cluster)",
       x = "Variable",
       y = "Mean Decrease in Gini") +
  theme_minimal()
```



I will now try a linear model: 
```{r}
library(tidyverse)
library(caret)
library(broom)  # For tidy model output

# Define the function to train and evaluate linear models
train_and_evaluate_linear <- function(train_data, test_data) {
  # Ensure feedback is numeric (-1 and 1) for linear model
  train_data$feedback_numeric <- as.numeric(as.character(train_data$feedback))
  test_data$feedback_numeric <- as.numeric(as.character(test_data$feedback))
  
  # Train the linear model
  model <- lm(feedback_numeric ~ ., data = train_data %>%dplyr::select(-feedback))
  
  # Make predictions
  predictions <- predict(model, newdata = test_data)
  
  # Convert predictions back to classification
  predicted_classes <- ifelse(predictions > 0, 1, -1)
  
  # Calculate accuracy
  accuracy <- mean(predicted_classes == test_data$feedback_numeric)
  
  # Create confusion matrix
  cm <- confusionMatrix(
    factor(predicted_classes, levels = c(-1, 1)),
    factor(test_data$feedback_numeric, levels = c(-1, 1))
  )
  
  return(list(
    model = model,
    predictions = predictions,
    predicted_classes = predicted_classes,
    accuracy = accuracy,
    confusion_matrix = cm,
    model_summary = summary(model)
  ))
}

# 1. Assuming your existing dataframes are already defined:
# train_set_1, test_set_1, train_set_2, test_set_2, etc.
# with the enhanced versions: train_set_1_enhanced, test_set_1_enhanced, etc.

# 2. Compare linear models with and without clustering information (using fold 1)
# Model without cluster information
cat("Evaluating LINEAR model WITHOUT clustering information:\n")
linear_result_without_cluster <- train_and_evaluate_linear(train_set_1, test_set_1)
cat("Accuracy without cluster:", round(linear_result_without_cluster$accuracy * 100, 2), "%\n\n")

# Print model summary
cat("Model summary (without clustering):\n")
print(linear_result_without_cluster$model_summary)

# Model with cluster information
cat("\nEvaluating LINEAR model WITH clustering information:\n")
linear_result_with_cluster <- train_and_evaluate_linear(train_set_1_enhanced, test_set_1_enhanced)
cat("Accuracy with cluster:", round(linear_result_with_cluster$accuracy * 100, 2), "%\n\n")

# Print model summary
cat("Model summary (with clustering):\n")
print(linear_result_with_cluster$model_summary)

# 3. Evaluate model with distance features
cat("\nEvaluating LINEAR model with DISTANCE features:\n")
linear_result_with_distances <- train_and_evaluate_linear(
  train_set_1_with_distances, 
  test_set_1_with_distances
)
cat("Accuracy with distance features:", round(linear_result_with_distances$accuracy * 100, 2), "%\n\n")

# 4. Evaluate all 4 folds with linear models
evaluate_all_folds_linear <- function() {
  results <- list()
  
  # Fold 1
  results[[1]] <- train_and_evaluate_linear(train_set_1_enhanced, test_set_1_enhanced)
  
  # Fold 2
  results[[2]] <- train_and_evaluate_linear(train_set_2_enhanced, test_set_2_enhanced)
  
  # Fold 3
  results[[3]] <- train_and_evaluate_linear(train_set_3_enhanced, test_set_3_enhanced)
  
  # Fold 4
  results[[4]] <- train_and_evaluate_linear(train_set_4_enhanced, test_set_4_enhanced)
  
  # Calculate overall accuracy
  accuracies <- sapply(results, function(x) x$accuracy)
  mean_accuracy <- mean(accuracies)
  
  # Print results
  cat("Results for all folds with LINEAR model (with clusters):\n")
  for (i in 1:4) {
    cat(sprintf("Fold %d accuracy: %.2f%%\n", i, accuracies[i] * 100))
  }
  cat(sprintf("\nMean accuracy across all folds: %.2f%%\n", mean_accuracy * 100))
  
  return(results)
}

# Run evaluation on all folds
all_fold_linear_results <- evaluate_all_folds_linear()

# 5. Examine coefficient significance from the linear model
# Get tidy summary of the model
if (requireNamespace("broom", quietly = TRUE)) {
  model_coefficients <- tidy(linear_result_with_cluster$model)
  
  # Sort by absolute value of estimate
  model_coefficients <- model_coefficients %>%
    mutate(abs_estimate = abs(estimate)) %>%
    arrange(desc(abs_estimate))
  
  print(model_coefficients)
  
  # Create a plot of coefficient estimates
  ggplot(model_coefficients %>% filter(term != "(Intercept)"), 
         aes(x = reorder(term, abs_estimate), y = estimate)) +
    geom_col() +
    geom_errorbar(aes(ymin = estimate - std.error, ymax = estimate + std.error), width = 0.2) +
    coord_flip() +
    labs(title = "Linear Model Coefficient Estimates",
         x = "Variable",
         y = "Coefficient Estimate") +
    theme_minimal()
}

# 6. Additional analysis: R-squared values to compare model fit
r_squared_without_cluster <- summary(linear_result_without_cluster$model)$r.squared
r_squared_with_cluster <- summary(linear_result_with_cluster$model)$r.squared
r_squared_with_distances <- summary(linear_result_with_distances$model)$r.squared

cat("\nR-squared comparison:\n")
cat("Linear model without cluster:", round(r_squared_without_cluster, 4), "\n")
cat("Linear model with cluster:", round(r_squared_with_cluster, 4), "\n")
cat("Linear model with distances:", round(r_squared_with_distances, 4), "\n")

# 7. Create an interaction plot to visualize how cluster membership affects the relationship
# between avg_spike_count and feedback
ggplot(train_set_1_enhanced, aes(x = avg_spike_count, y = as.numeric(as.character(feedback)), 
                                 color = cluster, group = cluster)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "lm", se = TRUE) +
  labs(title = "Relationship between Spike Count and Feedback by Cluster",
       x = "Average Spike Count",
       y = "Feedback (-1 to 1)",
       color = "Cluster") +
  theme_minimal()
```

I will now try xgboost:

```{r, eval=FALSE}
library(tidyverse)
library(caret)
library(xgboost)  # Make sure this is installed with install.packages("xgboost") if needed

# Define the function to train and evaluate XGBoost models
train_and_evaluate_xgb <- function(train_data, test_data, nrounds = 100) {
  # Prepare data for XGBoost
  # First, convert categorical variables to numeric
  train_data_xgb <- train_data %>% 
    mutate(across(where(is.factor), as.numeric)) %>%
    mutate(feedback_numeric = as.numeric(as.character(feedback)))
  
  test_data_xgb <- test_data %>% 
    mutate(across(where(is.factor), as.numeric)) %>%
    mutate(feedback_numeric = as.numeric(as.character(feedback)))
  
  # Create XGBoost DMatrix objects
  dtrain <- xgb.DMatrix(
    data = as.matrix(train_data_xgb %>%dplyr::select(-feedback, -feedback_numeric)),
    label = ifelse(train_data_xgb$feedback_numeric == 1, 1, 0)  # Convert to 0-1 for XGBoost
  )
  
  dtest <- xgb.DMatrix(
    data = as.matrix(test_data_xgb %>%dplyr::select(-feedback, -feedback_numeric)),
    label = ifelse(test_data_xgb$feedback_numeric == 1, 1, 0)
  )
  
  # Set XGBoost parameters
  params <- list(
    objective = "binary:logistic",
    eval_metric = "error",
    eta = 0.1,
    max_depth = 6,
    subsample = 0.8,
    colsample_bytree = 0.8
  )
  
  # Train the model
  xgb_model <- xgb.train(
    params = params,
    data = dtrain,
    nrounds = nrounds,
    watchlist = list(train = dtrain, test = dtest),
    verbose = 0  # Set to 1 to see training progress
  )
  
  # Make predictions
  predictions_prob <- predict(xgb_model, dtest)
  predictions <- ifelse(predictions_prob > 0.5, 1, -1)
  
  # Calculate accuracy
  accuracy <- mean(predictions == test_data_xgb$feedback_numeric)
  
  # Create confusion matrix
  cm <- confusionMatrix(
    factor(predictions, levels = c(-1, 1)),
    factor(test_data_xgb$feedback_numeric, levels = c(-1, 1))
  )
  
  # Get feature importance
  importance <- xgb.importance(
    feature_names = colnames(train_data_xgb %>%dplyr::select(-feedback, -feedback_numeric)),
    model = xgb_model
  )
  
  return(list(
    model = xgb_model,
    predictions = predictions,
    accuracy = accuracy,
    confusion_matrix = cm,
    importance = importance
  ))
}

# 1. Assuming your existing dataframes are already defined:
# train_set_1, test_set_1, train_set_2, test_set_2, etc.
# with the enhanced versions: train_set_1_enhanced, test_set_1_enhanced, etc.

# 2. Compare XGBoost models with and without clustering information (using fold 1)
# Set seed for reproducibility
set.seed(123)

# Model without cluster information
cat("Evaluating XGBoost model WITHOUT clustering information:\n")
xgb_result_without_cluster <- train_and_evaluate_xgb(train_set_1, test_set_1)
cat("Accuracy without cluster:", round(xgb_result_without_cluster$accuracy * 100, 2), "%\n\n")

# Print confusion matrix
print(xgb_result_without_cluster$confusion_matrix$table)

# Model with cluster information
cat("\nEvaluating XGBoost model WITH clustering information:\n")
xgb_result_with_cluster <- train_and_evaluate_xgb(train_set_1_enhanced, test_set_1_enhanced)
cat("Accuracy with cluster:", round(xgb_result_with_cluster$accuracy * 100, 2), "%\n\n")

# Print confusion matrix
print(xgb_result_with_cluster$confusion_matrix$table)

# 3. Evaluate model with distance features
cat("\nEvaluating XGBoost model with DISTANCE features:\n")
xgb_result_with_distances <- train_and_evaluate_xgb(
  train_set_1_with_distances, 
  test_set_1_with_distances
)
cat("Accuracy with distance features:", round(xgb_result_with_distances$accuracy * 100, 2), "%\n\n")

# Print confusion matrix
print(xgb_result_with_distances$confusion_matrix$table)

# 4. Evaluate all 4 folds with XGBoost
evaluate_all_folds_xgb <- function() {
  results <- list()
  
  # Fold 1
  results[[1]] <- train_and_evaluate_xgb(train_set_1_enhanced, test_set_1_enhanced)
  
  # Fold 2
  results[[2]] <- train_and_evaluate_xgb(train_set_2_enhanced, test_set_2_enhanced)
  
  # Fold 3
  results[[3]] <- train_and_evaluate_xgb(train_set_3_enhanced, test_set_3_enhanced)
  
  # Fold 4
  results[[4]] <- train_and_evaluate_xgb(train_set_4_enhanced, test_set_4_enhanced)
  
  # Calculate overall accuracy
  accuracies <- sapply(results, function(x) x$accuracy)
  mean_accuracy <- mean(accuracies)
  
  # Print results
  cat("Results for all folds with XGBoost model (with clusters):\n")
  for (i in 1:4) {
    cat(sprintf("Fold %d accuracy: %.2f%%\n", i, accuracies[i] * 100))
  }
  cat(sprintf("\nMean accuracy across all folds: %.2f%%\n", mean_accuracy * 100))
  
  return(results)
}

# Run evaluation on all folds
all_fold_xgb_results <- evaluate_all_folds_xgb()

# 5. Plot feature importance from the XGBoost model
# For model with cluster
importance_with_cluster <- xgb_result_with_cluster$importance
if (nrow(importance_with_cluster) > 0) {
  ggplot(importance_with_cluster, aes(x = reorder(Feature, Gain), y = Gain)) +
    geom_col() +
    coord_flip() +
    labs(title = "XGBoost Feature Importance (with cluster)",
         x = "Feature",
         y = "Gain") +
    theme_minimal()
}

# 6. Hyperparameter Tuning for XGBoost (Optional - can be time-consuming)
# This section uses a simplified grid search for demonstration
tune_xgboost <- function(train_data, test_data) {
  # Define parameter grid
  param_grid <- expand.grid(
    eta = c(0.01, 0.1, 0.3),
    max_depth = c(3, 6, 9),
    nrounds = c(50, 100, 200)
  )
  
  # Initialize results
  results <- list()
  
  cat("Starting hyperparameter tuning for XGBoost (this may take a while)...\n")
  
  # Loop through parameter combinations
  for (i in 1:nrow(param_grid)) {
    # Extract parameters
    eta_val <- param_grid$eta[i]
    max_depth_val <- param_grid$max_depth[i]
    nrounds_val <- param_grid$nrounds[i]
    
    # Prepare data for XGBoost
    train_data_xgb <- train_data %>% 
      mutate(across(where(is.factor), as.numeric)) %>%
      mutate(feedback_numeric = as.numeric(as.character(feedback)))
    
    test_data_xgb <- test_data %>% 
      mutate(across(where(is.factor), as.numeric)) %>%
      mutate(feedback_numeric = as.numeric(as.character(feedback)))
    
    # Create DMatrix objects
    dtrain <- xgb.DMatrix(
      data = as.matrix(train_data_xgb %>%dplyr::select(-feedback, -feedback_numeric)),
      label = ifelse(train_data_xgb$feedback_numeric == 1, 1, 0)
    )
    
    dtest <- xgb.DMatrix(
      data = as.matrix(test_data_xgb %>%dplyr::select(-feedback, -feedback_numeric)),
      label = ifelse(test_data_xgb$feedback_numeric == 1, 1, 0)
    )
    
    # Set parameters
    params <- list(
      objective = "binary:logistic",
      eval_metric = "error",
      eta = eta_val,
      max_depth = max_depth_val,
      subsample = 0.8,
      colsample_bytree = 0.8
    )
    
    # Train model
    xgb_model <- xgb.train(
      params = params,
      data = dtrain,
      nrounds = nrounds_val,
      watchlist = list(train = dtrain, test = dtest),
      verbose = 0
    )
    
    # Evaluate
    predictions_prob <- predict(xgb_model, dtest)
    predictions <- ifelse(predictions_prob > 0.5, 1, -1)
    accuracy <- mean(predictions == test_data_xgb$feedback_numeric)
    
    # Store results
    results[[i]] <- list(
      params = list(
        eta = eta_val,
        max_depth = max_depth_val,
        nrounds = nrounds_val
      ),
      accuracy = accuracy
    )
    
    cat(sprintf("Params %d/%d: eta=%.2f, max_depth=%d, nrounds=%d, accuracy=%.4f\n", 
                i, nrow(param_grid), eta_val, max_depth_val, nrounds_val, accuracy))
  }
  
  # Find best parameters
  accuracies <- sapply(results, function(x) x$accuracy)
  best_idx <- which.max(accuracies)
  best_params <- results[[best_idx]]$params
  best_accuracy <- accuracies[best_idx]
  
  cat("\nBest parameters:\n")
  cat(sprintf("eta: %.2f\n", best_params$eta))
  cat(sprintf("max_depth: %d\n", best_params$max_depth))
  cat(sprintf("nrounds: %d\n", best_params$nrounds))
  cat(sprintf("Accuracy: %.4f\n", best_accuracy))
  
  return(list(
    best_params = best_params,
    best_accuracy = best_accuracy,
    all_results = results
  ))
}

# Uncomment to run hyperparameter tuning (warning: can be time-consuming)
# tuning_results <- tune_xgboost(train_set_1_enhanced, test_set_1_enhanced)

# 7. Compare performance across different models
cat("\nModel Performance Comparison:\n")
cat("Logistic Regression (if you ran it before): [Insert accuracy here]\n")
cat("Linear Model (if you ran it before): [Insert accuracy here]\n")
cat("XGBoost without clustering:", round(xgb_result_without_cluster$accuracy * 100, 2), "%\n")
cat("XGBoost with clustering:", round(xgb_result_with_cluster$accuracy * 100, 2), "%\n")
cat("XGBoost with distance features:", round(xgb_result_with_distances$accuracy * 100, 2), "%\n")
```



I will now try Lasso regression:

```{r, eval=FALSE}
library(tidyverse)
library(caret)
library(glmnet)  # For lasso regression

# Define the function to train and evaluate Lasso models - FIXED VERSION WITH NA HANDLING
train_and_evaluate_lasso <- function(train_data, test_data, alpha = 1) {
  # First, check for any NA values
  if(sum(is.na(train_data)) > 0) {
    cat("Warning: Training data contains", sum(is.na(train_data)), "NA values\n")
    cat("Removing NA values from training data\n")
    train_data <- na.omit(train_data)
  }
  
  if(sum(is.na(test_data)) > 0) {
    cat("Warning: Test data contains", sum(is.na(test_data)), "NA values\n")
    cat("Removing NA values from test data\n")
    test_data <- na.omit(test_data)
  }
  
  # Make a copy of the data to avoid modifying the original
  train_data_copy <- train_data
  test_data_copy <- test_data
  
  # Convert feedback to numeric (-1 and 1)
  train_data_copy$feedback_numeric <- as.numeric(as.character(train_data_copy$feedback))
  test_data_copy$feedback_numeric <- as.numeric(as.character(test_data_copy$feedback))
  
  # Create matrices for Lasso - use only predictors, not the response variables
  predictors <- setdiff(names(train_data_copy), c("feedback", "feedback_numeric"))
  
  # Print predictors to verify
  cat("Using these predictors:", paste(predictors, collapse=", "), "\n")
  
  # Ensure all variables are numeric
  train_data_matrix <- train_data_copy %>%
   dplyr::select(all_of(predictors)) %>%
    mutate(across(everything(), as.numeric)) %>%
    as.matrix()
  
  test_data_matrix <- test_data_copy %>%
    dplyr::select(all_of(predictors)) %>%
    mutate(across(everything(), as.numeric)) %>%
    as.matrix()
  
  # Verify no NA values in matrices
  if(sum(is.na(train_data_matrix)) > 0) {
    stop("Training matrix still contains NA values after conversion. Please check your data.")
  }
  
  if(sum(is.na(test_data_matrix)) > 0) {
    stop("Test matrix still contains NA values after conversion. Please check your data.")
  }
  
  y_train <- train_data_copy$feedback_numeric
  y_test <- test_data_copy$feedback_numeric
  
  # Find optimal lambda using cross-validation
  set.seed(123)
  cv_model <- cv.glmnet(train_data_matrix, y_train, alpha = alpha, nfolds = 5)
  best_lambda <- cv_model$lambda.min
  
  # Train the Lasso model with optimal lambda
  lasso_model <- glmnet(train_data_matrix, y_train, alpha = alpha, lambda = best_lambda)
  
  # Make predictions
  predictions <- predict(lasso_model, newx = test_data_matrix, s = best_lambda)
  
  # Convert predictions back to classification
  predicted_classes <- ifelse(predictions > 0, 1, -1)
  
  # Calculate accuracy
  accuracy <- mean(predicted_classes == y_test)
  
  # Create confusion matrix
  cm <- confusionMatrix(
    factor(predicted_classes, levels = c(-1, 1)),
    factor(y_test, levels = c(-1, 1))
  )
  
  return(list(
    model = lasso_model,
    cv_model = cv_model,
    best_lambda = best_lambda,
    predictions = predictions,
    predicted_classes = predicted_classes,
    accuracy = accuracy,
    confusion_matrix = cm,
    coefficients = as.matrix(coef(lasso_model, s = best_lambda))
  ))
}

# Debugging - print column information before running models
print_dataset_info <- function(data, name) {
  cat("\nInformation for dataset:", name, "\n")
  cat("Dimensions:", dim(data)[1], "rows,", dim(data)[2], "columns\n")
  cat("Column names:", paste(names(data), collapse=", "), "\n")
  cat("Column classes:\n")
  for(col in names(data)) {
    cat("  ", col, ":", class(data[[col]]), "\n")
  }
  cat("NA counts:\n")
  for(col in names(data)) {
    na_count <- sum(is.na(data[[col]]))
    if(na_count > 0) {
      cat("  ", col, ":", na_count, "NAs\n")
    }
  }
  cat("Feedback distribution:", table(data$feedback), "\n\n")
}

# Run the debug function to check data
print_dataset_info(train_set_1, "train_set_1")
print_dataset_info(test_set_1, "test_set_1")
if(exists("train_set_1_enhanced")) {
  print_dataset_info(train_set_1_enhanced, "train_set_1_enhanced")
}

# Make sure feedback is properly formatted as a factor with correct levels
train_set_1$feedback <- factor(train_set_1$feedback, levels = c("-1", "1"))
test_set_1$feedback <- factor(test_set_1$feedback, levels = c("-1", "1"))

if(exists("train_set_1_enhanced")) {
  train_set_1_enhanced$feedback <- factor(train_set_1_enhanced$feedback, levels = c("-1", "1"))
  test_set_1_enhanced$feedback <- factor(test_set_1_enhanced$feedback, levels = c("-1", "1"))
}

if(exists("train_set_1_with_distances")) {
  train_set_1_with_distances$feedback <- factor(train_set_1_with_distances$feedback, levels = c("-1", "1"))
  test_set_1_with_distances$feedback <- factor(test_set_1_with_distances$feedback, levels = c("-1", "1"))
}

# 2. Compare Lasso models with and without clustering information (using fold 1)
tryCatch({
  # Model without cluster information
  cat("Evaluating Lasso model WITHOUT clustering information:\n")
  lasso_result_without_cluster <- train_and_evaluate_lasso(train_set_1, test_set_1)
  cat("Accuracy without cluster:", round(lasso_result_without_cluster$accuracy * 100, 2), "%\n")
  cat("Best lambda:", lasso_result_without_cluster$best_lambda, "\n\n")
  
  # Print confusion matrix
  print(lasso_result_without_cluster$confusion_matrix$table)
}, error = function(e) {
  cat("Error in model without clustering:", e$message, "\n")
})

tryCatch({
  # Model with cluster information
  cat("\nEvaluating Lasso model WITH clustering information:\n")
  if(exists("train_set_1_enhanced") && exists("test_set_1_enhanced")) {
    lasso_result_with_cluster <- train_and_evaluate_lasso(train_set_1_enhanced, test_set_1_enhanced)
    cat("Accuracy with cluster:", round(lasso_result_with_cluster$accuracy * 100, 2), "%\n")
    cat("Best lambda:", lasso_result_with_cluster$best_lambda, "\n\n")
    
    # Print confusion matrix
    print(lasso_result_with_cluster$confusion_matrix$table)
  } else {
    cat("Enhanced datasets not found. Please run clustering code first.\n")
  }
}, error = function(e) {
  cat("Error in model with clustering:", e$message, "\n")
})

tryCatch({
  # Evaluate model with distance features
  cat("\nEvaluating Lasso model with DISTANCE features:\n")
  if(exists("train_set_1_with_distances") && exists("test_set_1_with_distances")) {
    lasso_result_with_distances <- train_and_evaluate_lasso(
      train_set_1_with_distances, 
      test_set_1_with_distances
    )
    cat("Accuracy with distance features:", round(lasso_result_with_distances$accuracy * 100, 2), "%\n")
    cat("Best lambda:", lasso_result_with_distances$best_lambda, "\n\n")
    
    # Print confusion matrix
    print(lasso_result_with_distances$confusion_matrix$table)
  } else {
    cat("Distance feature datasets not found. Please run clustering with distances code first.\n")
  }
}, error = function(e) {
  cat("Error in model with distance features:", e$message, "\n")
})

# 4. Evaluate all 4 folds with Lasso
evaluate_all_folds_lasso <- function() {
  results <- list()
  
  tryCatch({
    # Fold 1
    cat("Processing fold 1...\n")
    results[[1]] <- train_and_evaluate_lasso(train_set_1_enhanced, test_set_1_enhanced)
    
    # Fold 2
    cat("Processing fold 2...\n")
    results[[2]] <- train_and_evaluate_lasso(train_set_2_enhanced, test_set_2_enhanced)
    
    # Fold 3
    cat("Processing fold 3...\n")
    results[[3]] <- train_and_evaluate_lasso(train_set_3_enhanced, test_set_3_enhanced)
    
    # Fold 4
    cat("Processing fold 4...\n")
    results[[4]] <- train_and_evaluate_lasso(train_set_4_enhanced, test_set_4_enhanced)
    
    # Calculate overall accuracy
    accuracies <- sapply(results, function(x) x$accuracy)
    mean_accuracy <- mean(accuracies)
    
    # Print results
    cat("Results for all folds with Lasso model (with clusters):\n")
    for (i in 1:4) {
      cat(sprintf("Fold %d accuracy: %.2f%%\n", i, accuracies[i] * 100))
    }
    cat(sprintf("\nMean accuracy across all folds: %.2f%%\n", mean_accuracy * 100))
  }, error = function(e) {
    cat("Error in evaluating folds:", e$message, "\n")
  })
  
  return(results)
}

# Run evaluation on all folds
if(all(exists("train_set_1_enhanced"), exists("train_set_2_enhanced"), 
       exists("train_set_3_enhanced"), exists("train_set_4_enhanced"))) {
  cat("Running evaluation on all folds...\n")
  all_fold_lasso_results <- evaluate_all_folds_lasso()
} else {
  cat("Cannot evaluate all folds: Missing one or more enhanced datasets.\n")
}
```

Now I am trying LDA:

```{r, eval=FALSE}
library(tidyverse)
library(caret)
library(MASS)  # For LDA function

# Define the function to train and evaluate LDA models
train_and_evaluate_lda <- function(train_data, test_data) {
  # Check for any NA values
  if(sum(is.na(train_data)) > 0) {
    cat("Warning: Training data contains", sum(is.na(train_data)), "NA values\n")
    cat("Removing NA values from training data\n")
    train_data <- na.omit(train_data)
  }
  
  if(sum(is.na(test_data)) > 0) {
    cat("Warning: Test data contains", sum(is.na(test_data)), "NA values\n")
    cat("Removing NA values from test data\n")
    test_data <- na.omit(test_data)
  }
  
  # Train the LDA model
  lda_model <- lda(feedback ~ ., data = train_data)
  
  # Make predictions
  predictions <- predict(lda_model, newdata = test_data)
  
  # Extract the class predictions
  predicted_classes <- predictions$class
  
  # Calculate accuracy
  accuracy <- mean(predicted_classes == test_data$feedback)
  
  # Create confusion matrix
  cm <- confusionMatrix(predicted_classes, test_data$feedback)
  
  return(list(
    model = lda_model,
    predictions = predictions,
    predicted_classes = predicted_classes,
    accuracy = accuracy,
    confusion_matrix = cm
  ))
}

# 1. Assuming your datasets are already created with balanced sampling
# Run the clustering on the train and test sets if not already done

# If you haven't run clustering yet, here's a function to do it
run_clustering <- function(train_sets, test_sets, k = 4) {
  # Combine all training data for better clustering
  all_train_data <- bind_rows(train_sets)
  
  # Features for clustering
  features <- all_train_data %>%dplyr::select(avg_spike_count, leftcontrast, rightcontrast)
  scaled_features <- scale(features)
  
  # Run k-means
  set.seed(123)
  km_result <- kmeans(scaled_features, centers = k, nstart = 25)
  
  # Function to assign clusters and calculate distances
  assign_clusters <- function(data) {
    # Scale using the same parameters
    scaled_data <- scale(
      data %>% dplyr::select(avg_spike_count, leftcontrast, rightcontrast), 
      center = attr(scaled_features, "scaled:center"),
      scale = attr(scaled_features, "scaled:scale")
    )
    
    # Calculate distances to centroids
    distances <- matrix(0, nrow = nrow(scaled_data), ncol = k)
    for (i in 1:nrow(scaled_data)) {
      for (j in 1:k) {
        distances[i, j] <- sqrt(sum((scaled_data[i,] - km_result$centers[j,])^2))
      }
    }
    
    # Find closest centroid
    clusters <- apply(distances, 1, which.min)
    
    # Create enhanced dataset with cluster
    enhanced <- data %>% mutate(cluster = factor(clusters))
    
    # Create dataset with distances
    distance_df <- as.data.frame(distances)
    names(distance_df) <- paste0("dist_to_cluster_", 1:k)
    with_distances <- cbind(data, distance_df)
    
    return(list(enhanced = enhanced, with_distances = with_distances))
  }
  
  # Process all datasets
  train_enhanced <- list()
  train_distances <- list()
  test_enhanced <- list()
  test_distances <- list()
  
  for (i in 1:length(train_sets)) {
    result_train <- assign_clusters(train_sets[[i]])
    train_enhanced[[i]] <- result_train$enhanced
    train_distances[[i]] <- result_train$with_distances
    
    result_test <- assign_clusters(test_sets[[i]])
    test_enhanced[[i]] <- result_test$enhanced
    test_distances[[i]] <- result_test$with_distances
  }
  
  return(list(
    train_enhanced = train_enhanced,
    train_distances = train_distances,
    test_enhanced = test_enhanced,
    test_distances = test_distances,
    km_result = km_result
  ))
}

# Check if cluster enhanced datasets exist, otherwise create them
if (!exists("train_set_1_enhanced")) {
  cat("Running clustering to create enhanced datasets...\n")
  # Put your datasets into lists
  train_sets <- list(train_set_1, train_set_2, train_set_3, train_set_4)
  test_sets <- list(test_set_1, test_set_2, test_set_3, test_set_4)
  
  # Run clustering
  cluster_results <- run_clustering(train_sets, test_sets)
  
  # Extract enhanced datasets
  train_set_1_enhanced <- cluster_results$train_enhanced[[1]]
  train_set_2_enhanced <- cluster_results$train_enhanced[[2]]
  train_set_3_enhanced <- cluster_results$train_enhanced[[3]]
  train_set_4_enhanced <- cluster_results$train_enhanced[[4]]
  
  test_set_1_enhanced <- cluster_results$test_enhanced[[1]]
  test_set_2_enhanced <- cluster_results$test_enhanced[[2]]
  test_set_3_enhanced <- cluster_results$test_enhanced[[3]]
  test_set_4_enhanced <- cluster_results$test_enhanced[[4]]
  
  # Extract datasets with distance features
  train_set_1_with_distances <- cluster_results$train_distances[[1]]
  train_set_2_with_distances <- cluster_results$train_distances[[2]]
  train_set_3_with_distances <- cluster_results$train_distances[[3]]
  train_set_4_with_distances <- cluster_results$train_distances[[4]]
  
  test_set_1_with_distances <- cluster_results$test_distances[[1]]
  test_set_2_with_distances <- cluster_results$test_distances[[2]]
  test_set_3_with_distances <- cluster_results$test_distances[[3]]
  test_set_4_with_distances <- cluster_results$test_distances[[4]]
}

# 2. Compare LDA models with and without clustering information (using fold 1)
# Model without cluster information
cat("Evaluating LDA model WITHOUT clustering information:\n")
lda_result_without_cluster <- train_and_evaluate_lda(train_set_1, test_set_1)
cat("Accuracy without cluster:", round(lda_result_without_cluster$accuracy * 100, 2), "%\n\n")

# Print confusion matrix
print(lda_result_without_cluster$confusion_matrix$table)

# Model with cluster information
cat("\nEvaluating LDA model WITH clustering information:\n")
lda_result_with_cluster <- train_and_evaluate_lda(train_set_1_enhanced, test_set_1_enhanced)
cat("Accuracy with cluster:", round(lda_result_with_cluster$accuracy * 100, 2), "%\n\n")

# Print confusion matrix
print(lda_result_with_cluster$confusion_matrix$table)

# 3. Evaluate model with distance features
cat("\nEvaluating LDA model with DISTANCE features:\n")
lda_result_with_distances <- train_and_evaluate_lda(
  train_set_1_with_distances, 
  test_set_1_with_distances
)
cat("Accuracy with distance features:", round(lda_result_with_distances$accuracy * 100, 2), "%\n\n")

# Print confusion matrix
print(lda_result_with_distances$confusion_matrix$table)

# 4. Evaluate all 4 folds with LDA
evaluate_all_folds_lda <- function() {
  results <- list()
  
  # Fold 1
  cat("Processing fold 1...\n")
  results[[1]] <- train_and_evaluate_lda(train_set_1_enhanced, test_set_1_enhanced)
  
  # Fold 2
  cat("Processing fold 2...\n")
  results[[2]] <- train_and_evaluate_lda(train_set_2_enhanced, test_set_2_enhanced)
  
  # Fold 3
  cat("Processing fold 3...\n")
  results[[3]] <- train_and_evaluate_lda(train_set_3_enhanced, test_set_3_enhanced)
  
  # Fold 4
  cat("Processing fold 4...\n")
  results[[4]] <- train_and_evaluate_lda(train_set_4_enhanced, test_set_4_enhanced)
  
  # Calculate overall accuracy
  accuracies <- sapply(results, function(x) x$accuracy)
  mean_accuracy <- mean(accuracies)
  
  # Print results
  cat("Results for all folds with LDA model (with clusters):\n")
  for (i in 1:4) {
    cat(sprintf("Fold %d accuracy: %.2f%%\n", i, accuracies[i] * 100))
  }
  cat(sprintf("\nMean accuracy across all folds: %.2f%%\n", mean_accuracy * 100))
  
  return(results)
}

# Run evaluation on all folds
all_fold_lda_results <- evaluate_all_folds_lda()

# 5. Examine the LDA model's coefficients and insights
# Display LDA coefficients for the model with clustering
cat("\nLDA Coefficients (with clustering):\n")
print(lda_result_with_cluster$model$scaling)

# Calculate variable importance based on absolute coefficient values
lda_coef <- as.data.frame(abs(lda_result_with_cluster$model$scaling))
lda_coef$Variable <- rownames(lda_coef)
names(lda_coef)[1] <- "Importance"
lda_coef <- lda_coef %>% arrange(desc(Importance))

# Plot variable importance
ggplot(lda_coef, aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_col() +
  coord_flip() +
  labs(title = "LDA Variable Importance",
       x = "Variable",
       y = "Absolute Coefficient Value") +
  theme_minimal()

# 6. Visualize LDA's discriminant function
# Create a data frame with LDA values
if(exists("lda_result_with_cluster") && !is.null(lda_result_with_cluster$predictions)) {
  lda_plot_data <- data.frame(
    LD1 = lda_result_with_cluster$predictions$x,
    feedback = test_set_1_enhanced$feedback
  )
  
  # Add cluster information if available
  if("cluster" %in% names(test_set_1_enhanced)) {
    lda_plot_data$cluster <- test_set_1_enhanced$cluster
  }
  
  # Plot the LDA scores
  ggplot(lda_plot_data, aes(x = LD1, fill = feedback)) +
    geom_density(alpha = 0.5) +
    labs(title = "LDA Discriminant Scores",
         x = "Linear Discriminant 1",
         y = "Density") +
    theme_minimal()
  
  # If we have cluster information, make a scatter plot with clusters
  if("cluster" %in% names(lda_plot_data)) {
    ggplot(lda_plot_data, aes(x = LD1, y = as.numeric(feedback), color = cluster)) +
      geom_jitter(width = 0, height = 0.05, alpha = 0.5) +
      labs(title = "LDA Scores by Cluster",
           x = "Linear Discriminant 1",
           y = "Feedback (-1/1)") +
      theme_minimal()
  }
}

# 7. Compare performance across different models
cat("\nModel Performance Comparison:\n")
cat("LDA without clustering:", round(lda_result_without_cluster$accuracy * 100, 2), "%\n")
cat("LDA with clustering:", round(lda_result_with_cluster$accuracy * 100, 2), "%\n")
cat("LDA with distance features:", round(lda_result_with_distances$accuracy * 100, 2), "%\n")
```

Now I am trying kNN:

```{r, eval=FALSE}
library(tidyverse)
library(caret)
library(class)  # For knn function

# Define the function to train and evaluate kNN models - FIXED VERSION
train_and_evaluate_knn <- function(train_data, test_data, k_value = 5) {
  # Check for any NA values
  if(sum(is.na(train_data)) > 0) {
    cat("Warning: Training data contains", sum(is.na(train_data)), "NA values\n")
    cat("Removing NA values from training data\n")
    train_data <- na.omit(train_data)
  }
  
  if(sum(is.na(test_data)) > 0) {
    cat("Warning: Test data contains", sum(is.na(test_data)), "NA values\n")
    cat("Removing NA values from test data\n")
    test_data <- na.omit(test_data)
  }
  
  # Check if feedback column exists
  if(!"feedback" %in% names(train_data) || !"feedback" %in% names(test_data)) {
    stop("Error: 'feedback' column not found in data. Please check your column names.")
  }
  
  # Print column names for debugging
  cat("Train data columns:", paste(names(train_data), collapse=", "), "\n")
  cat("Test data columns:", paste(names(test_data), collapse=", "), "\n")
  
  # Separate features and labels
  train_features <- train_data %>% dplyr::select(-one_of("feedback"))
  train_labels <- train_data$feedback
  
  test_features <- test_data %>% dplyr::select(-one_of("feedback"))
  test_labels <- test_data$feedback
  
  # Scale the features (important for kNN)
  scale_params <- preProcess(train_features, method = c("center", "scale"))
  train_features_scaled <- predict(scale_params, train_features)
  test_features_scaled <- predict(scale_params, test_features)
  
  # Train the kNN model and make predictions
  predicted_classes <- knn(
    train = train_features_scaled,
    test = test_features_scaled,
    cl = train_labels,
    k = k_value
  )
  
  # Calculate accuracy
  accuracy <- mean(predicted_classes == test_labels)
  
  # Create confusion matrix
  cm <- confusionMatrix(predicted_classes, test_labels)
  
  return(list(
    k_value = k_value,
    predicted_classes = predicted_classes,
    accuracy = accuracy,
    confusion_matrix = cm,
    scale_params = scale_params
  ))
}

# Function to find optimal k value for kNN
find_optimal_k <- function(train_data, test_data, k_range = 1:20) {
  # Make sure both datasets have the required columns
  if(!"feedback" %in% names(train_data) || !"feedback" %in% names(test_data)) {
    stop("Error: 'feedback' column not found in data. Please check your column names.")
  }
  
  k_results <- data.frame(k = integer(), accuracy = numeric())
  
  for (k in k_range) {
    cat("Testing k =", k, "\n")
    result <- tryCatch({
      train_and_evaluate_knn(train_data, test_data, k_value = k)
    }, error = function(e) {
      cat("Error with k =", k, ":", e$message, "\n")
      return(NULL)
    })
    
    if(!is.null(result)) {
      k_results <- rbind(k_results, data.frame(k = k, accuracy = result$accuracy))
    }
  }
  
  if(nrow(k_results) == 0) {
    stop("All k values failed. Check your data structure.")
  }
  
  # Find optimal k
  optimal_k <- k_results$k[which.max(k_results$accuracy)]
  
  # Plot results
  p <- ggplot(k_results, aes(x = k, y = accuracy)) +
    geom_line() +
    geom_point() +
    geom_vline(xintercept = optimal_k, linetype = "dashed", color = "red") +
    labs(title = "kNN Performance by k Value",
         subtitle = paste("Optimal k =", optimal_k),
         x = "k (Number of Neighbors)",
         y = "Accuracy") +
    theme_minimal()
  
  print(p)
  
  cat("Optimal k value:", optimal_k, "with accuracy:", 
      round(max(k_results$accuracy) * 100, 2), "%\n")
  
  return(optimal_k)
}

# Debug function to check data structure
check_data_structure <- function(data, name) {
  cat("\nChecking structure of", name, ":\n")
  cat("Dimensions:", dim(data)[1], "rows,", dim(data)[2], "columns\n")
  cat("Column names:", paste(names(data), collapse=", "), "\n")
  cat("First few rows of data:\n")
  print(head(data, 3))
  
  # Check feedback column
  if("feedback" %in% names(data)) {
    cat("Feedback column exists and has class:", class(data$feedback), "\n")
    cat("Feedback distribution:", table(data$feedback), "\n")
  } else {
    cat("WARNING: 'feedback' column not found!\n")
  }
  cat("\n")
}

# Check your dataset structure
check_data_structure(train_set_1, "train_set_1")
check_data_structure(test_set_1, "test_set_1")

# 1. Find optimal k value using fold 1
tryCatch({
  cat("Finding optimal k value for kNN...\n")
  optimal_k <- find_optimal_k(train_set_1, test_set_1, k_range = 1:15)
}, error = function(e) {
  cat("Error finding optimal k:", e$message, "\n")
  # Fallback to a reasonable default
  cat("Using default k = 5 instead\n")
  optimal_k <<- 5
})

# 2. Compare kNN models with and without clustering information (using fold 1)
# Model without cluster information
tryCatch({
  cat("\nEvaluating kNN model WITHOUT clustering information:\n")
  knn_result_without_cluster <- train_and_evaluate_knn(train_set_1, test_set_1, k_value = optimal_k)
  cat("Accuracy without cluster:", round(knn_result_without_cluster$accuracy * 100, 2), "%\n\n")
  
  # Print confusion matrix
  print(knn_result_without_cluster$confusion_matrix$table)
}, error = function(e) {
  cat("Error in basic kNN model:", e$message, "\n")
})

# Model with cluster information
if(exists("train_set_1_enhanced") && exists("test_set_1_enhanced")) {
  tryCatch({
    cat("\nEvaluating kNN model WITH clustering information:\n")
    knn_result_with_cluster <- train_and_evaluate_knn(train_set_1_enhanced, test_set_1_enhanced, k_value = optimal_k)
    cat("Accuracy with cluster:", round(knn_result_with_cluster$accuracy * 100, 2), "%\n\n")
    
    # Print confusion matrix
    print(knn_result_with_cluster$confusion_matrix$table)
  }, error = function(e) {
    cat("Error in cluster-enhanced kNN model:", e$message, "\n")
    check_data_structure(train_set_1_enhanced, "train_set_1_enhanced")
  })
} else {
  cat("\nSkipping cluster-enhanced model - datasets not found\n")
}

# Model with distance features
if(exists("train_set_1_with_distances") && exists("test_set_1_with_distances")) {
  tryCatch({
    cat("\nEvaluating kNN model with DISTANCE features:\n")
    knn_result_with_distances <- train_and_evaluate_knn(
      train_set_1_with_distances, 
      test_set_1_with_distances,
      k_value = optimal_k
    )
    cat("Accuracy with distance features:", round(knn_result_with_distances$accuracy * 100, 2), "%\n\n")
    
    # Print confusion matrix
    print(knn_result_with_distances$confusion_matrix$table)
  }, error = function(e) {
    cat("Error in distance-enhanced kNN model:", e$message, "\n")
  })
} else {
  cat("\nSkipping distance-enhanced model - datasets not found\n")
}

# 3. Evaluate all 4 folds with kNN
evaluate_all_folds_knn <- function(k_value) {
  results <- list()
  
  # Check if all datasets exist
  all_sets_exist <- all(exists("train_set_1_enhanced"), exists("test_set_1_enhanced"),
                        exists("train_set_2_enhanced"), exists("test_set_2_enhanced"),
                        exists("train_set_3_enhanced"), exists("test_set_3_enhanced"),
                        exists("train_set_4_enhanced"), exists("test_set_4_enhanced"))
  
  if(!all_sets_exist) {
    cat("Cannot evaluate all folds - some datasets are missing\n")
    return(NULL)
  }
  
  # Fold 1
  tryCatch({
    cat("Processing fold 1...\n")
    results[[1]] <- train_and_evaluate_knn(train_set_1_enhanced, test_set_1_enhanced, k_value = k_value)
  }, error = function(e) {
    cat("Error in fold 1:", e$message, "\n")
    results[[1]] <- list(accuracy = NA)
  })
  
  # Fold 2
  tryCatch({
    cat("Processing fold 2...\n")
    results[[2]] <- train_and_evaluate_knn(train_set_2_enhanced, test_set_2_enhanced, k_value = k_value)
  }, error = function(e) {
    cat("Error in fold 2:", e$message, "\n")
    results[[2]] <- list(accuracy = NA)
  })
  
  # Fold 3
  tryCatch({
    cat("Processing fold 3...\n")
    results[[3]] <- train_and_evaluate_knn(train_set_3_enhanced, test_set_3_enhanced, k_value = k_value)
  }, error = function(e) {
    cat("Error in fold 3:", e$message, "\n")
    results[[3]] <- list(accuracy = NA)
  })
  
  # Fold 4
  tryCatch({
    cat("Processing fold 4...\n")
    results[[4]] <- train_and_evaluate_knn(train_set_4_enhanced, test_set_4_enhanced, k_value = k_value)
  }, error = function(e) {
    cat("Error in fold 4:", e$message, "\n")
    results[[4]] <- list(accuracy = NA)
  })
  
  # Calculate overall accuracy (ignoring NA values)
  accuracies <- sapply(results, function(x) x$accuracy)
  valid_accuracies <- accuracies[!is.na(accuracies)]
  
  if(length(valid_accuracies) > 0) {
    mean_accuracy <- mean(valid_accuracies)
    
    # Print results
    cat("Results for all folds with kNN model (with clusters):\n")
    for (i in 1:4) {
      if(!is.na(accuracies[i])) {
        cat(sprintf("Fold %d accuracy: %.2f%%\n", i, accuracies[i] * 100))
      } else {
        cat(sprintf("Fold %d: Failed\n", i))
      }
    }
    cat(sprintf("\nMean accuracy across valid folds: %.2f%%\n", mean_accuracy * 100))
  } else {
    cat("No valid results to report\n")
  }
  
  return(results)
}

# Run evaluation on all folds using the optimal k if it exists
if(exists("optimal_k")) {
  all_fold_knn_results <- evaluate_all_folds_knn(optimal_k)
}
```




```{r, eval=FALSE}
library(tidyverse)
library(knitr)
library(kableExtra)  # For nicer tables, install if needed with install.packages("kableExtra")

# Function to extract accuracies from model results
extract_accuracies <- function(model_results) {
  sapply(model_results, function(x) x$accuracy)
}

# Create a data frame to store all results
# First, initialize with fold information
comparison_table <- data.frame(
  Fold = c("Fold 1", "Fold 2", "Fold 3", "Fold 4", "Mean")
)

# Add results for logistic regression (if you ran it)
# Replace "all_fold_results" with the actual variable name you used
if(exists("all_fold_results")) {
  logistic_accuracies <- extract_accuracies(all_fold_results)
  comparison_table$`Logistic Regression` <- c(
    logistic_accuracies * 100,
    mean(logistic_accuracies) * 100
  )
}

# Add results for linear model (if you ran it)
if(exists("all_fold_linear_results")) {
  linear_accuracies <- extract_accuracies(all_fold_linear_results)
  comparison_table$`Linear Model` <- c(
    linear_accuracies * 100,
    mean(linear_accuracies) * 100
  )
}

# Add results for XGBoost (if you ran it)
if(exists("all_fold_xgb_results")) {
  xgb_accuracies <- extract_accuracies(all_fold_xgb_results)
  comparison_table$`XGBoost` <- c(
    xgb_accuracies * 100,
    mean(xgb_accuracies) * 100
  )
}

# Add results for Lasso (if you ran it)
if(exists("all_fold_lasso_results")) {
  lasso_accuracies <- extract_accuracies(all_fold_lasso_results)
  comparison_table$`Lasso` <- c(
    lasso_accuracies * 100,
    mean(lasso_accuracies) * 100
  )
}

# Round all numeric columns to 2 decimal places
comparison_table <- comparison_table %>%
  mutate(across(where(is.numeric), ~ round(., 2)))

# Print the table
print(comparison_table)

# Create a nicer formatted table if kableExtra is available
if(requireNamespace("kableExtra", quietly = TRUE)) {
  formatted_table <- comparison_table %>%
    kable(format = "html", caption = "Prediction Accuracy (%) by Method and Fold") %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
    row_spec(5, bold = TRUE, background = "#f2f2f2")  # Highlight the mean row
  
  print(formatted_table)
}

# Also create a ggplot visualization of the results
comparison_long <- comparison_table %>%
  pivot_longer(cols = -Fold, names_to = "Method", values_to = "Accuracy") %>%
  mutate(Fold = factor(Fold, levels = c("Fold 1", "Fold 2", "Fold 3", "Fold 4", "Mean")))

# Plot
ggplot(comparison_long %>% filter(Fold != "Mean"), 
       aes(x = Method, y = Accuracy, fill = Fold)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_hline(data = comparison_long %>% filter(Fold == "Mean"),
             aes(yintercept = Accuracy, color = Method),
             linetype = "dashed", size = 1) +
  labs(title = "Prediction Accuracy by Method and Fold",
       x = "Method",
       y = "Accuracy (%)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Alternative: Create a line plot to better visualize performance across folds
ggplot(comparison_long %>% filter(Fold != "Mean"), 
       aes(x = Fold, y = Accuracy, color = Method, group = Method)) +
  geom_line(size = 1) +
  geom_point(size = 3) +
  labs(title = "Prediction Accuracy Across Folds",
       x = "Fold",
       y = "Accuracy (%)") +
  theme_minimal()
```



















```{r, eval=FALSE}
library(tidyverse)
library(caret)

# First, ensure 'contrastdiff' exists - create it as the difference between leftcontrast and rightcontrast


# Elbow method for optimal number of clusters
# 1. Prepare the data for clustering
kmeans_data <- results_df_allsessions %>% ungroup() %>%
  dplyr::select(avg_spike_count, contrastdiff) 

# 2. Take a sample to avoid memory issues
set.seed(123)  # For reproducibility
sample_size <- 5000
sampled_data <- kmeans_data %>% sample_n(min(nrow(kmeans_data), sample_size))

# More targeted conversion
sampled_data <- sampled_data %>%
  mutate(
    avg_spike_count = as.numeric(avg_spike_count),
    contrastdiff = as.numeric(contrastdiff)
  )

# 3. Scale the data
scaled_data <- scale(sampled_data)

# 4. Elbow Method - calculate total within-cluster sum of squares for different k values
wss <- numeric(10)  # We'll try k from 1 to 10
for (i in 1:10) {
  # Print progress
  cat("Running k-means with k =", i, "\n")
  
  # Run k-means
  km <- kmeans(scaled_data, centers = i, nstart = 25)
  
  # Store the within-cluster sum of squares
  wss[i] <- km$tot.withinss
}

# 5. Create a data frame for plotting
elbow_df <- data.frame(k = 1:10, wss = wss)

# 6. Print the WSS values for each k
print(elbow_df)

# 7. Plot the elbow curve
ggplot(elbow_df, aes(x = k, y = wss)) +
  geom_line() +
  geom_point(size = 3) +
  labs(title = "Elbow Method for Optimal Number of Clusters",
       x = "Number of Clusters (k)",
       y = "Total Within-Cluster Sum of Squares") +
  theme_minimal()

# Create a balanced sample for modeling
# 1. Set a seed for reproducibility
set.seed(123)

# 2. First, check the original class distribution
cat("Original class distribution in results_df_allsessions:\n")
table(results_df_allsessions$feedback)

# 3. Create a balanced sample with equal numbers of each class
feedback_counts <- table(results_df_allsessions$feedback)
cat("Class counts in original data:", feedback_counts, "\n")

# Make sure we can get 5000 of each class (for 10,000 total)
samples_per_class <- 5000
if (any(feedback_counts < samples_per_class)) {
  # If we can't get 5000 of each, use the maximum possible
  samples_per_class <- min(feedback_counts)
  cat("Warning: Reduced sample size to", samples_per_class * 2, 
      "total because minimum class count is", samples_per_class, "\n")
}

# Sample equally from each class
balanced_data <- results_df_allsessions %>%
  ungroup() %>%
  # Group by feedback to sample separately from each class
  group_by(feedback) %>%
  # Sample equal numbers from each class
  sample_n(size = samples_per_class, replace = FALSE) %>%
  ungroup() %>%
  #dplyr::select only the columns you need for modeling
  dplyr::select(feedback, avg_spike_count, leftcontrast, rightcontrast, contrastdiff) %>%
  # Make sure feedback is a factor for classification
  mutate(feedback = factor(feedback)) %>%
  # Remove any rows with NA values
  na.omit()

# Verify the balanced sample
cat("Balanced sample class distribution:\n")
table(balanced_data$feedback)

# 4. Create stratified folds for cross-validation
set.seed(456)  # Different seed for fold creation
k_folds <- 4
fold_indices <- createFolds(balanced_data$feedback, k = k_folds, list = TRUE, returnTrain = FALSE)

# 5. Create individual dataframes for each train and test set
# Test set 1
test_set_1 <- balanced_data[fold_indices[[1]], ]
# Train set 1 (all rows except those in test_set_1)
train_set_1 <- balanced_data[-fold_indices[[1]], ]

# Test set 2
test_set_2 <- balanced_data[fold_indices[[2]], ]
# Train set 2
train_set_2 <- balanced_data[-fold_indices[[2]], ]

# Test set 3
test_set_3 <- balanced_data[fold_indices[[3]], ]
# Train set 3
train_set_3 <- balanced_data[-fold_indices[[3]], ]

# Test set 4
test_set_4 <- balanced_data[fold_indices[[4]], ]
# Train set 4
train_set_4 <- balanced_data[-fold_indices[[4]], ]

# 6. Print information about each fold to verify balance
cat("Fold 1:\n")
cat("Training set:", nrow(train_set_1), "rows\n")
cat("Test set:", nrow(test_set_1), "rows\n")
cat("Training class balance:", table(train_set_1$feedback), "\n")
cat("Test class balance:", table(test_set_1$feedback), "\n\n")

cat("Fold 2:\n")
cat("Training set:", nrow(train_set_2), "rows\n")
cat("Test set:", nrow(test_set_2), "rows\n")
cat("Training class balance:", table(train_set_2$feedback), "\n")
cat("Test class balance:", table(test_set_2$feedback), "\n\n")

cat("Fold 3:\n")
cat("Training set:", nrow(train_set_3), "rows\n")
cat("Test set:", nrow(test_set_3), "rows\n")
cat("Training class balance:", table(train_set_3$feedback), "\n")
cat("Test class balance:", table(test_set_3$feedback), "\n\n")

cat("Fold 4:\n")
cat("Training set:", nrow(train_set_4), "rows\n")
cat("Test set:", nrow(test_set_4), "rows\n")
cat("Training class balance:", table(train_set_4$feedback), "\n")
cat("Test class balance:", table(test_set_4$feedback), "\n\n")

# 7. Calculate class proportions to verify near-perfect balance
proportion_check <- function(data_set, name) {
  counts <- table(data_set$feedback)
  proportions <- prop.table(counts)
  cat(name, "class proportions:", round(proportions, 3), "\n")
}

cat("Class proportion verification:\n")
proportion_check(balanced_data, "Overall sample")
proportion_check(train_set_1, "Train set 1")
proportion_check(test_set_1, "Test set 1")

# Define the basic train_and_evaluate function
train_and_evaluate <- function(train_data, test_data) {
  # Train the model
  model <- glm(feedback ~ ., data = train_data, family = "binomial")
  
  # Make predictions
  predictions_prob <- predict(model, newdata = test_data, type = "response")
  predictions <- ifelse(predictions_prob > 0.5, "1", "-1")
  predictions <- factor(predictions, levels = levels(test_data$feedback))
  
  # Calculate accuracy
  accuracy <- mean(predictions == test_data$feedback)
  
  # Create confusion matrix
  cm <- confusionMatrix(predictions, test_data$feedback)
  
  return(list(
    model = model,
    accuracy = accuracy,
    confusion_matrix = cm
  ))
}

# 1. Combine all training data for clustering
all_training_data <- rbind(train_set_1, train_set_2, train_set_3, train_set_4)

# 2. Perform k-means clustering on the combined training data
set.seed(123)  # For reproducibility
features_for_clustering <- all_training_data %>%
  dplyr::select(avg_spike_count, contrastdiff)  # Changed to use these two variables


features_for_clustering <- features_for_clustering %>%
  mutate(
    avg_spike_count = as.numeric(as.character(avg_spike_count)),
    contrastdiff = as.numeric(as.character(contrastdiff))
  )

# Scale the data for better clustering
scaled_features <- scale(features_for_clustering)

# Perform k-means clustering with optimal k (4 from elbow method)
k <- 4
km_result <- kmeans(scaled_features, centers = k, nstart = 25)

# 3. Function to assign clusters to new data
assign_clusters <- function(new_data, km_model, original_center, original_scale) {
  #dplyr::select only the features used for clustering
  features <- new_data %>%
    dplyr::select(avg_spike_count, contrastdiff)  # Changed to use these two variables
  
  # Scale using the same parameters as the original data
  scaled_data <- scale(features, 
                      center = original_center,
                      scale = original_scale)
  
  # Calculate distance to each cluster center
  n_obs <- nrow(scaled_data)
  n_centers <- nrow(km_model$centers)
  distances <- matrix(0, nrow = n_obs, ncol = n_centers)
  
  for (i in 1:n_obs) {
    for (j in 1:n_centers) {
      distances[i, j] <- sqrt(sum((scaled_data[i,] - km_model$centers[j,])^2))
    }
  }
  
  # Assign each observation to the nearest cluster
  clusters <- apply(distances, 1, which.min)
  
  return(list(
    cluster = clusters,
    distances = distances
  ))
}

# 4. Add cluster assignments to all datasets
# For training sets
train_assignments_1 <- assign_clusters(
  train_set_1, 
  km_result, 
  attr(scaled_features, "scaled:center"),
  attr(scaled_features, "scaled:scale")
)
train_set_1_enhanced <- train_set_1 %>%
  mutate(cluster = factor(train_assignments_1$cluster))

train_assignments_2 <- assign_clusters(
  train_set_2, 
  km_result, 
  attr(scaled_features, "scaled:center"),
  attr(scaled_features, "scaled:scale")
)
train_set_2_enhanced <- train_set_2 %>%
  mutate(cluster = factor(train_assignments_2$cluster))

train_assignments_3 <- assign_clusters(
  train_set_3, 
  km_result, 
  attr(scaled_features, "scaled:center"),
  attr(scaled_features, "scaled:scale")
)
train_set_3_enhanced <- train_set_3 %>%
  mutate(cluster = factor(train_assignments_3$cluster))

train_assignments_4 <- assign_clusters(
  train_set_4, 
  km_result, 
  attr(scaled_features, "scaled:center"),
  attr(scaled_features, "scaled:scale")
)
train_set_4_enhanced <- train_set_4 %>%
  mutate(cluster = factor(train_assignments_4$cluster))

# For test sets
test_assignments_1 <- assign_clusters(
  test_set_1, 
  km_result, 
  attr(scaled_features, "scaled:center"),
  attr(scaled_features, "scaled:scale")
)
test_set_1_enhanced <- test_set_1 %>%
  mutate(cluster = factor(test_assignments_1$cluster))

test_assignments_2 <- assign_clusters(
  test_set_2, 
  km_result, 
  attr(scaled_features, "scaled:center"),
  attr(scaled_features, "scaled:scale")
)
test_set_2_enhanced <- test_set_2 %>%
  mutate(cluster = factor(test_assignments_2$cluster))

test_assignments_3 <- assign_clusters(
  test_set_3, 
  km_result, 
  attr(scaled_features, "scaled:center"),
  attr(scaled_features, "scaled:scale")
)
test_set_3_enhanced <- test_set_3 %>%
  mutate(cluster = factor(test_assignments_3$cluster))

test_assignments_4 <- assign_clusters(
  test_set_4, 
  km_result, 
  attr(scaled_features, "scaled:center"),
  attr(scaled_features, "scaled:scale")
)
test_set_4_enhanced <- test_set_4 %>%
  mutate(cluster = factor(test_assignments_4$cluster))

# 5. Compare models with and without clustering information (using fold 1)
# Model without cluster information
cat("Evaluating model WITHOUT clustering information:\n")
result_without_cluster <- train_and_evaluate(train_set_1, test_set_1)
cat("Accuracy without cluster:", round(result_without_cluster$accuracy * 100, 2), "%\n\n")

# Model with cluster information
cat("Evaluating model WITH clustering information:\n")
result_with_cluster <- train_and_evaluate(train_set_1_enhanced, test_set_1_enhanced)
cat("Accuracy with cluster:", round(result_with_cluster$accuracy * 100, 2), "%\n\n")

# 6. Create distance features
# Function to convert distances to features
distances_to_features <- function(distances) {
  distance_df <- as.data.frame(distances)
  colnames(distance_df) <- paste0("dist_to_cluster_", 1:ncol(distances))
  return(distance_df)
}

# Add distance features to train and test sets
train_distances_1 <- distances_to_features(train_assignments_1$distances)
train_set_1_with_distances <- cbind(train_set_1, train_distances_1)

test_distances_1 <- distances_to_features(test_assignments_1$distances)
test_set_1_with_distances <- cbind(test_set_1, test_distances_1)

# Train and evaluate model with distance features
cat("Evaluating model with DISTANCE features:\n")
result_with_distances <- train_and_evaluate(train_set_1_with_distances, test_set_1_with_distances)
cat("Accuracy with distance features:", round(result_with_distances$accuracy * 100, 2), "%\n\n")

# 7. Evaluate all 4 folds with the enhanced approach
evaluate_all_folds <- function() {
  results <- list()
  
  # Fold 1
  results[[1]] <- train_and_evaluate(train_set_1_enhanced, test_set_1_enhanced)
  
  # Fold 2
  results[[2]] <- train_and_evaluate(train_set_2_enhanced, test_set_2_enhanced)
  
  # Fold 3
  results[[3]] <- train_and_evaluate(train_set_3_enhanced, test_set_3_enhanced)
  
  # Fold 4
  results[[4]] <- train_and_evaluate(train_set_4_enhanced, test_set_4_enhanced)
  
  # Calculate overall accuracy
  accuracies <- sapply(results, function(x) x$accuracy)
  mean_accuracy <- mean(accuracies)
  
  # Print results
  cat("Results for all folds with cluster feature:\n")
  for (i in 1:4) {
    cat(sprintf("Fold %d accuracy: %.2f%%\n", i, accuracies[i] * 100))
  }
  cat(sprintf("\nMean accuracy across all folds: %.2f%%\n", mean_accuracy * 100))
  
  return(results)
}

# Run evaluation on all folds
all_fold_results <- evaluate_all_folds()

# 8. Visualize the importance of features (including cluster)
if (requireNamespace("randomForest", quietly = TRUE)) {
  library(randomForest)
  
  # Train a random forest model
  rf_model <- randomForest(feedback ~ ., data = train_set_1_enhanced, importance = TRUE)
  
  # Get variable importance
  importance_df <- as.data.frame(importance(rf_model))
  importance_df$Variable <- rownames(importance_df)
  
  # Plot variable importance
  ggplot(importance_df, aes(x = reorder(Variable, MeanDecreaseGini), y = MeanDecreaseGini)) +
    geom_col() +
    coord_flip() +
    labs(title = "Variable Importance (including cluster)",
         x = "Variable",
         y = "Mean Decrease in Gini") +
    theme_minimal()
}

# Also run other models as needed - reuse your existing code for:
# - Linear models
# - XGBoost
# - Lasso
# - LDA
# - kNN
# Each would need to be adjusted to use the new cluster assignments.

# For comparison table at the end
library(tidyverse)
library(knitr)
if(requireNamespace("kableExtra", quietly = TRUE)) {
  library(kableExtra)
}

# Function to extract accuracies from model results
extract_accuracies <- function(model_results) {
  sapply(model_results, function(x) x$accuracy)
}

# Create a data frame to store all results
# First, initialize with fold information
comparison_table <- data.frame(
  Fold = c("Fold 1", "Fold 2", "Fold 3", "Fold 4", "Mean")
)

# Add results for logistic regression
if(exists("all_fold_results")) {
  logistic_accuracies <- extract_accuracies(all_fold_results)
  comparison_table$`Logistic Regression` <- c(
    logistic_accuracies * 100,
    mean(logistic_accuracies) * 100
  )
}

# Add other model results as they become available
# Round all numeric columns to 2 decimal places
comparison_table <- comparison_table %>%
  mutate(across(where(is.numeric), ~ round(., 2)))

# Print the table
print(comparison_table)

# Create a visualization of the results
if(ncol(comparison_table) > 1) { # Only if we have model results
  comparison_long <- comparison_table %>%
    pivot_longer(cols = -Fold, names_to = "Method", values_to = "Accuracy") %>%
    mutate(Fold = factor(Fold, levels = c("Fold 1", "Fold 2", "Fold 3", "Fold 4", "Mean")))
  
  # Plot
  ggplot(comparison_long %>% filter(Fold != "Mean"), 
         aes(x = Method, y = Accuracy, fill = Fold)) +
    geom_bar(stat = "identity", position = "dodge") +
    labs(title = "Prediction Accuracy by Method and Fold",
         x = "Method",
         y = "Accuracy (%)") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
}
```














*****************************************************************

```{r, eval=FALSE}
library(tidyverse)
# 1. Prepare the data for clustering
kmeans_data <- results_df_allsessions %>% ungroup() %>%
  dplyr::select(avg_spike_count, contrastdiff) 

# 1.5. Convert to numeric if needed
kmeans_data <- kmeans_data %>%
  mutate(
    avg_spike_count = as.numeric(as.character(avg_spike_count)),
    contrastdiff = as.numeric(as.character(contrastdiff))
  )

# Check if conversion created any NAs and remove them
if(any(is.na(kmeans_data))) {
  cat("Warning: NA values created during conversion. Removing them...\n")
  kmeans_data <- kmeans_data %>% filter(!is.na(avg_spike_count), !is.na(contrastdiff))
}

# 2. Take a sample to avoid memory issues
set.seed(123)  # For reproducibility
sample_size <- 5000
sampled_data <- kmeans_data %>% sample_n(min(nrow(kmeans_data), sample_size))

# Verify data is numeric before scaling
str(sampled_data)

# 3. Scale the data
scaled_data <- scale(sampled_data)

# 4. Elbow Method - calculate total within-cluster sum of squares for different k values
wss <- numeric(10)  # We'll try k from 1 to 10
for (i in 1:10) {
  # Print progress
  cat("Running k-means with k =", i, "\n")
  
  # Run k-means
  km <- kmeans(scaled_data, centers = i, nstart = 25)
  
  # Store the within-cluster sum of squares
  wss[i] <- km$tot.withinss
}

# 5. Create a data frame for plotting
elbow_df <- data.frame(k = 1:10, wss = wss)

# 6. Print the WSS values for each k
print(elbow_df)

# 7. Plot the elbow curve
ggplot(elbow_df, aes(x = k, y = wss)) +
  geom_line() +
  geom_point(size = 3) +
  labs(title = "Elbow Method for Optimal Number of Clusters",
       x = "Number of Clusters (k)",
       y = "Total Within-Cluster Sum of Squares") +
  theme_minimal()
```

```{r, eval=FALSE}
library(tidyverse)
library(caret)  # For cross-validation functions

# 1. Set a seed for reproducibility
set.seed(123)

# 2. First, check the original class distribution
cat("Original class distribution in results_df_allsessions:\n")
table(results_df_allsessions$feedback)

# 3. Create a balanced sample with equal numbers of each class
# Find the minimum count to determine how many we can sample from each class
feedback_counts <- table(results_df_allsessions$feedback)
cat("Class counts in original data:", feedback_counts, "\n")

# Make sure we can get 5000 of each class (for 10,000 total)
samples_per_class <- 5000
if (any(feedback_counts < samples_per_class)) {
  # If we can't get 5000 of each, use the maximum possible
  samples_per_class <- min(feedback_counts)
  cat("Warning: Reduced sample size to", samples_per_class * 2, 
      "total because minimum class count is", samples_per_class, "\n")
}

# Sample equally from each class
balanced_data <- results_df_allsessions %>%
  ungroup() %>%
  # Group by feedback to sample separately from each class
  group_by(feedback) %>%
  # Sample equal numbers from each class
  sample_n(size = samples_per_class, replace = FALSE) %>%
  ungroup() %>%
  #dplyr::select only the columns you need for modeling
  dplyr::select(feedback, avg_spike_count, leftcontrast, rightcontrast) %>%
  # Make sure feedback is a factor for classification
  mutate(feedback = factor(feedback)) %>%
  # Remove any rows with NA values
  na.omit()

# Verify the balanced sample
cat("Balanced sample class distribution:\n")
table(balanced_data$feedback)

# 4. Create stratified folds for cross-validation
# createFolds ensures proportional class representation in each fold
set.seed(456)  # Different seed for fold creation
k_folds <- 4
fold_indices <- createFolds(balanced_data$feedback, k = k_folds, list = TRUE, returnTrain = FALSE)

# 5. Create individual dataframes for each train and test set
# Test set 1
test_set_1 <- balanced_data[fold_indices[[1]], ]
# Train set 1 (all rows except those in test_set_1)
train_set_1 <- balanced_data[-fold_indices[[1]], ]

# Test set 2
test_set_2 <- balanced_data[fold_indices[[2]], ]
# Train set 2
train_set_2 <- balanced_data[-fold_indices[[2]], ]

# Test set 3
test_set_3 <- balanced_data[fold_indices[[3]], ]
# Train set 3
train_set_3 <- balanced_data[-fold_indices[[3]], ]

# Test set 4
test_set_4 <- balanced_data[fold_indices[[4]], ]
# Train set 4
train_set_4 <- balanced_data[-fold_indices[[4]], ]

# 6. Print information about each fold to verify balance
cat("Fold 1:\n")
cat("Training set:", nrow(train_set_1), "rows\n")
cat("Test set:", nrow(test_set_1), "rows\n")
cat("Training class balance:", table(train_set_1$feedback), "\n")
cat("Test class balance:", table(test_set_1$feedback), "\n\n")

cat("Fold 2:\n")
cat("Training set:", nrow(train_set_2), "rows\n")
cat("Test set:", nrow(test_set_2), "rows\n")
cat("Training class balance:", table(train_set_2$feedback), "\n")
cat("Test class balance:", table(test_set_2$feedback), "\n\n")

cat("Fold 3:\n")
cat("Training set:", nrow(train_set_3), "rows\n")
cat("Test set:", nrow(test_set_3), "rows\n")
cat("Training class balance:", table(train_set_3$feedback), "\n")
cat("Test class balance:", table(test_set_3$feedback), "\n\n")

cat("Fold 4:\n")
cat("Training set:", nrow(train_set_4), "rows\n")
cat("Test set:", nrow(test_set_4), "rows\n")
cat("Training class balance:", table(train_set_4$feedback), "\n")
cat("Test class balance:", table(test_set_4$feedback), "\n\n")

# 7. Calculate class proportions to verify near-perfect balance
proportion_check <- function(data_set, name) {
  counts <- table(data_set$feedback)
  proportions <- prop.table(counts)
  cat(name, "class proportions:", round(proportions, 3), "\n")
}

cat("Class proportion verification:\n")
proportion_check(balanced_data, "Overall sample")
proportion_check(train_set_1, "Train set 1")
proportion_check(test_set_1, "Test set 1")
```






```{r, eval=FALSE}
library(tidyverse)
library(caret)

# Define the basic train_and_evaluate function
train_and_evaluate <- function(train_data, test_data) {
  # Train the model
  model <- glm(feedback ~ ., data = train_data, family = "binomial")
  
  # Make predictions
  predictions_prob <- predict(model, newdata = test_data, type = "response")
  predictions <- ifelse(predictions_prob > 0.5, "1", "-1")
  predictions <- factor(predictions, levels = levels(test_data$feedback))
  
  # Calculate accuracy
  accuracy <- mean(predictions == test_data$feedback)
  
  # Create confusion matrix
  cm <- confusionMatrix(predictions, test_data$feedback)
  
  return(list(
    model = model,
    accuracy = accuracy,
    confusion_matrix = cm
  ))
}

# 1. Assuming your existing dataframes are named:
# train_set_1, test_set_1, train_set_2, test_set_2, etc.

# 2. Combine all training data for clustering
# This gives us a larger dataset to identify more stable clusters
all_training_data <- rbind(train_set_1, train_set_2, train_set_3, train_set_4)

# 3. Perform k-means clustering on the combined training data
set.seed(123)  # For reproducibility
features_for_clustering <- all_training_data %>%
  dplyr::select(avg_spike_count, leftcontrast, rightcontrast)

# Scale the data for better clustering
scaled_features <- scale(features_for_clustering)


# Perform k-means clustering with optimal k (4 in this example)
k <- 4  # From your elbow method analysis
km_result <- kmeans(scaled_features, centers = k, nstart = 25)

# 4. Function to assign clusters to new data
assign_clusters <- function(new_data, km_model, original_center, original_scale) {
  #dplyr::select the same features used for clustering
  features <- new_data %>%
    dplyr::select(avg_spike_count, leftcontrast, rightcontrast)
  
  # Scale using the same parameters as the original data
  scaled_data <- scale(features, 
                      center = original_center,
                      scale = original_scale)
  
  # Calculate distance to each cluster center
  n_obs <- nrow(scaled_data)
  n_centers <- nrow(km_model$centers)
  distances <- matrix(0, nrow = n_obs, ncol = n_centers)
  
  for (i in 1:n_obs) {
    for (j in 1:n_centers) {
      distances[i, j] <- sqrt(sum((scaled_data[i,] - km_model$centers[j,])^2))
    }
  }
  
  # Assign each observation to the nearest cluster
  clusters <- apply(distances, 1, which.min)
  
  return(list(
    cluster = clusters,
    distances = distances
  ))
}

# 5. Add cluster assignments to all datasets
# For training sets
train_assignments_1 <- assign_clusters(
  train_set_1, 
  km_result, 
  attr(scaled_features, "scaled:center"),
  attr(scaled_features, "scaled:scale")
)
train_set_1_enhanced <- train_set_1 %>%
  mutate(cluster = factor(train_assignments_1$cluster))

train_assignments_2 <- assign_clusters(
  train_set_2, 
  km_result, 
  attr(scaled_features, "scaled:center"),
  attr(scaled_features, "scaled:scale")
)
train_set_2_enhanced <- train_set_2 %>%
  mutate(cluster = factor(train_assignments_2$cluster))

train_assignments_3 <- assign_clusters(
  train_set_3, 
  km_result, 
  attr(scaled_features, "scaled:center"),
  attr(scaled_features, "scaled:scale")
)
train_set_3_enhanced <- train_set_3 %>%
  mutate(cluster = factor(train_assignments_3$cluster))

train_assignments_4 <- assign_clusters(
  train_set_4, 
  km_result, 
  attr(scaled_features, "scaled:center"),
  attr(scaled_features, "scaled:scale")
)
train_set_4_enhanced <- train_set_4 %>%
  mutate(cluster = factor(train_assignments_4$cluster))

# For test sets
test_assignments_1 <- assign_clusters(
  test_set_1, 
  km_result, 
  attr(scaled_features, "scaled:center"),
  attr(scaled_features, "scaled:scale")
)
test_set_1_enhanced <- test_set_1 %>%
  mutate(cluster = factor(test_assignments_1$cluster))

test_assignments_2 <- assign_clusters(
  test_set_2, 
  km_result, 
  attr(scaled_features, "scaled:center"),
  attr(scaled_features, "scaled:scale")
)
test_set_2_enhanced <- test_set_2 %>%
  mutate(cluster = factor(test_assignments_2$cluster))

test_assignments_3 <- assign_clusters(
  test_set_3, 
  km_result, 
  attr(scaled_features, "scaled:center"),
  attr(scaled_features, "scaled:scale")
)
test_set_3_enhanced <- test_set_3 %>%
  mutate(cluster = factor(test_assignments_3$cluster))

test_assignments_4 <- assign_clusters(
  test_set_4, 
  km_result, 
  attr(scaled_features, "scaled:center"),
  attr(scaled_features, "scaled:scale")
)
test_set_4_enhanced <- test_set_4 %>%
  mutate(cluster = factor(test_assignments_4$cluster))

# 6. Compare models with and without clustering information (using fold 1)
# Model without cluster information
cat("Evaluating model WITHOUT clustering information:\n")
result_without_cluster <- train_and_evaluate(train_set_1, test_set_1)
cat("Accuracy without cluster:", round(result_without_cluster$accuracy * 100, 2), "%\n\n")

# Model with cluster information
cat("Evaluating model WITH clustering information:\n")
result_with_cluster <- train_and_evaluate(train_set_1_enhanced, test_set_1_enhanced)
cat("Accuracy with cluster:", round(result_with_cluster$accuracy * 100, 2), "%\n\n")

# 7. Create distance features
# Function to convert distances to features
distances_to_features <- function(distances) {
  distance_df <- as.data.frame(distances)
  colnames(distance_df) <- paste0("dist_to_cluster_", 1:ncol(distances))
  return(distance_df)
}

# Add distance features to train and test sets
train_distances_1 <- distances_to_features(train_assignments_1$distances)
train_set_1_with_distances <- cbind(train_set_1, train_distances_1)

test_distances_1 <- distances_to_features(test_assignments_1$distances)
test_set_1_with_distances <- cbind(test_set_1, test_distances_1)

# Train and evaluate model with distance features
cat("Evaluating model with DISTANCE features:\n")
result_with_distances <- train_and_evaluate(train_set_1_with_distances, test_set_1_with_distances)
cat("Accuracy with distance features:", round(result_with_distances$accuracy * 100, 2), "%\n\n")

# 8. Evaluate all 4 folds with the enhanced approach
evaluate_all_folds <- function() {
  results <- list()
  
  # Fold 1
  results[[1]] <- train_and_evaluate(train_set_1_enhanced, test_set_1_enhanced)
  
  # Fold 2
  results[[2]] <- train_and_evaluate(train_set_2_enhanced, test_set_2_enhanced)
  
  # Fold 3
  results[[3]] <- train_and_evaluate(train_set_3_enhanced, test_set_3_enhanced)
  
  # Fold 4
  results[[4]] <- train_and_evaluate(train_set_4_enhanced, test_set_4_enhanced)
  
  # Calculate overall accuracy
  accuracies <- sapply(results, function(x) x$accuracy)
  mean_accuracy <- mean(accuracies)
  
  # Print results
  cat("Results for all folds with cluster feature:\n")
  for (i in 1:4) {
    cat(sprintf("Fold %d accuracy: %.2f%%\n", i, accuracies[i] * 100))
  }
  cat(sprintf("\nMean accuracy across all folds: %.2f%%\n", mean_accuracy * 100))
  
  return(results)
}

# Run evaluation on all folds
all_fold_results <- evaluate_all_folds()

# 9. Visualize the importance of features (including cluster)
if (requireNamespace("randomForest", quietly = TRUE)) {
  library(randomForest)
  
  # Train a random forest model
  rf_model <- randomForest(feedback ~ ., data = train_set_1_enhanced, importance = TRUE)
  
  # Get variable importance
  importance_df <- as.data.frame(importance(rf_model))
  importance_df$Variable <- rownames(importance_df)
  
  # Plot variable importance
  ggplot(importance_df, aes(x = reorder(Variable, MeanDecreaseGini), y = MeanDecreaseGini)) +
    geom_col() +
    coord_flip() +
    labs(title = "Variable Importance (including cluster)",
         x = "Variable",
         y = "Mean Decrease in Gini") +
    theme_minimal()
}
```
I compared the pediction power of 






???????????????????

```{r, eval=FALSE}
library(randomForest)
library(ggplot2)

# Train a random forest model
rf_model <- randomForest(feedback ~ ., data = train_set_1_enhanced, importance = TRUE)

# Get variable importance
importance_df <- as.data.frame(importance(rf_model))
importance_df$Variable <- rownames(importance_df)

# Plot variable importance
ggplot(importance_df, aes(x = reorder(Variable, MeanDecreaseGini), y = MeanDecreaseGini)) +
  geom_col() +
  coord_flip() +
  labs(title = "Variable Importance (including cluster)",
       x = "Variable",
       y = "Mean Decrease in Gini") +
  theme_minimal()
```



I will now try a linear model: 
```{r, eval=FALSE}
library(tidyverse)
library(caret)
library(broom)  # For tidy model output

# Define the function to train and evaluate linear models
train_and_evaluate_linear <- function(train_data, test_data) {
  # Ensure feedback is numeric (-1 and 1) for linear model
  train_data$feedback_numeric <- as.numeric(as.character(train_data$feedback))
  test_data$feedback_numeric <- as.numeric(as.character(test_data$feedback))
  
  # Train the linear model
  model <- lm(feedback_numeric ~ ., data = train_data %>% dplyr::select(-feedback))
  
  # Make predictions
  predictions <- predict(model, newdata = test_data)
  
  # Convert predictions back to classification
  predicted_classes <- ifelse(predictions > 0, 1, -1)
  
  # Calculate accuracy
  accuracy <- mean(predicted_classes == test_data$feedback_numeric)
  
  # Create confusion matrix
  cm <- confusionMatrix(
    factor(predicted_classes, levels = c(-1, 1)),
    factor(test_data$feedback_numeric, levels = c(-1, 1))
  )
  
  return(list(
    model = model,
    predictions = predictions,
    predicted_classes = predicted_classes,
    accuracy = accuracy,
    confusion_matrix = cm,
    model_summary = summary(model)
  ))
}

# 1. Assuming your existing dataframes are already defined:
# train_set_1, test_set_1, train_set_2, test_set_2, etc.
# with the enhanced versions: train_set_1_enhanced, test_set_1_enhanced, etc.

# 2. Compare linear models with and without clustering information (using fold 1)
# Model without cluster information
cat("Evaluating LINEAR model WITHOUT clustering information:\n")
linear_result_without_cluster <- train_and_evaluate_linear(train_set_1, test_set_1)
cat("Accuracy without cluster:", round(linear_result_without_cluster$accuracy * 100, 2), "%\n\n")

# Print model summary
cat("Model summary (without clustering):\n")
print(linear_result_without_cluster$model_summary)

# Model with cluster information
cat("\nEvaluating LINEAR model WITH clustering information:\n")
linear_result_with_cluster <- train_and_evaluate_linear(train_set_1_enhanced, test_set_1_enhanced)
cat("Accuracy with cluster:", round(linear_result_with_cluster$accuracy * 100, 2), "%\n\n")

# Print model summary
cat("Model summary (with clustering):\n")
print(linear_result_with_cluster$model_summary)

# 3. Evaluate model with distance features
cat("\nEvaluating LINEAR model with DISTANCE features:\n")
linear_result_with_distances <- train_and_evaluate_linear(
  train_set_1_with_distances, 
  test_set_1_with_distances
)
cat("Accuracy with distance features:", round(linear_result_with_distances$accuracy * 100, 2), "%\n\n")

# 4. Evaluate all 4 folds with linear models
evaluate_all_folds_linear <- function() {
  results <- list()
  
  # Fold 1
  results[[1]] <- train_and_evaluate_linear(train_set_1_enhanced, test_set_1_enhanced)
  
  # Fold 2
  results[[2]] <- train_and_evaluate_linear(train_set_2_enhanced, test_set_2_enhanced)
  
  # Fold 3
  results[[3]] <- train_and_evaluate_linear(train_set_3_enhanced, test_set_3_enhanced)
  
  # Fold 4
  results[[4]] <- train_and_evaluate_linear(train_set_4_enhanced, test_set_4_enhanced)
  
  # Calculate overall accuracy
  accuracies <- sapply(results, function(x) x$accuracy)
  mean_accuracy <- mean(accuracies)
  
  # Print results
  cat("Results for all folds with LINEAR model (with clusters):\n")
  for (i in 1:4) {
    cat(sprintf("Fold %d accuracy: %.2f%%\n", i, accuracies[i] * 100))
  }
  cat(sprintf("\nMean accuracy across all folds: %.2f%%\n", mean_accuracy * 100))
  
  return(results)
}

# Run evaluation on all folds
all_fold_linear_results <- evaluate_all_folds_linear()

# 5. Examine coefficient significance from the linear model
# Get tidy summary of the model
if (requireNamespace("broom", quietly = TRUE)) {
  model_coefficients <- tidy(linear_result_with_cluster$model)
  
  # Sort by absolute value of estimate
  model_coefficients <- model_coefficients %>%
    mutate(abs_estimate = abs(estimate)) %>%
    arrange(desc(abs_estimate))
  
  print(model_coefficients)
  
  # Create a plot of coefficient estimates
  ggplot(model_coefficients %>% filter(term != "(Intercept)"), 
         aes(x = reorder(term, abs_estimate), y = estimate)) +
    geom_col() +
    geom_errorbar(aes(ymin = estimate - std.error, ymax = estimate + std.error), width = 0.2) +
    coord_flip() +
    labs(title = "Linear Model Coefficient Estimates",
         x = "Variable",
         y = "Coefficient Estimate") +
    theme_minimal()
}

# 6. Additional analysis: R-squared values to compare model fit
r_squared_without_cluster <- summary(linear_result_without_cluster$model)$r.squared
r_squared_with_cluster <- summary(linear_result_with_cluster$model)$r.squared
r_squared_with_distances <- summary(linear_result_with_distances$model)$r.squared

cat("\nR-squared comparison:\n")
cat("Linear model without cluster:", round(r_squared_without_cluster, 4), "\n")
cat("Linear model with cluster:", round(r_squared_with_cluster, 4), "\n")
cat("Linear model with distances:", round(r_squared_with_distances, 4), "\n")

# 7. Create an interaction plot to visualize how cluster membership affects the relationship
# between avg_spike_count and feedback
ggplot(train_set_1_enhanced, aes(x = avg_spike_count, y = as.numeric(as.character(feedback)), 
                                 color = cluster, group = cluster)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "lm", se = TRUE) +
  labs(title = "Relationship between Spike Count and Feedback by Cluster",
       x = "Average Spike Count",
       y = "Feedback (-1 to 1)",
       color = "Cluster") +
  theme_minimal()
```

I will now try xgboost:

```{r, eval=FALSE}
library(tidyverse)
library(caret)
library(xgboost)  # Make sure this is installed with install.packages("xgboost") if needed

# Define the function to train and evaluate XGBoost models
train_and_evaluate_xgb <- function(train_data, test_data, nrounds = 100) {
  # Prepare data for XGBoost
  # First, convert categorical variables to numeric
  train_data_xgb <- train_data %>% 
    mutate(across(where(is.factor), as.numeric)) %>%
    mutate(feedback_numeric = as.numeric(as.character(feedback)))
  
  test_data_xgb <- test_data %>% 
    mutate(across(where(is.factor), as.numeric)) %>%
    mutate(feedback_numeric = as.numeric(as.character(feedback)))
  
  # Create XGBoost DMatrix objects
  dtrain <- xgb.DMatrix(
    data = as.matrix(train_data_xgb %>% dplyr::select(-feedback, -feedback_numeric)),
    label = ifelse(train_data_xgb$feedback_numeric == 1, 1, 0)  # Convert to 0-1 for XGBoost
  )
  
  dtest <- xgb.DMatrix(
    data = as.matrix(test_data_xgb %>% dplyr::select(-feedback, -feedback_numeric)),
    label = ifelse(test_data_xgb$feedback_numeric == 1, 1, 0)
  )
  
  # Set XGBoost parameters
  params <- list(
    objective = "binary:logistic",
    eval_metric = "error",
    eta = 0.1,
    max_depth = 6,
    subsample = 0.8,
    colsample_bytree = 0.8
  )
  
  # Train the model
  xgb_model <- xgb.train(
    params = params,
    data = dtrain,
    nrounds = nrounds,
    watchlist = list(train = dtrain, test = dtest),
    verbose = 0  # Set to 1 to see training progress
  )
  
  # Make predictions
  predictions_prob <- predict(xgb_model, dtest)
  predictions <- ifelse(predictions_prob > 0.5, 1, -1)
  
  # Calculate accuracy
  accuracy <- mean(predictions == test_data_xgb$feedback_numeric)
  
  # Create confusion matrix
  cm <- confusionMatrix(
    factor(predictions, levels = c(-1, 1)),
    factor(test_data_xgb$feedback_numeric, levels = c(-1, 1))
  )
  
  # Get feature importance
  importance <- xgb.importance(
    feature_names = colnames(train_data_xgb %>% dplyr::select(-feedback, -feedback_numeric)),
    model = xgb_model
  )
  
  return(list(
    model = xgb_model,
    predictions = predictions,
    accuracy = accuracy,
    confusion_matrix = cm,
    importance = importance
  ))
}

# 1. Assuming your existing dataframes are already defined:
# train_set_1, test_set_1, train_set_2, test_set_2, etc.
# with the enhanced versions: train_set_1_enhanced, test_set_1_enhanced, etc.

# 2. Compare XGBoost models with and without clustering information (using fold 1)
# Set seed for reproducibility
set.seed(123)

# Model without cluster information
cat("Evaluating XGBoost model WITHOUT clustering information:\n")
xgb_result_without_cluster <- train_and_evaluate_xgb(train_set_1, test_set_1)
cat("Accuracy without cluster:", round(xgb_result_without_cluster$accuracy * 100, 2), "%\n\n")

# Print confusion matrix
print(xgb_result_without_cluster$confusion_matrix$table)

# Model with cluster information
cat("\nEvaluating XGBoost model WITH clustering information:\n")
xgb_result_with_cluster <- train_and_evaluate_xgb(train_set_1_enhanced, test_set_1_enhanced)
cat("Accuracy with cluster:", round(xgb_result_with_cluster$accuracy * 100, 2), "%\n\n")

# Print confusion matrix
print(xgb_result_with_cluster$confusion_matrix$table)

# 3. Evaluate model with distance features
cat("\nEvaluating XGBoost model with DISTANCE features:\n")
xgb_result_with_distances <- train_and_evaluate_xgb(
  train_set_1_with_distances, 
  test_set_1_with_distances
)
cat("Accuracy with distance features:", round(xgb_result_with_distances$accuracy * 100, 2), "%\n\n")

# Print confusion matrix
print(xgb_result_with_distances$confusion_matrix$table)

# 4. Evaluate all 4 folds with XGBoost
evaluate_all_folds_xgb <- function() {
  results <- list()
  
  # Fold 1
  results[[1]] <- train_and_evaluate_xgb(train_set_1_enhanced, test_set_1_enhanced)
  
  # Fold 2
  results[[2]] <- train_and_evaluate_xgb(train_set_2_enhanced, test_set_2_enhanced)
  
  # Fold 3
  results[[3]] <- train_and_evaluate_xgb(train_set_3_enhanced, test_set_3_enhanced)
  
  # Fold 4
  results[[4]] <- train_and_evaluate_xgb(train_set_4_enhanced, test_set_4_enhanced)
  
  # Calculate overall accuracy
  accuracies <- sapply(results, function(x) x$accuracy)
  mean_accuracy <- mean(accuracies)
  
  # Print results
  cat("Results for all folds with XGBoost model (with clusters):\n")
  for (i in 1:4) {
    cat(sprintf("Fold %d accuracy: %.2f%%\n", i, accuracies[i] * 100))
  }
  cat(sprintf("\nMean accuracy across all folds: %.2f%%\n", mean_accuracy * 100))
  
  return(results)
}

# Run evaluation on all folds
all_fold_xgb_results <- evaluate_all_folds_xgb()

# 5. Plot feature importance from the XGBoost model
# For model with cluster
importance_with_cluster <- xgb_result_with_cluster$importance
if (nrow(importance_with_cluster) > 0) {
  ggplot(importance_with_cluster, aes(x = reorder(Feature, Gain), y = Gain)) +
    geom_col() +
    coord_flip() +
    labs(title = "XGBoost Feature Importance (with cluster)",
         x = "Feature",
         y = "Gain") +
    theme_minimal()
}

# 6. Hyperparameter Tuning for XGBoost (Optional - can be time-consuming)
# This section uses a simplified grid search for demonstration
tune_xgboost <- function(train_data, test_data) {
  # Define parameter grid
  param_grid <- expand.grid(
    eta = c(0.01, 0.1, 0.3),
    max_depth = c(3, 6, 9),
    nrounds = c(50, 100, 200)
  )
  
  # Initialize results
  results <- list()
  
  cat("Starting hyperparameter tuning for XGBoost (this may take a while)...\n")
  
  # Loop through parameter combinations
  for (i in 1:nrow(param_grid)) {
    # Extract parameters
    eta_val <- param_grid$eta[i]
    max_depth_val <- param_grid$max_depth[i]
    nrounds_val <- param_grid$nrounds[i]
    
    # Prepare data for XGBoost
    train_data_xgb <- train_data %>% 
      mutate(across(where(is.factor), as.numeric)) %>%
      mutate(feedback_numeric = as.numeric(as.character(feedback)))
    
    test_data_xgb <- test_data %>% 
      mutate(across(where(is.factor), as.numeric)) %>%
      mutate(feedback_numeric = as.numeric(as.character(feedback)))
    
    # Create DMatrix objects
    dtrain <- xgb.DMatrix(
      data = as.matrix(train_data_xgb %>%dplyr::select(-feedback, -feedback_numeric)),
      label = ifelse(train_data_xgb$feedback_numeric == 1, 1, 0)
    )
    
    dtest <- xgb.DMatrix(
      data = as.matrix(test_data_xgb %>%dplyr::select(-feedback, -feedback_numeric)),
      label = ifelse(test_data_xgb$feedback_numeric == 1, 1, 0)
    )
    
    # Set parameters
    params <- list(
      objective = "binary:logistic",
      eval_metric = "error",
      eta = eta_val,
      max_depth = max_depth_val,
      subsample = 0.8,
      colsample_bytree = 0.8
    )
    
    # Train model
    xgb_model <- xgb.train(
      params = params,
      data = dtrain,
      nrounds = nrounds_val,
      watchlist = list(train = dtrain, test = dtest),
      verbose = 0
    )
    
    # Evaluate
    predictions_prob <- predict(xgb_model, dtest)
    predictions <- ifelse(predictions_prob > 0.5, 1, -1)
    accuracy <- mean(predictions == test_data_xgb$feedback_numeric)
    
    # Store results
    results[[i]] <- list(
      params = list(
        eta = eta_val,
        max_depth = max_depth_val,
        nrounds = nrounds_val
      ),
      accuracy = accuracy
    )
    
    cat(sprintf("Params %d/%d: eta=%.2f, max_depth=%d, nrounds=%d, accuracy=%.4f\n", 
                i, nrow(param_grid), eta_val, max_depth_val, nrounds_val, accuracy))
  }
  
  # Find best parameters
  accuracies <- sapply(results, function(x) x$accuracy)
  best_idx <- which.max(accuracies)
  best_params <- results[[best_idx]]$params
  best_accuracy <- accuracies[best_idx]
  
  cat("\nBest parameters:\n")
  cat(sprintf("eta: %.2f\n", best_params$eta))
  cat(sprintf("max_depth: %d\n", best_params$max_depth))
  cat(sprintf("nrounds: %d\n", best_params$nrounds))
  cat(sprintf("Accuracy: %.4f\n", best_accuracy))
  
  return(list(
    best_params = best_params,
    best_accuracy = best_accuracy,
    all_results = results
  ))
}

# Uncomment to run hyperparameter tuning (warning: can be time-consuming)
# tuning_results <- tune_xgboost(train_set_1_enhanced, test_set_1_enhanced)

# 7. Compare performance across different models
cat("\nModel Performance Comparison:\n")
cat("Logistic Regression (if you ran it before): [Insert accuracy here]\n")
cat("Linear Model (if you ran it before): [Insert accuracy here]\n")
cat("XGBoost without clustering:", round(xgb_result_without_cluster$accuracy * 100, 2), "%\n")
cat("XGBoost with clustering:", round(xgb_result_with_cluster$accuracy * 100, 2), "%\n")
cat("XGBoost with distance features:", round(xgb_result_with_distances$accuracy * 100, 2), "%\n")
```



I will now try Lasso regression:

```{r, eval=FALSE}
library(tidyverse)
library(caret)
library(glmnet)  # For lasso regression

# Define the function to train and evaluate Lasso models - FIXED VERSION WITH NA HANDLING
train_and_evaluate_lasso <- function(train_data, test_data, alpha = 1) {
  # First, check for any NA values
  if(sum(is.na(train_data)) > 0) {
    cat("Warning: Training data contains", sum(is.na(train_data)), "NA values\n")
    cat("Removing NA values from training data\n")
    train_data <- na.omit(train_data)
  }
  
  if(sum(is.na(test_data)) > 0) {
    cat("Warning: Test data contains", sum(is.na(test_data)), "NA values\n")
    cat("Removing NA values from test data\n")
    test_data <- na.omit(test_data)
  }
  
  # Make a copy of the data to avoid modifying the original
  train_data_copy <- train_data
  test_data_copy <- test_data
  
  # Convert feedback to numeric (-1 and 1)
  train_data_copy$feedback_numeric <- as.numeric(as.character(train_data_copy$feedback))
  test_data_copy$feedback_numeric <- as.numeric(as.character(test_data_copy$feedback))
  
  # Create matrices for Lasso - use only predictors, not the response variables
  predictors <- setdiff(names(train_data_copy), c("feedback", "feedback_numeric"))
  
  # Print predictors to verify
  cat("Using these predictors:", paste(predictors, collapse=", "), "\n")
  
  # Ensure all variables are numeric
  train_data_matrix <- train_data_copy %>%
   dplyr::select(all_of(predictors)) %>%
    mutate(across(everything(), as.numeric)) %>%
    as.matrix()
  
  test_data_matrix <- test_data_copy %>%
   dplyr::select(all_of(predictors)) %>%
    mutate(across(everything(), as.numeric)) %>%
    as.matrix()
  
  # Verify no NA values in matrices
  if(sum(is.na(train_data_matrix)) > 0) {
    stop("Training matrix still contains NA values after conversion. Please check your data.")
  }
  
  if(sum(is.na(test_data_matrix)) > 0) {
    stop("Test matrix still contains NA values after conversion. Please check your data.")
  }
  
  y_train <- train_data_copy$feedback_numeric
  y_test <- test_data_copy$feedback_numeric
  
  # Find optimal lambda using cross-validation
  set.seed(123)
  cv_model <- cv.glmnet(train_data_matrix, y_train, alpha = alpha, nfolds = 5)
  best_lambda <- cv_model$lambda.min
  
  # Train the Lasso model with optimal lambda
  lasso_model <- glmnet(train_data_matrix, y_train, alpha = alpha, lambda = best_lambda)
  
  # Make predictions
  predictions <- predict(lasso_model, newx = test_data_matrix, s = best_lambda)
  
  # Convert predictions back to classification
  predicted_classes <- ifelse(predictions > 0, 1, -1)
  
  # Calculate accuracy
  accuracy <- mean(predicted_classes == y_test)
  
  # Create confusion matrix
  cm <- confusionMatrix(
    factor(predicted_classes, levels = c(-1, 1)),
    factor(y_test, levels = c(-1, 1))
  )
  
  return(list(
    model = lasso_model,
    cv_model = cv_model,
    best_lambda = best_lambda,
    predictions = predictions,
    predicted_classes = predicted_classes,
    accuracy = accuracy,
    confusion_matrix = cm,
    coefficients = as.matrix(coef(lasso_model, s = best_lambda))
  ))
}

# Debugging - print column information before running models
print_dataset_info <- function(data, name) {
  cat("\nInformation for dataset:", name, "\n")
  cat("Dimensions:", dim(data)[1], "rows,", dim(data)[2], "columns\n")
  cat("Column names:", paste(names(data), collapse=", "), "\n")
  cat("Column classes:\n")
  for(col in names(data)) {
    cat("  ", col, ":", class(data[[col]]), "\n")
  }
  cat("NA counts:\n")
  for(col in names(data)) {
    na_count <- sum(is.na(data[[col]]))
    if(na_count > 0) {
      cat("  ", col, ":", na_count, "NAs\n")
    }
  }
  cat("Feedback distribution:", table(data$feedback), "\n\n")
}

# Run the debug function to check data
print_dataset_info(train_set_1, "train_set_1")
print_dataset_info(test_set_1, "test_set_1")
if(exists("train_set_1_enhanced")) {
  print_dataset_info(train_set_1_enhanced, "train_set_1_enhanced")
}

# Make sure feedback is properly formatted as a factor with correct levels
train_set_1$feedback <- factor(train_set_1$feedback, levels = c("-1", "1"))
test_set_1$feedback <- factor(test_set_1$feedback, levels = c("-1", "1"))

if(exists("train_set_1_enhanced")) {
  train_set_1_enhanced$feedback <- factor(train_set_1_enhanced$feedback, levels = c("-1", "1"))
  test_set_1_enhanced$feedback <- factor(test_set_1_enhanced$feedback, levels = c("-1", "1"))
}

if(exists("train_set_1_with_distances")) {
  train_set_1_with_distances$feedback <- factor(train_set_1_with_distances$feedback, levels = c("-1", "1"))
  test_set_1_with_distances$feedback <- factor(test_set_1_with_distances$feedback, levels = c("-1", "1"))
}

# 2. Compare Lasso models with and without clustering information (using fold 1)
tryCatch({
  # Model without cluster information
  cat("Evaluating Lasso model WITHOUT clustering information:\n")
  lasso_result_without_cluster <- train_and_evaluate_lasso(train_set_1, test_set_1)
  cat("Accuracy without cluster:", round(lasso_result_without_cluster$accuracy * 100, 2), "%\n")
  cat("Best lambda:", lasso_result_without_cluster$best_lambda, "\n\n")
  
  # Print confusion matrix
  print(lasso_result_without_cluster$confusion_matrix$table)
}, error = function(e) {
  cat("Error in model without clustering:", e$message, "\n")
})

tryCatch({
  # Model with cluster information
  cat("\nEvaluating Lasso model WITH clustering information:\n")
  if(exists("train_set_1_enhanced") && exists("test_set_1_enhanced")) {
    lasso_result_with_cluster <- train_and_evaluate_lasso(train_set_1_enhanced, test_set_1_enhanced)
    cat("Accuracy with cluster:", round(lasso_result_with_cluster$accuracy * 100, 2), "%\n")
    cat("Best lambda:", lasso_result_with_cluster$best_lambda, "\n\n")
    
    # Print confusion matrix
    print(lasso_result_with_cluster$confusion_matrix$table)
  } else {
    cat("Enhanced datasets not found. Please run clustering code first.\n")
  }
}, error = function(e) {
  cat("Error in model with clustering:", e$message, "\n")
})

tryCatch({
  # Evaluate model with distance features
  cat("\nEvaluating Lasso model with DISTANCE features:\n")
  if(exists("train_set_1_with_distances") && exists("test_set_1_with_distances")) {
    lasso_result_with_distances <- train_and_evaluate_lasso(
      train_set_1_with_distances, 
      test_set_1_with_distances
    )
    cat("Accuracy with distance features:", round(lasso_result_with_distances$accuracy * 100, 2), "%\n")
    cat("Best lambda:", lasso_result_with_distances$best_lambda, "\n\n")
    
    # Print confusion matrix
    print(lasso_result_with_distances$confusion_matrix$table)
  } else {
    cat("Distance feature datasets not found. Please run clustering with distances code first.\n")
  }
}, error = function(e) {
  cat("Error in model with distance features:", e$message, "\n")
})

# 4. Evaluate all 4 folds with Lasso
evaluate_all_folds_lasso <- function() {
  results <- list()
  
  tryCatch({
    # Fold 1
    cat("Processing fold 1...\n")
    results[[1]] <- train_and_evaluate_lasso(train_set_1_enhanced, test_set_1_enhanced)
    
    # Fold 2
    cat("Processing fold 2...\n")
    results[[2]] <- train_and_evaluate_lasso(train_set_2_enhanced, test_set_2_enhanced)
    
    # Fold 3
    cat("Processing fold 3...\n")
    results[[3]] <- train_and_evaluate_lasso(train_set_3_enhanced, test_set_3_enhanced)
    
    # Fold 4
    cat("Processing fold 4...\n")
    results[[4]] <- train_and_evaluate_lasso(train_set_4_enhanced, test_set_4_enhanced)
    
    # Calculate overall accuracy
    accuracies <- sapply(results, function(x) x$accuracy)
    mean_accuracy <- mean(accuracies)
    
    # Print results
    cat("Results for all folds with Lasso model (with clusters):\n")
    for (i in 1:4) {
      cat(sprintf("Fold %d accuracy: %.2f%%\n", i, accuracies[i] * 100))
    }
    cat(sprintf("\nMean accuracy across all folds: %.2f%%\n", mean_accuracy * 100))
  }, error = function(e) {
    cat("Error in evaluating folds:", e$message, "\n")
  })
  
  return(results)
}

# Run evaluation on all folds
if(all(exists("train_set_1_enhanced"), exists("train_set_2_enhanced"), 
       exists("train_set_3_enhanced"), exists("train_set_4_enhanced"))) {
  cat("Running evaluation on all folds...\n")
  all_fold_lasso_results <- evaluate_all_folds_lasso()
} else {
  cat("Cannot evaluate all folds: Missing one or more enhanced datasets.\n")
}
```

Now I am trying LDA:

```{r, eval=FALSE}
library(tidyverse)
library(caret)
library(MASS)  # For LDA function

# Define the function to train and evaluate LDA models
train_and_evaluate_lda <- function(train_data, test_data) {
  # Check for any NA values
  if(sum(is.na(train_data)) > 0) {
    cat("Warning: Training data contains", sum(is.na(train_data)), "NA values\n")
    cat("Removing NA values from training data\n")
    train_data <- na.omit(train_data)
  }
  
  if(sum(is.na(test_data)) > 0) {
    cat("Warning: Test data contains", sum(is.na(test_data)), "NA values\n")
    cat("Removing NA values from test data\n")
    test_data <- na.omit(test_data)
  }
  
  # Train the LDA model
  lda_model <- lda(feedback ~ ., data = train_data)
  
  # Make predictions
  predictions <- predict(lda_model, newdata = test_data)
  
  # Extract the class predictions
  predicted_classes <- predictions$class
  
  # Calculate accuracy
  accuracy <- mean(predicted_classes == test_data$feedback)
  
  # Create confusion matrix
  cm <- confusionMatrix(predicted_classes, test_data$feedback)
  
  return(list(
    model = lda_model,
    predictions = predictions,
    predicted_classes = predicted_classes,
    accuracy = accuracy,
    confusion_matrix = cm
  ))
}

# 1. Assuming your datasets are already created with balanced sampling
# Run the clustering on the train and test sets if not already done

# If you haven't run clustering yet, here's a function to do it
run_clustering <- function(train_sets, test_sets, k = 4) {
  # Combine all training data for better clustering
  all_train_data <- bind_rows(train_sets)
  
  # Features for clustering
  features <- all_train_data %>%dplyr::select(avg_spike_count, leftcontrast, rightcontrast)
  scaled_features <- scale(features)
  
  # Run k-means
  set.seed(123)
  km_result <- kmeans(scaled_features, centers = k, nstart = 25)
  
  # Function to assign clusters and calculate distances
  assign_clusters <- function(data) {
    # Scale using the same parameters
    scaled_data <- scale(
      data %>%dplyr::select(avg_spike_count, leftcontrast, rightcontrast), 
      center = attr(scaled_features, "scaled:center"),
      scale = attr(scaled_features, "scaled:scale")
    )
    
    # Calculate distances to centroids
    distances <- matrix(0, nrow = nrow(scaled_data), ncol = k)
    for (i in 1:nrow(scaled_data)) {
      for (j in 1:k) {
        distances[i, j] <- sqrt(sum((scaled_data[i,] - km_result$centers[j,])^2))
      }
    }
    
    # Find closest centroid
    clusters <- apply(distances, 1, which.min)
    
    # Create enhanced dataset with cluster
    enhanced <- data %>% mutate(cluster = factor(clusters))
    
    # Create dataset with distances
    distance_df <- as.data.frame(distances)
    names(distance_df) <- paste0("dist_to_cluster_", 1:k)
    with_distances <- cbind(data, distance_df)
    
    return(list(enhanced = enhanced, with_distances = with_distances))
  }
  
  # Process all datasets
  train_enhanced <- list()
  train_distances <- list()
  test_enhanced <- list()
  test_distances <- list()
  
  for (i in 1:length(train_sets)) {
    result_train <- assign_clusters(train_sets[[i]])
    train_enhanced[[i]] <- result_train$enhanced
    train_distances[[i]] <- result_train$with_distances
    
    result_test <- assign_clusters(test_sets[[i]])
    test_enhanced[[i]] <- result_test$enhanced
    test_distances[[i]] <- result_test$with_distances
  }
  
  return(list(
    train_enhanced = train_enhanced,
    train_distances = train_distances,
    test_enhanced = test_enhanced,
    test_distances = test_distances,
    km_result = km_result
  ))
}

# Check if cluster enhanced datasets exist, otherwise create them
if (!exists("train_set_1_enhanced")) {
  cat("Running clustering to create enhanced datasets...\n")
  # Put your datasets into lists
  train_sets <- list(train_set_1, train_set_2, train_set_3, train_set_4)
  test_sets <- list(test_set_1, test_set_2, test_set_3, test_set_4)
  
  # Run clustering
  cluster_results <- run_clustering(train_sets, test_sets)
  
  # Extract enhanced datasets
  train_set_1_enhanced <- cluster_results$train_enhanced[[1]]
  train_set_2_enhanced <- cluster_results$train_enhanced[[2]]
  train_set_3_enhanced <- cluster_results$train_enhanced[[3]]
  train_set_4_enhanced <- cluster_results$train_enhanced[[4]]
  
  test_set_1_enhanced <- cluster_results$test_enhanced[[1]]
  test_set_2_enhanced <- cluster_results$test_enhanced[[2]]
  test_set_3_enhanced <- cluster_results$test_enhanced[[3]]
  test_set_4_enhanced <- cluster_results$test_enhanced[[4]]
  
  # Extract datasets with distance features
  train_set_1_with_distances <- cluster_results$train_distances[[1]]
  train_set_2_with_distances <- cluster_results$train_distances[[2]]
  train_set_3_with_distances <- cluster_results$train_distances[[3]]
  train_set_4_with_distances <- cluster_results$train_distances[[4]]
  
  test_set_1_with_distances <- cluster_results$test_distances[[1]]
  test_set_2_with_distances <- cluster_results$test_distances[[2]]
  test_set_3_with_distances <- cluster_results$test_distances[[3]]
  test_set_4_with_distances <- cluster_results$test_distances[[4]]
}

# 2. Compare LDA models with and without clustering information (using fold 1)
# Model without cluster information
cat("Evaluating LDA model WITHOUT clustering information:\n")
lda_result_without_cluster <- train_and_evaluate_lda(train_set_1, test_set_1)
cat("Accuracy without cluster:", round(lda_result_without_cluster$accuracy * 100, 2), "%\n\n")

# Print confusion matrix
print(lda_result_without_cluster$confusion_matrix$table)

# Model with cluster information
cat("\nEvaluating LDA model WITH clustering information:\n")
lda_result_with_cluster <- train_and_evaluate_lda(train_set_1_enhanced, test_set_1_enhanced)
cat("Accuracy with cluster:", round(lda_result_with_cluster$accuracy * 100, 2), "%\n\n")

# Print confusion matrix
print(lda_result_with_cluster$confusion_matrix$table)

# 3. Evaluate model with distance features
cat("\nEvaluating LDA model with DISTANCE features:\n")
lda_result_with_distances <- train_and_evaluate_lda(
  train_set_1_with_distances, 
  test_set_1_with_distances
)
cat("Accuracy with distance features:", round(lda_result_with_distances$accuracy * 100, 2), "%\n\n")

# Print confusion matrix
print(lda_result_with_distances$confusion_matrix$table)

# 4. Evaluate all 4 folds with LDA
evaluate_all_folds_lda <- function() {
  results <- list()
  
  # Fold 1
  cat("Processing fold 1...\n")
  results[[1]] <- train_and_evaluate_lda(train_set_1_enhanced, test_set_1_enhanced)
  
  # Fold 2
  cat("Processing fold 2...\n")
  results[[2]] <- train_and_evaluate_lda(train_set_2_enhanced, test_set_2_enhanced)
  
  # Fold 3
  cat("Processing fold 3...\n")
  results[[3]] <- train_and_evaluate_lda(train_set_3_enhanced, test_set_3_enhanced)
  
  # Fold 4
  cat("Processing fold 4...\n")
  results[[4]] <- train_and_evaluate_lda(train_set_4_enhanced, test_set_4_enhanced)
  
  # Calculate overall accuracy
  accuracies <- sapply(results, function(x) x$accuracy)
  mean_accuracy <- mean(accuracies)
  
  # Print results
  cat("Results for all folds with LDA model (with clusters):\n")
  for (i in 1:4) {
    cat(sprintf("Fold %d accuracy: %.2f%%\n", i, accuracies[i] * 100))
  }
  cat(sprintf("\nMean accuracy across all folds: %.2f%%\n", mean_accuracy * 100))
  
  return(results)
}

# Run evaluation on all folds
all_fold_lda_results <- evaluate_all_folds_lda()

# 5. Examine the LDA model's coefficients and insights
# Display LDA coefficients for the model with clustering
cat("\nLDA Coefficients (with clustering):\n")
print(lda_result_with_cluster$model$scaling)

# Calculate variable importance based on absolute coefficient values
lda_coef <- as.data.frame(abs(lda_result_with_cluster$model$scaling))
lda_coef$Variable <- rownames(lda_coef)
names(lda_coef)[1] <- "Importance"
lda_coef <- lda_coef %>% arrange(desc(Importance))

# Plot variable importance
ggplot(lda_coef, aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_col() +
  coord_flip() +
  labs(title = "LDA Variable Importance",
       x = "Variable",
       y = "Absolute Coefficient Value") +
  theme_minimal()

# 6. Visualize LDA's discriminant function
# Create a data frame with LDA values
if(exists("lda_result_with_cluster") && !is.null(lda_result_with_cluster$predictions)) {
  lda_plot_data <- data.frame(
    LD1 = lda_result_with_cluster$predictions$x,
    feedback = test_set_1_enhanced$feedback
  )
  
  # Add cluster information if available
  if("cluster" %in% names(test_set_1_enhanced)) {
    lda_plot_data$cluster <- test_set_1_enhanced$cluster
  }
  
  # Plot the LDA scores
  ggplot(lda_plot_data, aes(x = LD1, fill = feedback)) +
    geom_density(alpha = 0.5) +
    labs(title = "LDA Discriminant Scores",
         x = "Linear Discriminant 1",
         y = "Density") +
    theme_minimal()
  
  # If we have cluster information, make a scatter plot with clusters
  if("cluster" %in% names(lda_plot_data)) {
    ggplot(lda_plot_data, aes(x = LD1, y = as.numeric(feedback), color = cluster)) +
      geom_jitter(width = 0, height = 0.05, alpha = 0.5) +
      labs(title = "LDA Scores by Cluster",
           x = "Linear Discriminant 1",
           y = "Feedback (-1/1)") +
      theme_minimal()
  }
}

# 7. Compare performance across different models
cat("\nModel Performance Comparison:\n")
cat("LDA without clustering:", round(lda_result_without_cluster$accuracy * 100, 2), "%\n")
cat("LDA with clustering:", round(lda_result_with_cluster$accuracy * 100, 2), "%\n")
cat("LDA with distance features:", round(lda_result_with_distances$accuracy * 100, 2), "%\n")
```

Now I am trying kNN:

```{r, eval=FALSE}
library(tidyverse)
library(caret)
library(class)  # For knn function

# Define the function to train and evaluate kNN models - FIXED VERSION
train_and_evaluate_knn <- function(train_data, test_data, k_value = 5) {
  # Check for any NA values
  if(sum(is.na(train_data)) > 0) {
    cat("Warning: Training data contains", sum(is.na(train_data)), "NA values\n")
    cat("Removing NA values from training data\n")
    train_data <- na.omit(train_data)
  }
  
  if(sum(is.na(test_data)) > 0) {
    cat("Warning: Test data contains", sum(is.na(test_data)), "NA values\n")
    cat("Removing NA values from test data\n")
    test_data <- na.omit(test_data)
  }
  
  # Check if feedback column exists
  if(!"feedback" %in% names(train_data) || !"feedback" %in% names(test_data)) {
    stop("Error: 'feedback' column not found in data. Please check your column names.")
  }
  
  # Print column names for debugging
  cat("Train data columns:", paste(names(train_data), collapse=", "), "\n")
  cat("Test data columns:", paste(names(test_data), collapse=", "), "\n")
  
  # Separate features and labels
  train_features <- train_data %>% dplyr::select(-one_of("feedback"))
  train_labels <- train_data$feedback
  
  test_features <- test_data %>% dplyr::select(-one_of("feedback"))
  test_labels <- test_data$feedback
  
  # Scale the features (important for kNN)
  scale_params <- preProcess(train_features, method = c("center", "scale"))
  train_features_scaled <- predict(scale_params, train_features)
  test_features_scaled <- predict(scale_params, test_features)
  
  # Train the kNN model and make predictions
  predicted_classes <- knn(
    train = train_features_scaled,
    test = test_features_scaled,
    cl = train_labels,
    k = k_value
  )
  
  # Calculate accuracy
  accuracy <- mean(predicted_classes == test_labels)
  
  # Create confusion matrix
  cm <- confusionMatrix(predicted_classes, test_labels)
  
  return(list(
    k_value = k_value,
    predicted_classes = predicted_classes,
    accuracy = accuracy,
    confusion_matrix = cm,
    scale_params = scale_params
  ))
}

# Function to find optimal k value for kNN
find_optimal_k <- function(train_data, test_data, k_range = 1:20) {
  # Make sure both datasets have the required columns
  if(!"feedback" %in% names(train_data) || !"feedback" %in% names(test_data)) {
    stop("Error: 'feedback' column not found in data. Please check your column names.")
  }
  
  k_results <- data.frame(k = integer(), accuracy = numeric())
  
  for (k in k_range) {
    cat("Testing k =", k, "\n")
    result <- tryCatch({
      train_and_evaluate_knn(train_data, test_data, k_value = k)
    }, error = function(e) {
      cat("Error with k =", k, ":", e$message, "\n")
      return(NULL)
    })
    
    if(!is.null(result)) {
      k_results <- rbind(k_results, data.frame(k = k, accuracy = result$accuracy))
    }
  }
  
  if(nrow(k_results) == 0) {
    stop("All k values failed. Check your data structure.")
  }
  
  # Find optimal k
  optimal_k <- k_results$k[which.max(k_results$accuracy)]
  
  # Plot results
  p <- ggplot(k_results, aes(x = k, y = accuracy)) +
    geom_line() +
    geom_point() +
    geom_vline(xintercept = optimal_k, linetype = "dashed", color = "red") +
    labs(title = "kNN Performance by k Value",
         subtitle = paste("Optimal k =", optimal_k),
         x = "k (Number of Neighbors)",
         y = "Accuracy") +
    theme_minimal()
  
  print(p)
  
  cat("Optimal k value:", optimal_k, "with accuracy:", 
      round(max(k_results$accuracy) * 100, 2), "%\n")
  
  return(optimal_k)
}

# Debug function to check data structure
check_data_structure <- function(data, name) {
  cat("\nChecking structure of", name, ":\n")
  cat("Dimensions:", dim(data)[1], "rows,", dim(data)[2], "columns\n")
  cat("Column names:", paste(names(data), collapse=", "), "\n")
  cat("First few rows of data:\n")
  print(head(data, 3))
  
  # Check feedback column
  if("feedback" %in% names(data)) {
    cat("Feedback column exists and has class:", class(data$feedback), "\n")
    cat("Feedback distribution:", table(data$feedback), "\n")
  } else {
    cat("WARNING: 'feedback' column not found!\n")
  }
  cat("\n")
}

# Check your dataset structure
check_data_structure(train_set_1, "train_set_1")
check_data_structure(test_set_1, "test_set_1")

# 1. Find optimal k value using fold 1
tryCatch({
  cat("Finding optimal k value for kNN...\n")
  optimal_k <- find_optimal_k(train_set_1, test_set_1, k_range = 1:15)
}, error = function(e) {
  cat("Error finding optimal k:", e$message, "\n")
  # Fallback to a reasonable default
  cat("Using default k = 5 instead\n")
  optimal_k <<- 5
})

# 2. Compare kNN models with and without clustering information (using fold 1)
# Model without cluster information
tryCatch({
  cat("\nEvaluating kNN model WITHOUT clustering information:\n")
  knn_result_without_cluster <- train_and_evaluate_knn(train_set_1, test_set_1, k_value = optimal_k)
  cat("Accuracy without cluster:", round(knn_result_without_cluster$accuracy * 100, 2), "%\n\n")
  
  # Print confusion matrix
  print(knn_result_without_cluster$confusion_matrix$table)
}, error = function(e) {
  cat("Error in basic kNN model:", e$message, "\n")
})

# Model with cluster information
if(exists("train_set_1_enhanced") && exists("test_set_1_enhanced")) {
  tryCatch({
    cat("\nEvaluating kNN model WITH clustering information:\n")
    knn_result_with_cluster <- train_and_evaluate_knn(train_set_1_enhanced, test_set_1_enhanced, k_value = optimal_k)
    cat("Accuracy with cluster:", round(knn_result_with_cluster$accuracy * 100, 2), "%\n\n")
    
    # Print confusion matrix
    print(knn_result_with_cluster$confusion_matrix$table)
  }, error = function(e) {
    cat("Error in cluster-enhanced kNN model:", e$message, "\n")
    check_data_structure(train_set_1_enhanced, "train_set_1_enhanced")
  })
} else {
  cat("\nSkipping cluster-enhanced model - datasets not found\n")
}

# Model with distance features
if(exists("train_set_1_with_distances") && exists("test_set_1_with_distances")) {
  tryCatch({
    cat("\nEvaluating kNN model with DISTANCE features:\n")
    knn_result_with_distances <- train_and_evaluate_knn(
      train_set_1_with_distances, 
      test_set_1_with_distances,
      k_value = optimal_k
    )
    cat("Accuracy with distance features:", round(knn_result_with_distances$accuracy * 100, 2), "%\n\n")
    
    # Print confusion matrix
    print(knn_result_with_distances$confusion_matrix$table)
  }, error = function(e) {
    cat("Error in distance-enhanced kNN model:", e$message, "\n")
  })
} else {
  cat("\nSkipping distance-enhanced model - datasets not found\n")
}

# 3. Evaluate all 4 folds with kNN
evaluate_all_folds_knn <- function(k_value) {
  results <- list()
  
  # Check if all datasets exist
  all_sets_exist <- all(exists("train_set_1_enhanced"), exists("test_set_1_enhanced"),
                        exists("train_set_2_enhanced"), exists("test_set_2_enhanced"),
                        exists("train_set_3_enhanced"), exists("test_set_3_enhanced"),
                        exists("train_set_4_enhanced"), exists("test_set_4_enhanced"))
  
  if(!all_sets_exist) {
    cat("Cannot evaluate all folds - some datasets are missing\n")
    return(NULL)
  }
  
  # Fold 1
  tryCatch({
    cat("Processing fold 1...\n")
    results[[1]] <- train_and_evaluate_knn(train_set_1_enhanced, test_set_1_enhanced, k_value = k_value)
  }, error = function(e) {
    cat("Error in fold 1:", e$message, "\n")
    results[[1]] <- list(accuracy = NA)
  })
  
  # Fold 2
  tryCatch({
    cat("Processing fold 2...\n")
    results[[2]] <- train_and_evaluate_knn(train_set_2_enhanced, test_set_2_enhanced, k_value = k_value)
  }, error = function(e) {
    cat("Error in fold 2:", e$message, "\n")
    results[[2]] <- list(accuracy = NA)
  })
  
  # Fold 3
  tryCatch({
    cat("Processing fold 3...\n")
    results[[3]] <- train_and_evaluate_knn(train_set_3_enhanced, test_set_3_enhanced, k_value = k_value)
  }, error = function(e) {
    cat("Error in fold 3:", e$message, "\n")
    results[[3]] <- list(accuracy = NA)
  })
  
  # Fold 4
  tryCatch({
    cat("Processing fold 4...\n")
    results[[4]] <- train_and_evaluate_knn(train_set_4_enhanced, test_set_4_enhanced, k_value = k_value)
  }, error = function(e) {
    cat("Error in fold 4:", e$message, "\n")
    results[[4]] <- list(accuracy = NA)
  })
  
  # Calculate overall accuracy (ignoring NA values)
  accuracies <- sapply(results, function(x) x$accuracy)
  valid_accuracies <- accuracies[!is.na(accuracies)]
  
  if(length(valid_accuracies) > 0) {
    mean_accuracy <- mean(valid_accuracies)
    
    # Print results
    cat("Results for all folds with kNN model (with clusters):\n")
    for (i in 1:4) {
      if(!is.na(accuracies[i])) {
        cat(sprintf("Fold %d accuracy: %.2f%%\n", i, accuracies[i] * 100))
      } else {
        cat(sprintf("Fold %d: Failed\n", i))
      }
    }
    cat(sprintf("\nMean accuracy across valid folds: %.2f%%\n", mean_accuracy * 100))
  } else {
    cat("No valid results to report\n")
  }
  
  return(results)
}

# Run evaluation on all folds using the optimal k if it exists
if(exists("optimal_k")) {
  all_fold_knn_results <- evaluate_all_folds_knn(optimal_k)
}
```




```{r}
library(tidyverse)
library(knitr)
library(kableExtra)  # For nicer tables, install if needed with install.packages("kableExtra")

# Function to extract accuracies from model results
extract_accuracies <- function(model_results) {
  sapply(model_results, function(x) x$accuracy)
}

# Create a data frame to store all results
# First, initialize with fold information
comparison_table <- data.frame(
  Fold = c("Fold 1", "Fold 2", "Fold 3", "Fold 4", "Mean")
)

# Add results for logistic regression (if you ran it)
# Replace "all_fold_results" with the actual variable name you used
if(exists("all_fold_results")) {
  logistic_accuracies <- extract_accuracies(all_fold_results)
  comparison_table$`Logistic Regression` <- c(
    logistic_accuracies * 100,
    mean(logistic_accuracies) * 100
  )
}

# Add results for linear model (if you ran it)
if(exists("all_fold_linear_results")) {
  linear_accuracies <- extract_accuracies(all_fold_linear_results)
  comparison_table$`Linear Model` <- c(
    linear_accuracies * 100,
    mean(linear_accuracies) * 100
  )
}

# Add results for XGBoost (if you ran it)
if(exists("all_fold_xgb_results")) {
  xgb_accuracies <- extract_accuracies(all_fold_xgb_results)
  comparison_table$`XGBoost` <- c(
    xgb_accuracies * 100,
    mean(xgb_accuracies) * 100
  )
}

# Add results for Lasso (if you ran it)
if(exists("all_fold_lasso_results")) {
  lasso_accuracies <- extract_accuracies(all_fold_lasso_results)
  comparison_table$`Lasso` <- c(
    lasso_accuracies * 100,
    mean(lasso_accuracies) * 100
  )
}

# Round all numeric columns to 2 decimal places
comparison_table <- comparison_table %>%
  mutate(across(where(is.numeric), ~ round(., 2)))

# Print the table
print(comparison_table)

# Create a nicer formatted table if kableExtra is available
if(requireNamespace("kableExtra", quietly = TRUE)) {
  formatted_table <- comparison_table %>%
    kable(format = "html", caption = "Prediction Accuracy (%) by Method and Fold") %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
    row_spec(5, bold = TRUE, background = "#f2f2f2")  # Highlight the mean row
  
  print(formatted_table)
}

# Also create a ggplot visualization of the results
comparison_long <- comparison_table %>%
  pivot_longer(cols = -Fold, names_to = "Method", values_to = "Accuracy") %>%
  mutate(Fold = factor(Fold, levels = c("Fold 1", "Fold 2", "Fold 3", "Fold 4", "Mean")))

# Plot
ggplot(comparison_long %>% filter(Fold != "Mean"), 
       aes(x = Method, y = Accuracy, fill = Fold)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_hline(data = comparison_long %>% filter(Fold == "Mean"),
             aes(yintercept = Accuracy, color = Method),
             linetype = "dashed", size = 1) +
  labs(title = "Prediction Accuracy by Method and Fold",
       x = "Method",
       y = "Accuracy (%)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Alternative: Create a line plot to better visualize performance across folds
ggplot(comparison_long %>% filter(Fold != "Mean"), 
       aes(x = Fold, y = Accuracy, color = Method, group = Method)) +
  geom_line(size = 1) +
  geom_point(size = 3) +
  labs(title = "Prediction Accuracy Across Folds",
       x = "Fold",
       y = "Accuracy (%)") +
  theme_minimal()
```












**********************************************************


```{r, eval=FALSE}
library(tidyverse)
library(caret)
library(MASS)  # For LDA
library(class)  # For kNN
library(glmnet)  # For Lasso

# 1. Function to apply PCA and create enhanced datasets
apply_pca <- function(train_data, test_data, variance_threshold = 0.95) {
  # Separate features from target
  train_features <- train_data %>% dplyr::select(-feedback)
  
  # Scale the data before PCA
  pca_prep <- preProcess(train_features, method = c("center", "scale"))
  train_scaled <- predict(pca_prep, train_features)
  
  # Perform PCA
  pca_result <- prcomp(train_scaled)
  
  # Determine how many components to keep based on variance threshold
  variance_explained <- cumsum(pca_result$sdev^2 / sum(pca_result$sdev^2))
  n_components <- min(which(variance_explained >= variance_threshold))
  
  cat("Using", n_components, "principal components to explain", 
      round(variance_explained[n_components] * 100, 2), "% of variance\n")
  
  # Extract PC scores for training data
  train_pca <- as.data.frame(predict(pca_result, train_scaled)[, 1:n_components])
  names(train_pca) <- paste0("PC", 1:n_components)
  
  # Add original target variable
  train_pca$feedback <- train_data$feedback
  
  # Apply same transformation to test data
  test_features <- test_data %>%dplyr::select(-feedback)
  test_scaled <- predict(pca_prep, test_features)
  test_pca <- as.data.frame(predict(pca_result, test_scaled)[, 1:n_components])
  names(test_pca) <- paste0("PC", 1:n_components)
  test_pca$feedback <- test_data$feedback
  
  # Create enhanced versions with original data and PC scores
  train_pca_enhanced <- cbind(train_data, train_pca %>%dplyr::select(-feedback))
  test_pca_enhanced <- cbind(test_data, test_pca %>%dplyr::select(-feedback))
  
  return(list(
    train_pca = train_pca,               # PCA components only
    test_pca = test_pca,                 # PCA components only
    train_pca_enhanced = train_pca_enhanced,  # Original + PCA
    test_pca_enhanced = test_pca_enhanced,    # Original + PCA
    pca_result = pca_result,             # PCA object for analysis
    pca_prep = pca_prep,                 # Preprocessing parameters
    n_components = n_components          # Number of components used
  ))
}

# 2. Function to visualize PCA results
visualize_pca <- function(pca_result, train_data) {
  # Create a scree plot
  variance_explained <- pca_result$sdev^2 / sum(pca_result$sdev^2)
  cumulative_variance <- cumsum(variance_explained)
  
  var_df <- data.frame(
    Component = factor(1:length(variance_explained), levels = 1:length(variance_explained)),
    VarianceExplained = variance_explained,
    CumulativeVariance = cumulative_variance
  )
  
  # Scree plot
  p1 <- ggplot(var_df, aes(x = Component, y = VarianceExplained)) +
    geom_col(fill = "steelblue") +
    geom_line(aes(y = CumulativeVariance), color = "red", group = 1) +
    geom_point(aes(y = CumulativeVariance), color = "red") +
    scale_y_continuous(sec.axis = sec_axis(~., name = "Cumulative Variance")) +
    labs(title = "PCA Scree Plot",
         x = "Principal Component",
         y = "Proportion of Variance Explained") +
    theme_minimal()
  
  # Biplot of the first two components
  pca_data <- as.data.frame(pca_result$x[, 1:2])
  pca_data$feedback <- train_data$feedback
  
  p2 <- ggplot(pca_data, aes(x = PC1, y = PC2, color = feedback)) +
    geom_point(alpha = 0.5) +
    labs(title = "PCA Biplot",
         x = paste0("PC1 (", round(variance_explained[1] * 100, 1), "%)"),
         y = paste0("PC2 (", round(variance_explained[2] * 100, 1), "%)")) +
    theme_minimal()
  
  # Variable contributions plot
  loadings <- as.data.frame(pca_result$rotation[, 1:2])
  loadings$Variable <- rownames(loadings)
  
  p3 <- ggplot(loadings, aes(x = PC1, y = PC2, label = Variable)) +
    geom_point() +
    geom_text(vjust = -0.5) +
    geom_hline(yintercept = 0, linetype = "dashed") +
    geom_vline(xintercept = 0, linetype = "dashed") +
    labs(title = "Variable Contributions to PC1 and PC2") +
    theme_minimal() +
    xlim(-1, 1) + ylim(-1, 1)
  
  cat("Printing PCA visualizations...\n")
  print(p1)
  print(p2)
  print(p3)
  
  return(list(p1 = p1, p2 = p2, p3 = p3))
}

# 3. Function to train and evaluate models with PCA data
train_evaluate_model <- function(train_data, test_data, model_type, ...) {
  # Handle NA values
  train_data <- na.omit(train_data)
  test_data <- na.omit(test_data)
  
  model <- NULL
  predicted_classes <- NULL
  additional_info <- list()
  
  if (model_type == "lda") {
    # Linear Discriminant Analysis
    model <- lda(feedback ~ ., data = train_data)
    predictions <- predict(model, newdata = test_data)
    predicted_classes <- predictions$class
    additional_info$coefficients <- model$scaling
    
  } else if (model_type == "knn") {
    # k-Nearest Neighbors
    k_value <- list(...)$k_value
    if (is.null(k_value)) k_value <- 5
    
    # Scale features
    train_features <- train_data %>%dplyr::select(-feedback)
    train_labels <- train_data$feedback
    test_features <- test_data %>%dplyr::select(-feedback)
    
    # Ensure consistent column order
    test_features <- test_features[, names(train_features)]
    
    scale_params <- preProcess(train_features, method = c("center", "scale"))
    train_scaled <- predict(scale_params, train_features)
    test_scaled <- predict(scale_params, test_features)
    
    predicted_classes <- knn(
      train = train_scaled,
      test = test_scaled,
      cl = train_labels,
      k = k_value
    )
    additional_info$k_value <- k_value
    
  } else if (model_type == "lasso") {
    # Lasso Regression
    alpha <- list(...)$alpha
    if (is.null(alpha)) alpha <- 1
    
    # Convert feedback to numeric
    train_data$feedback_numeric <- as.numeric(as.character(train_data$feedback))
    test_data$feedback_numeric <- as.numeric(as.character(test_data$feedback))
    
    # Create matrices
    x_train <- as.matrix(train_data %>%dplyr::select(-feedback, -feedback_numeric))
    y_train <- train_data$feedback_numeric
    x_test <- as.matrix(test_data %>%dplyr::select(-feedback, -feedback_numeric))
    
    # Find optimal lambda
    cv_model <- cv.glmnet(x_train, y_train, alpha = alpha, nfolds = 5)
    best_lambda <- cv_model$lambda.min
    
    # Train model
    model <- glmnet(x_train, y_train, alpha = alpha, lambda = best_lambda)
    
    # Predict
    predictions <- predict(model, newx = x_test, s = best_lambda)
    predicted_classes <- factor(ifelse(predictions > 0, "1", "-1"), 
                               levels = levels(test_data$feedback))
    
    additional_info$coefficients <- as.matrix(coef(model, s = best_lambda))
    additional_info$lambda <- best_lambda
    additional_info$cv_model <- cv_model
  }
  
  # Calculate accuracy
  accuracy <- mean(predicted_classes == test_data$feedback)
  
  # Create confusion matrix
  cm <- confusionMatrix(predicted_classes, test_data$feedback)
  
  return(list(
    model = model,
    predicted_classes = predicted_classes,
    accuracy = accuracy,
    confusion_matrix = cm,
    additional_info = additional_info
  ))
}

# 4. Apply PCA to the datasets
# Assuming you have train_set_1, test_set_1, etc. already defined
if (exists("train_set_1") && exists("test_set_1")) {
  cat("Applying PCA to Fold 1 data...\n")
  pca_fold1 <- apply_pca(train_set_1, test_set_1)
  
  # Visualize PCA results
  viz_fold1 <- visualize_pca(pca_fold1$pca_result, train_set_1)
  
  # Create datasets for all folds if they exist
  if (all(exists("train_set_1"), exists("test_set_1"),
          exists("train_set_2"), exists("test_set_2"),
          exists("train_set_3"), exists("test_set_3"),
          exists("train_set_4"), exists("test_set_4"))) {
    
    cat("\nApplying PCA to all folds...\n")
    
    pca_fold2 <- apply_pca(train_set_2, test_set_2)
    pca_fold3 <- apply_pca(train_set_3, test_set_3)
    pca_fold4 <- apply_pca(train_set_4, test_set_4)
  }
} else {
  stop("Training and test datasets not found. Please run the cross-validation code first.")
}

# 5. Compare models using PCA data (using fold 1)
# 5.1 Evaluate LDA
cat("\nEvaluating LDA on PCA data...\n")
lda_result_pca <- train_evaluate_model(pca_fold1$train_pca, pca_fold1$test_pca, "lda")
cat("LDA with PCA only:", round(lda_result_pca$accuracy * 100, 2), "%\n")
print(lda_result_pca$confusion_matrix$table)

lda_result_pca_enhanced <- train_evaluate_model(pca_fold1$train_pca_enhanced, 
                                               pca_fold1$test_pca_enhanced, "lda")
cat("LDA with original + PCA:", round(lda_result_pca_enhanced$accuracy * 100, 2), "%\n")
print(lda_result_pca_enhanced$confusion_matrix$table)

# 5.2 Evaluate kNN
cat("\nEvaluating kNN on PCA data...\n")
# Find optimal k first
find_optimal_k <- function(train_data, test_data, k_range = 1:15) {
  k_results <- data.frame(k = integer(), accuracy = numeric())
  
  for (k in k_range) {
    cat("Testing k =", k, "\n")
    result <- train_evaluate_model(train_data, test_data, "knn", k_value = k)
    k_results <- rbind(k_results, data.frame(k = k, accuracy = result$accuracy))
  }
  
  optimal_k <- k_results$k[which.max(k_results$accuracy)]
  cat("Optimal k =", optimal_k, "with accuracy:", 
      round(max(k_results$accuracy) * 100, 2), "%\n")
  
  return(optimal_k)
}

optimal_k <- find_optimal_k(pca_fold1$train_pca, pca_fold1$test_pca)
knn_result_pca <- train_evaluate_model(pca_fold1$train_pca, pca_fold1$test_pca, 
                                      "knn", k_value = optimal_k)
cat("kNN with PCA only:", round(knn_result_pca$accuracy * 100, 2), "%\n")
print(knn_result_pca$confusion_matrix$table)

knn_result_pca_enhanced <- train_evaluate_model(pca_fold1$train_pca_enhanced, 
                                               pca_fold1$test_pca_enhanced, 
                                               "knn", k_value = optimal_k)
cat("kNN with original + PCA:", round(knn_result_pca_enhanced$accuracy * 100, 2), "%\n")
print(knn_result_pca_enhanced$confusion_matrix$table)

# 5.3 Evaluate Lasso
cat("\nEvaluating Lasso on PCA data...\n")
lasso_result_pca <- train_evaluate_model(pca_fold1$train_pca, pca_fold1$test_pca, "lasso")
cat("Lasso with PCA only:", round(lasso_result_pca$accuracy * 100, 2), "%\n")
print(lasso_result_pca$confusion_matrix$table)

lasso_result_pca_enhanced <- train_evaluate_model(pca_fold1$train_pca_enhanced, 
                                                 pca_fold1$test_pca_enhanced, "lasso")
cat("Lasso with original + PCA:", round(lasso_result_pca_enhanced$accuracy * 100, 2), "%\n")
print(lasso_result_pca_enhanced$confusion_matrix$table)

# 6. Evaluate all models across all folds
evaluate_all_folds <- function(model_type, ...) {
  results <- list()
  
  # Fold 1
  if (exists("pca_fold1")) {
    cat("Evaluating", model_type, "on fold 1...\n")
    results[[1]] <- train_evaluate_model(pca_fold1$train_pca, pca_fold1$test_pca, 
                                        model_type, ...)
  }
  
  # Fold 2
  if (exists("pca_fold2")) {
    cat("Evaluating", model_type, "on fold 2...\n")
    results[[2]] <- train_evaluate_model(pca_fold2$train_pca, pca_fold2$test_pca, 
                                        model_type, ...)
  }
  
  # Fold 3
  if (exists("pca_fold3")) {
    cat("Evaluating", model_type, "on fold 3...\n")
    results[[3]] <- train_evaluate_model(pca_fold3$train_pca, pca_fold3$test_pca, 
                                        model_type, ...)
  }
  
  # Fold 4
  if (exists("pca_fold4")) {
    cat("Evaluating", model_type, "on fold 4...\n")
    results[[4]] <- train_evaluate_model(pca_fold4$train_pca, pca_fold4$test_pca, 
                                        model_type, ...)
  }
  
  # Calculate overall accuracy
  accuracies <- sapply(results, function(x) x$accuracy)
  mean_accuracy <- mean(accuracies)
  
  # Print results
  cat("Results for all folds with", model_type, "model on PCA data:\n")
  for (i in seq_along(results)) {
    cat(sprintf("Fold %d accuracy: %.2f%%\n", i, accuracies[i] * 100))
  }
  cat(sprintf("\nMean accuracy across all folds: %.2f%%\n", mean_accuracy * 100))
  
  return(results)
}

# Evaluate LDA across all folds
lda_all_folds <- evaluate_all_folds("lda")

# Evaluate kNN across all folds
knn_all_folds <- evaluate_all_folds("knn", k_value = optimal_k)

# Evaluate Lasso across all folds
lasso_all_folds <- evaluate_all_folds("lasso")

# 7. Create comprehensive comparison table
create_comparison_table <- function() {
  # Create base table structure
  comparison_table <- data.frame(
    Fold = c("Fold 1", "Fold 2", "Fold 3", "Fold 4", "Mean")
  )
  
  # Add LDA results if available
  if (exists("lda_all_folds")) {
    lda_accuracies <- sapply(lda_all_folds, function(x) x$accuracy)
    comparison_table$LDA_PCA <- c(
      lda_accuracies * 100,
      mean(lda_accuracies) * 100
    )
  }
  
  # Add kNN results if available
  if (exists("knn_all_folds")) {
    knn_accuracies <- sapply(knn_all_folds, function(x) x$accuracy)
    comparison_table$kNN_PCA <- c(
      knn_accuracies * 100,
      mean(knn_accuracies) * 100
    )
  }
  
  # Add Lasso results if available
  if (exists("lasso_all_folds")) {
    lasso_accuracies <- sapply(lasso_all_folds, function(x) x$accuracy)
    comparison_table$Lasso_PCA <- c(
      lasso_accuracies * 100,
      mean(lasso_accuracies) * 100
    )
  }
  
  # Add enhanced models results if available
  if (exists("lda_result_pca_enhanced")) {
    comparison_table$LDA_Enhanced <- NA
    comparison_table$LDA_Enhanced[1] <- lda_result_pca_enhanced$accuracy * 100
  }
  
  if (exists("knn_result_pca_enhanced")) {
    comparison_table$kNN_Enhanced <- NA
    comparison_table$kNN_Enhanced[1] <- knn_result_pca_enhanced$accuracy * 100
  }
  
  if (exists("lasso_result_pca_enhanced")) {
    comparison_table$Lasso_Enhanced <- NA
    comparison_table$Lasso_Enhanced[1] <- lasso_result_pca_enhanced$accuracy * 100
  }
  
  # Round all numeric columns
  comparison_table <- comparison_table %>%
    mutate(across(where(is.numeric), ~ round(., 2)))
  
  # Highlight the mean row
  comparison_table$Fold[5] <- paste0("**", comparison_table$Fold[5], "**")
  
  return(comparison_table)
}

# Create and print the comparison table
final_comparison <- create_comparison_table()
print(final_comparison)

# 8. Create visualization of performance comparison
if (ncol(final_comparison) > 1) {
  comparison_long <- final_comparison %>%
    pivot_longer(cols = -Fold, names_to = "Model", values_to = "Accuracy") %>%
    filter(Fold != "**Mean**")
  
  ggplot(comparison_long, aes(x = Model, y = Accuracy, fill = Fold)) +
    geom_bar(stat = "identity", position = "dodge") +
    labs(title = "Model Performance Comparison with PCA",
         x = "Model Type",
         y = "Accuracy (%)") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
}
```
```{r}
library(tidyverse)
library(caret)
library(xgboost)  # For XGBoost

# Function to train and evaluate logistic regression (GLM)
train_evaluate_glm <- function(train_data, test_data) {
  # Check for any NA values
  if(sum(is.na(train_data)) > 0) {
    cat("Warning: Training data contains", sum(is.na(train_data)), "NA values\n")
    cat("Removing NA values from training data\n")
    train_data <- na.omit(train_data)
  }
  
  if(sum(is.na(test_data)) > 0) {
    cat("Warning: Test data contains", sum(is.na(test_data)), "NA values\n")
    cat("Removing NA values from test data\n")
    test_data <- na.omit(test_data)
  }
  
  # Train the logistic regression model
  glm_model <- glm(feedback ~ ., data = train_data, family = "binomial")
  
  # Make predictions
  predictions_prob <- predict(glm_model, newdata = test_data, type = "response")
  predicted_classes <- factor(ifelse(predictions_prob > 0.5, "1", "-1"), 
                             levels = levels(test_data$feedback))
  
  # Calculate accuracy
  accuracy <- mean(predicted_classes == test_data$feedback)
  
  # Create confusion matrix
  cm <- confusionMatrix(predicted_classes, test_data$feedback)
  
  return(list(
    model = glm_model,
    predictions = predictions_prob,
    predicted_classes = predicted_classes,
    accuracy = accuracy,
    confusion_matrix = cm
  ))
}

# Function to train and evaluate linear regression (LM)
train_evaluate_lm <- function(train_data, test_data) {
  # Check for any NA values
  if(sum(is.na(train_data)) > 0) {
    cat("Warning: Training data contains", sum(is.na(train_data)), "NA values\n")
    cat("Removing NA values from training data\n")
    train_data <- na.omit(train_data)
  }
  
  if(sum(is.na(test_data)) > 0) {
    cat("Warning: Test data contains", sum(is.na(test_data)), "NA values\n")
    cat("Removing NA values from test data\n")
    test_data <- na.omit(test_data)
  }
  
  # Convert feedback to numeric for linear regression
  train_data$feedback_numeric <- as.numeric(as.character(train_data$feedback))
  test_data$feedback_numeric <- as.numeric(as.character(test_data$feedback))
  
  # Train the linear model
  lm_model <- lm(feedback_numeric ~ ., data = train_data %>%dplyr::select(-feedback))
  
  # Make predictions
  predictions <- predict(lm_model, newdata = test_data %>%dplyr::select(-feedback, -feedback_numeric))
  predicted_classes <- factor(ifelse(predictions > 0, "1", "-1"), 
                             levels = levels(test_data$feedback))
  
  # Calculate accuracy
  accuracy <- mean(predicted_classes == test_data$feedback)
  
  # Create confusion matrix
  cm <- confusionMatrix(predicted_classes, test_data$feedback)
  
  return(list(
    model = lm_model,
    predictions = predictions,
    predicted_classes = predicted_classes,
    accuracy = accuracy,
    confusion_matrix = cm
  ))
}

# Function to train and evaluate XGBoost
train_evaluate_xgboost <- function(train_data, test_data) {
  # Check for any NA values
  if(sum(is.na(train_data)) > 0) {
    cat("Warning: Training data contains", sum(is.na(train_data)), "NA values\n")
    cat("Removing NA values from training data\n")
    train_data <- na.omit(train_data)
  }
  
  if(sum(is.na(test_data)) > 0) {
    cat("Warning: Test data contains", sum(is.na(test_data)), "NA values\n")
    cat("Removing NA values from test data\n")
    test_data <- na.omit(test_data)
  }
  
  # Prepare data for XGBoost
  # Convert feedback to numeric (0/1)
  train_data$feedback_numeric <- as.numeric(as.character(train_data$feedback))
  test_data$feedback_numeric <- as.numeric(as.character(test_data$feedback))
  
  # Convert -1 to 0 for binary classification
  train_data$feedback_numeric <- ifelse(train_data$feedback_numeric == -1, 0, 1)
  test_data$feedback_numeric <- ifelse(test_data$feedback_numeric == -1, 0, 1)
  
  # Create DMatrix objects
  dtrain <- xgb.DMatrix(
    data = as.matrix(train_data %>%dplyr::select(-feedback, -feedback_numeric)),
    label = train_data$feedback_numeric
  )
  
  dtest <- xgb.DMatrix(
    data = as.matrix(test_data %>%dplyr::select(-feedback, -feedback_numeric)),
    label = test_data$feedback_numeric
  )
  
  # Set parameters
  params <- list(
    objective = "binary:logistic",
    eval_metric = "error",
    eta = 0.1,
    max_depth = 6,
    subsample = 0.8,
    colsample_bytree = 0.8
  )
  
  # Train model
  xgb_model <- xgb.train(
    params = params,
    data = dtrain,
    nrounds = 100,
    watchlist = list(train = dtrain, test = dtest),
    verbose = 0
  )
  
  # Make predictions
  predictions_prob <- predict(xgb_model, dtest)
  predicted_classes <- factor(ifelse(predictions_prob > 0.5, "1", "-1"), 
                             levels = levels(test_data$feedback))
  
  # Calculate accuracy
  accuracy <- mean(predicted_classes == test_data$feedback)
  
  # Create confusion matrix
  cm <- confusionMatrix(predicted_classes, test_data$feedback)
  
  # Get feature importance
  importance <- xgb.importance(
    feature_names = colnames(train_data %>%dplyr::select(-feedback, -feedback_numeric)),
    model = xgb_model
  )
  
  return(list(
    model = xgb_model,
    predictions = predictions_prob,
    predicted_classes = predicted_classes,
    accuracy = accuracy,
    confusion_matrix = cm,
    importance = importance
  ))
}

# Assuming PCA datasets are already created (pca_fold1, etc.)
# If they're not available, uncomment and run this code:
# 
# apply_pca <- function(train_data, test_data, variance_threshold = 0.95) {
#   # PCA implementation code here (from previous example)
# }
#
# pca_fold1 <- apply_pca(train_set_1, test_set_1)
# pca_fold2 <- apply_pca(train_set_2, test_set_2)
# pca_fold3 <- apply_pca(train_set_3, test_set_3)
# pca_fold4 <- apply_pca(train_set_4, test_set_4)

# Now evaluate our three models:

# 1. Evaluate Logistic Regression (GLM) on PCA data
if(exists("pca_fold1")) {
  cat("\nEvaluating Logistic Regression (GLM) on PCA data...\n")
  glm_result_pca <- train_evaluate_glm(pca_fold1$train_pca, pca_fold1$test_pca)
  cat("GLM with PCA only:", round(glm_result_pca$accuracy * 100, 2), "%\n")
  print(glm_result_pca$confusion_matrix$table)
  
  glm_result_pca_enhanced <- train_evaluate_glm(pca_fold1$train_pca_enhanced, 
                                              pca_fold1$test_pca_enhanced)
  cat("GLM with original + PCA:", round(glm_result_pca_enhanced$accuracy * 100, 2), "%\n")
  print(glm_result_pca_enhanced$confusion_matrix$table)
} else {
  cat("PCA datasets not found. Please run the PCA code first.\n")
}

# 2. Evaluate Linear Regression (LM) on PCA data
if(exists("pca_fold1")) {
  cat("\nEvaluating Linear Regression (LM) on PCA data...\n")
  lm_result_pca <- train_evaluate_lm(pca_fold1$train_pca, pca_fold1$test_pca)
  cat("LM with PCA only:", round(lm_result_pca$accuracy * 100, 2), "%\n")
  print(lm_result_pca$confusion_matrix$table)
  
  lm_result_pca_enhanced <- train_evaluate_lm(pca_fold1$train_pca_enhanced, 
                                            pca_fold1$test_pca_enhanced)
  cat("LM with original + PCA:", round(lm_result_pca_enhanced$accuracy * 100, 2), "%\n")
  print(lm_result_pca_enhanced$confusion_matrix$table)
}

# 3. Evaluate XGBoost on PCA data
if(exists("pca_fold1")) {
  cat("\nEvaluating XGBoost on PCA data...\n")
  xgb_result_pca <- train_evaluate_xgboost(pca_fold1$train_pca, pca_fold1$test_pca)
  cat("XGBoost with PCA only:", round(xgb_result_pca$accuracy * 100, 2), "%\n")
  print(xgb_result_pca$confusion_matrix$table)
  
  xgb_result_pca_enhanced <- train_evaluate_xgboost(pca_fold1$train_pca_enhanced, 
                                                  pca_fold1$test_pca_enhanced)
  cat("XGBoost with original + PCA:", round(xgb_result_pca_enhanced$accuracy * 100, 2), "%\n")
  print(xgb_result_pca_enhanced$confusion_matrix$table)
  
  # Plot XGBoost feature importance
  if(nrow(xgb_result_pca$importance) > 0) {
    ggplot(xgb_result_pca$importance, aes(x = reorder(Feature, Gain), y = Gain)) +
      geom_col() +
      coord_flip() +
      labs(title = "XGBoost Feature Importance (PCA)",
           x = "Feature",
           y = "Gain") +
      theme_minimal()
  }
}

# Function to evaluate all folds for a specific model
evaluate_all_folds <- function(model_type) {
  results <- list()
  
  # Fold 1
  if (exists("pca_fold1")) {
    cat("Evaluating", model_type, "on fold 1...\n")
    if (model_type == "glm") {
      results[[1]] <- train_evaluate_glm(pca_fold1$train_pca, pca_fold1$test_pca)
    } else if (model_type == "lm") {
      results[[1]] <- train_evaluate_lm(pca_fold1$train_pca, pca_fold1$test_pca)
    } else if (model_type == "xgboost") {
      results[[1]] <- train_evaluate_xgboost(pca_fold1$train_pca, pca_fold1$test_pca)
    }
  }
  
  # Fold 2
  if (exists("pca_fold2")) {
    cat("Evaluating", model_type, "on fold 2...\n")
    if (model_type == "glm") {
      results[[2]] <- train_evaluate_glm(pca_fold2$train_pca, pca_fold2$test_pca)
    } else if (model_type == "lm") {
      results[[2]] <- train_evaluate_lm(pca_fold2$train_pca, pca_fold2$test_pca)
    } else if (model_type == "xgboost") {
      results[[2]] <- train_evaluate_xgboost(pca_fold2$train_pca, pca_fold2$test_pca)
    }
  }
  
  # Fold 3
  if (exists("pca_fold3")) {
    cat("Evaluating", model_type, "on fold 3...\n")
    if (model_type == "glm") {
      results[[3]] <- train_evaluate_glm(pca_fold3$train_pca, pca_fold3$test_pca)
    } else if (model_type == "lm") {
      results[[3]] <- train_evaluate_lm(pca_fold3$train_pca, pca_fold3$test_pca)
    } else if (model_type == "xgboost") {
      results[[3]] <- train_evaluate_xgboost(pca_fold3$train_pca, pca_fold3$test_pca)
    }
  }
  
  # Fold 4
  if (exists("pca_fold4")) {
    cat("Evaluating", model_type, "on fold 4...\n")
    if (model_type == "glm") {
      results[[4]] <- train_evaluate_glm(pca_fold4$train_pca, pca_fold4$test_pca)
    } else if (model_type == "lm") {
      results[[4]] <- train_evaluate_lm(pca_fold4$train_pca, pca_fold4$test_pca)
    } else if (model_type == "xgboost") {
      results[[4]] <- train_evaluate_xgboost(pca_fold4$train_pca, pca_fold4$test_pca)
    }
  }
  
  # Calculate overall accuracy
  accuracies <- sapply(results, function(x) x$accuracy)
  mean_accuracy <- mean(accuracies)
  
  # Print results
  cat("Results for all folds with", model_type, "model on PCA data:\n")
  for (i in seq_along(results)) {
    cat(sprintf("Fold %d accuracy: %.2f%%\n", i, accuracies[i] * 100))
  }
  cat(sprintf("\nMean accuracy across all folds: %.2f%%\n", mean_accuracy * 100))
  
  return(results)
}

# Evaluate all models across all folds
if (all(exists("pca_fold1"), exists("pca_fold2"), exists("pca_fold3"), exists("pca_fold4"))) {
  # Logistic Regression
  cat("\nEvaluating Logistic Regression across all folds...\n")
  glm_all_folds <- evaluate_all_folds("glm")
  
  # Linear Regression
  cat("\nEvaluating Linear Regression across all folds...\n")
  lm_all_folds <- evaluate_all_folds("lm")
  
  # XGBoost
  cat("\nEvaluating XGBoost across all folds...\n")
  xgb_all_folds <- evaluate_all_folds("xgboost")
}

# Create comprehensive comparison table
create_comparison_table <- function() {
  # Create base table structure
  comparison_table <- data.frame(
    Fold = c("Fold 1", "Fold 2", "Fold 3", "Fold 4", "Mean")
  )
  
  # Add GLM results if available
  if (exists("glm_all_folds")) {
    glm_accuracies <- sapply(glm_all_folds, function(x) x$accuracy)
    comparison_table$GLM_PCA <- c(
      glm_accuracies * 100,
      mean(glm_accuracies) * 100
    )
  }
  
  # Add LM results if available
  if (exists("lm_all_folds")) {
    lm_accuracies <- sapply(lm_all_folds, function(x) x$accuracy)
    comparison_table$LM_PCA <- c(
      lm_accuracies * 100,
      mean(lm_accuracies) * 100
    )
  }
  
  # Add XGBoost results if available
  if (exists("xgb_all_folds")) {
    xgb_accuracies <- sapply(xgb_all_folds, function(x) x$accuracy)
    comparison_table$XGBoost_PCA <- c(
      xgb_accuracies * 100,
      mean(xgb_accuracies) * 100
    )
  }
  
  # Add enhanced models results if available
  if (exists("glm_result_pca_enhanced")) {
    comparison_table$GLM_Enhanced <- NA
    comparison_table$GLM_Enhanced[1] <- glm_result_pca_enhanced$accuracy * 100
  }
  
  if (exists("lm_result_pca_enhanced")) {
    comparison_table$LM_Enhanced <- NA
    comparison_table$LM_Enhanced[1] <- lm_result_pca_enhanced$accuracy * 100
  }
  
  if (exists("xgb_result_pca_enhanced")) {
    comparison_table$XGBoost_Enhanced <- NA
    comparison_table$XGBoost_Enhanced[1] <- xgb_result_pca_enhanced$accuracy * 100
  }
  
  # Round all numeric columns
  comparison_table <- comparison_table %>%
    mutate(across(where(is.numeric), ~ round(., 2)))
  
  # Highlight the mean row
  comparison_table$Fold[5] <- paste0("**", comparison_table$Fold[5], "**")
  
  return(comparison_table)
}

# Create and print the comparison table
final_comparison <- create_comparison_table()
print(final_comparison)

# Create visualization of performance comparison
if (ncol(final_comparison) > 1) {
  comparison_long <- final_comparison %>%
    pivot_longer(cols = -Fold, names_to = "Model", values_to = "Accuracy") %>%
    filter(!grepl("\\*\\*", Fold))  # Remove the mean row with asterisks
  
  ggplot(comparison_long, aes(x = Model, y = Accuracy, fill = Fold)) +
    geom_bar(stat = "identity", position = "dodge") +
    labs(title = "Model Performance Comparison with PCA",
         x = "Model Type",
         y = "Accuracy (%)") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
}
```




















# Project report outline 

The final submission of the course project is a report in HTML format, along with a link to the Github repository that can be used to reproduce your report. The project report must be legible and the exposition of the report is part of the grading rubrics. For consistency in grading, please follow the outline listed below. 

- Title.

- Abstract (5 pts).

- Section 1 Introduction (5 pts). 

- Section 2 Exploratory analysis (20 pts). 

- Section 3 Data integration (20 pts). 

- Section 4 Predictive modeling (20 pts). 

- Section 5 Prediction performance on the test sets (5 pts). 

- Section 6 Discussion (5 pts). 

In addition, the remaining 20 points will be allocated to report organization and legibility and creativity and originality. 


# Project milestones

A series of milestones are set throughout the quarter in order to encourage, and reward, early starts on the course project. Furthermore, there are several project discussion sessions throughout the quarter for students to utilize. 


- Project proposal January 24th (optional): 0 points. Students are **strongly recommended** to attend the project discussion during the regular lecture time on Zoom. 
- Milestone I February 14th  (optional): 0 points but eligible for bonus points for outstanding progress or novel findings. Draft analysis and results for Part I visualization. Students are **recommended** to attend the optional project discussion during the regular lecture time on Zoom. 
- Milestone II March 7th (optional): 0 points but eligible for bonus points for outstanding progress or novel findings. Draft analysis and results for Part II data integration. Students are **recommended** to attend the optional project discussion during the regular lecture time on Zoom. 
- March 17th Project report: 60 points. Students are **strongly recommended** to attend at least one project consulting session in Week 10. 


**Remark**: One important thing to note is that a course project is not an exam where questions on the exam are kept confidential. Instead, the instructor and TAs are more than happy to share with you our thoughts on how to improve your projects before you submit them. From a practical perspective, it is more rewarding to solicit advice and suggestions before we grade your reports than to wait for feedback afterwards. That said, we understand that you may have other courses and obligations that are more important than this course. Therefore, all submissions and attendance are optional except for the final project report due on June 12th.

# Reference {-}


Steinmetz, N.A., Zatka-Haas, P., Carandini, M. et al. Distributed coding of choice, action and engagement across the mouse brain. Nature 576, 266–273 (2019). https://doi.org/10.1038/s41586-019-1787-x


- Exploratory data analysis
- raw data into matrix x
- prediciton model: x --> y --> -1 or 1
  - using matrix x 
  
  - average spike over time
  
  - for one session: put it into x matrix
  - 
  
   plug picture into prediction model 
   
   - 01 matrix into signal: functional PCA to transformed signal
  - outcome = 

- TOC tells you how good the regression is
- compare the different models to see which is better
- feed PCA --> tabular data into here (look at last few weeks of notes to get to this point)

*** can look at past rpojects for inspiration

1) EDA
feature ~ feedback (success/success+failure)

Result: have few features you are goingn to incorporate
- how does success vary with feature 
- ex) feedback type vs brain area
- what types of things have people found online

2) transform data to get X matrix: summary statistic or dimensional reduction technique (clustering/PCA)

- for the pca, you input a dataframe that has all of the trials from all of the different sessions with all of the variables you think are important based ont he EDA portion
- too many features = decreased prediction power
- start with one sessionn, thenn concatenate other sessions after
  - different sessions have different numbers of brain areas
  - compare different types of PCA (with different variables), can compare it to just doing prediction on the full dataset

- dat matrix in Discussion 8: X matrix
  - n observzations, feedback type, decisionn, average_spikes

make a matrix of X: number of features, number of observations

3) Fit a prediction model 
 - looking at accuracy and area under curve (want model that yields highest values of both)
  - look at percent error and area under curve
 - look at many different types of models 
 - logistic regression: strong bias towards success
    -  can try to address this through resampling to have equal number of successes and failures in sample before making X matrix
    - weighting the faiolure group
  - model should perform better than a logistic regression
  
  
  4) Test data set
  - another way of checking strength of model
  
  
  - trainig dataset
  - if our test dataset only sess 1 and 18, then should we use the entire dataset for training the model?
  - all sessions and mice are very different
  - how do I reliably train a model with only 200 trials if I use only session 1
  - how do you justify your point
  - polish the format of this html file 
    
    
  * spike correlated to success
  - left contrast and right contrast
    
    
    summation of wi(yi - fxi)^2
 - loigstic regression is not a good model when using innput of summary statistic
 
 
 
 - train separate model under separate condtions?
  - if both L and R contrast are the same, then the outocme is randomized
  - different than if the mous eactually turns the wheel correctly?
  - keep this accuracy separately from other conditions
 
 
 - should we weight the data before prediction
 -  after pca but before prediction
 
 
 
 - format this report correctly
 - document what is going on
 
 
 
 - utilize code from discussion sections and proejct consulting sessions
 
 - can sign up for two slots 
 
 
 
 
talk with veda
- veda did an chi square test to determine if congrast has an effect on feedvback score
- do another test evliuating the effect of spike count on feedback score 


friday
- consultingmsII
- other ways to have mean statistic that isnt average spike count
- pay attention to time windows: 
  - lets only look at certain time windows
  - justify why you are using the variables that you are doing
  - YOU ARE MAKING CHOICES: JUSTIFY
  - if you only want to analyze a few sessions, explain why
  - explain what criteria you are using for evaluaiting a fitted model 
  
  -  compare PCA apporach to benchmark approach in this course notes
  - how many PCA are we going to use
  - on each session separately
  - PC score: three entry vector for each trial in session 1, 2
  - randomlydplyr::select 100 neurons from each session 
  - PCA assumes low dimensional variables behind dataset: rationale behind downsampling
  - it might be ideal to do this, but I don't have time so I'm doing this less good approach..... 
  